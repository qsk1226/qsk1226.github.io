<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[大爷来玩儿啊~]]></title>
  <link href="http://www.throne4j.com/atom.xml" rel="self"/>
  <link href="http://www.throne4j.com/"/>
  <updated>2020-12-18T00:53:12+08:00</updated>
  <id>http://www.throne4j.com/</id>
  <author>
    <name><![CDATA[]]></name>
    
  </author>
  <generator uri="http://www.mweb.im/">MWeb</generator>
  
  <entry>
    <title type="html"><![CDATA[ResponseBodyAdvice]]></title>
    <link href="http://www.throne4j.com/16079180311610.html"/>
    <updated>2020-12-14T11:53:51+08:00</updated>
    <id>http://www.throne4j.com/16079180311610.html</id>
    <content type="html"><![CDATA[

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[HandlerInterceptor]]></title>
    <link href="http://www.throne4j.com/16079177661857.html"/>
    <updated>2020-12-14T11:49:26+08:00</updated>
    <id>http://www.throne4j.com/16079177661857.html</id>
    <content type="html"><![CDATA[

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[RequestContextHolder 类详解]]></title>
    <link href="http://www.throne4j.com/16078839880324.html"/>
    <updated>2020-12-14T02:26:28+08:00</updated>
    <id>http://www.throne4j.com/16078839880324.html</id>
    <content type="html"><![CDATA[

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[WebApplicationContext]]></title>
    <link href="http://www.throne4j.com/16078839202895.html"/>
    <updated>2020-12-14T02:25:20+08:00</updated>
    <id>http://www.throne4j.com/16078839202895.html</id>
    <content type="html"><![CDATA[

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[ContextLoaderListener]]></title>
    <link href="http://www.throne4j.com/16078838973010.html"/>
    <updated>2020-12-14T02:24:57+08:00</updated>
    <id>http://www.throne4j.com/16078838973010.html</id>
    <content type="html"><![CDATA[

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[HttpMessage]]></title>
    <link href="http://www.throne4j.com/16078835677518.html"/>
    <updated>2020-12-14T02:19:27+08:00</updated>
    <id>http://www.throne4j.com/16078835677518.html</id>
    <content type="html"><![CDATA[

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[HttpMessageConverter]]></title>
    <link href="http://www.throne4j.com/16078834576798.html"/>
    <updated>2020-12-14T02:17:37+08:00</updated>
    <id>http://www.throne4j.com/16078834576798.html</id>
    <content type="html"><![CDATA[

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[WebApplicationContext]]></title>
    <link href="http://www.throne4j.com/16078833088880.html"/>
    <updated>2020-12-14T02:15:08+08:00</updated>
    <id>http://www.throne4j.com/16078833088880.html</id>
    <content type="html"><![CDATA[

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[并发编程经典题]]></title>
    <link href="http://www.throne4j.com/16078608737984.html"/>
    <updated>2020-12-13T20:01:13+08:00</updated>
    <id>http://www.throne4j.com/16078608737984.html</id>
    <content type="html"><![CDATA[
<ul>
<li>
<a href="#toc_0">多线程的出现是要解决什么问题的?</a>
</li>
<li>
<a href="#toc_1">并发出现问题的根源</a>
</li>
<li>
<a href="#toc_2">java 线程状态</a>
</li>
<li>
<a href="#toc_3">并行和并发</a>
</li>
<li>
<a href="#toc_4">进程与线程的区别</a>
</li>
<li>
<a href="#toc_5">java中守护进程和本地进程的区别</a>
</li>
<li>
<a href="#toc_6">通常线程有哪几种使用方式?</a>
</li>
<li>
<a href="#toc_7">死锁和活锁的区别，死锁与饥饿的区别</a>
</li>
<li>
<a href="#toc_8">怎么唤醒一个阻塞的线程？</a>
</li>
<li>
<a href="#toc_9">不可变对象对多线程有什么帮助</a>
</li>
<li>
<a href="#toc_10">什么是线程的上下文切换</a>
</li>
<li>
<a href="#toc_11">java 中用到的线程调度算法是什么</a>
</li>
<li>
<a href="#toc_12">Thread.sleep(0)的作用是什么？</a>
</li>
<li>
<a href="#toc_13">Runnable接口和Callable接口的区别？</a>
</li>
<li>
<a href="#toc_14">什么是线程组，为什么在 Java 中不推荐使用?</a>
</li>
<li>
<a href="#toc_15">多线程同步和互斥有几种实现方法？</a>
</li>
<li>
<a href="#toc_16">什么是java内存模型？</a>
</li>
<li>
<a href="#toc_17">volatile关键字的作用？</a>
</li>
<li>
<a href="#toc_18">什么是 CAS</a>
</li>
<li>
<a href="#toc_19">ThreadLocal 有什么作用</a>
</li>
<li>
<a href="#toc_20">为什么 wait, notify 和 notifyAll 这些方法不在 thread 类里面?</a>
<ul>
<li>
<a href="#toc_21">为什么 wait 和 notify 方法要在同步块中调用?</a>
</li>
</ul>
</li>
<li>
<a href="#toc_22">怎么检测一个线程是否持有对象监视器</a>
</li>
<li>
<a href="#toc_23">同步方法和同步块，哪个是更好的选择</a>
</li>
<li>
<a href="#toc_24">乐观锁和悲观锁的理解及如何实现，有哪些实现方式?</a>
</li>
<li>
<a href="#toc_25">什么是线程安全</a>
</li>
<li>
<a href="#toc_26">线程类的构造方法、静态块是被哪个线程调用的？</a>
</li>
<li>
<a href="#toc_27">Java中如何获取到线程dump文件？</a>
</li>
<li>
<a href="#toc_28">一个线程如果出现了运行时异常会怎么样？</a>
</li>
<li>
<a href="#toc_29">如何在两个线程之间共享数据？</a>
</li>
<li>
<a href="#toc_30">sleep方法和wait方法有什么区别 ？</a>
</li>
<li>
<a href="#toc_31">Java中interrupted 和 isInterrupted方法的区别？</a>
</li>
<li>
<a href="#toc_32">单例模式的线程安全性？</a>
</li>
<li>
<a href="#toc_33">Hashtable的size()方法中明明只有一条语句&quot;return count&quot;，为什么还要做同步？</a>
</li>
<li>
<a href="#toc_34">介绍一下Executor 框架</a>
<ul>
<li>
<a href="#toc_35">为什么使用 Executor 框架</a>
</li>
</ul>
</li>
<li>
<a href="#toc_36">Java 线程池中 submit() 和 execute()方法有什么区别?</a>
</li>
<li>
<a href="#toc_37">如果你提交任务时，线程池队列已满，这时会发生什么？</a>
</li>
<li>
<a href="#toc_38">高并发、任务执行时间短的业务怎样使用线程池？并发不高、任务执行时间长的业务怎样使用线程池？并发高、业务执行时间长的业务怎样使用线程池？</a>
</li>
<li>
<a href="#toc_39">什么是 FutureTask</a>
</li>
<li>
<a href="#toc_40">什么是阻塞队列？阻塞队列的实现原理是什么？如何使用阻塞队列来实现生产者-消费者模型？</a>
</li>
<li>
<a href="#toc_41">什么是 AQS</a>
</li>
<li>
<a href="#toc_42">CyclicBarrier和CountDownLatch的区别？</a>
</li>
<li>
<a href="#toc_43">Semaphore 有什么作用</a>
</li>
<li>
<a href="#toc_44">写时复制容器可以用于什么应用场景</a>
</li>
<li>
<a href="#toc_45">生产者消费者模型的作用是什么？</a>
</li>
<li>
<a href="#toc_46">什么是可重入锁(ReentrantLock)?谈谈它的实现</a>
</li>
<li>
<a href="#toc_47">synchronized 和 ReentrantLock 的对比</a>
</li>
<li>
<a href="#toc_48">说一下 synchronized 的实现原理</a>
</li>
<li>
<a href="#toc_49">什么是自旋？</a>
</li>
<li>
<a href="#toc_50">ReadWriteLock 是什么</a>
</li>
<li>
<a href="#toc_51">ConcurrentHashMap的并发度是什么？</a>
</li>
<li>
<a href="#toc_52">Linux环境下如何查找哪个线程使用CPU最长</a>
</li>
</ul>


<h2 id="toc_0">多线程的出现是要解决什么问题的?</h2>

<p>随着计算机硬件技术的发展，CPU、内存、IO等设备的速度差异有了极大的差异，为了充分的利用 CPU 资源，平衡这三者的速度差异，计算机体系结构、操作系统、编译程序都做出了贡献。</p>

<ul>
<li>CPU 增加了缓存，以均衡与内存的速度差异， 导致了 可见性问题 </li>
<li>操作系统增加了进程、线程，以分时复用 CPU，进而均衡 CPU 与 I/O 设备的速度差异，导致了原子性问题 </li>
<li>编译程序优化指令执行次序，使得缓存能够得到更加合理地利用。导致了顺序性问题</li>
</ul>

<h2 id="toc_1">并发出现问题的根源</h2>

<p>结合上一个问题看这个问题<br/>
原子性、可见性、有序性：</p>

<ul>
<li><p>原子性<br/>
原子性指的是一个或者多个操作，要么全部执行并且在执行的过程中不被其他操作打断，要么就全部都不执行。</p>
<p>最有代表性的示例就是银行转账问题了：比如从账户A 向账户B 转1000元，那么必然包括2个操作：从账户A减去1000元，往账户B加上1000元。 </p>
<p>如果这2个操作不具备原子性，会造成什么样的后果。假如从账户A减去1000元之后，操作突然中止。则只有 账户A 上面少了 1000元，而账户B 上却没有收到这笔钱，1000元就这样蒸发了，这种情况是不是很可怕，要是转账1000W的话，不知有多少人要跳楼喽。</p></li>
<li><p>可见性<br/>
可见性指多个线程操作一个共享变量时，其中一个线程对变量进行修改后，其他线程可以立即看到修改的结果。</p>
<p>现在假设有两个线程，分别是 CPU1 和 CPU2, 现在要修改一个变量 count = 0，先是 CPU1 从主内存中加载到 CPU1 的高速缓冲区，并对 count 进行修改 count = 10，但是这时候，count = 10 还未刷新回主存，CPU2 从主内存中 读取count，这时候的 count 值仍然是 0，CPU2 未立即得到 CPU1 的计算结果 count = 10，这既是可见性问题</p></li>
<li><p>有序性<br/>
有序性，即程序的执行顺序按照代码的先后顺序来执行。</p>
<p>在执行程序时为了提高性能，编译器和处理器常常会对指令做重排序。重排序分三种类型： </p>
<ul>
<li>编译器优化的重排序。编译器在不改变单线程程序语义的前提下，可以重新安排语句的执行顺序。 </li>
<li>指令级并行的重排序。现代处理器采用了指令级并行技术（Instruction-Level Parallelism， ILP）来将多条指令重叠执行。如果不存在数据依赖性，处理器可以改变语句对应机器指令的执行顺序。 </li>
<li>内存系统的重排序。由于处理器使用缓存和 读/写 缓冲区，这使得加载和存储操作看上去可能是在乱序执行。</li>
</ul></li>
</ul>

<h2 id="toc_2">java 线程状态</h2>

<p><figure><img src="media/16078608737984/16078815638754.jpg" alt=""/></figure></p>

<p>在Java中线程的状态一共被分成6种：</p>

<ul>
<li>初始态：</li>
</ul>

<p>创建一个Thread对象，但还未调用start()启动线程时，线程处于初始态。</p>

<ul>
<li><p>运行态：<br/>
在Java中，运行态包括就绪态和运行态。</p>
<ul>
<li>就绪态该状态下的线程已经获得执行所需的所有资源，只要CPU分配执行权就能运行。所有就绪态的线程存放在就绪队列中。</li>
<li>运行态获得CPU执行权，正在执行的线程。由于一个CPU同一时刻只能执行一条线程，因此每个CPU每个时刻只有一条运行态的线程。</li>
</ul></li>
<li><p>阻塞态：<br/>
当一条正在执行的线程请求某一资源失败时，就会进入阻塞态。而在Java中，阻塞态专指请求锁失败时进入的状态。由一个阻塞队列存放所有阻塞态的线程。处于阻塞态的线程会不断请求资源，一旦请求成功，就会进入就绪队列，等待执行。PS：锁、IO、Socket等都资源。</p></li>
<li><p>等待态：<br/>
当前线程中调用wait、join、park 函数时，当前线程就会进入等待态。也有一个等待队列存放所有等待态的线程。线程处于等待态表示它需要等待其他线程的指示才能继续运行。进入等待态的线程会释放CPU执行权，并释放资源（如：锁）</p></li>
<li><p>超时等待态：<br/>
当运行中的线程调用sleep(time)、wait、join、parkNanos、parkUntil时，就会进入该状态；它和等待态一样，并不是因为请求不到资源，而是主动进入，并且进入后需要其他线程唤醒；进入该状态后释放CPU执行权 和 占有的资源。与等待态的区别：到了超时时间后自动进入阻塞队列，开始竞争锁。</p></li>
<li><p>终止态：<br/>
线程执行结束后的状态。</p></li>
</ul>

<p><strong>注意</strong>：</p>

<ul>
<li>wait()方法会释放CPU执行权和占有的锁。</li>
<li>sleep(long)方法仅释放CPU使用权，锁仍然占用；线程被放入超时等待队列，与yield相比，它会使线程较长时间得不到运行。</li>
<li>yield()方法仅释放CPU执行权，锁仍然占用，线程会被放入就绪队列，会在短时间内再次执行。</li>
<li>wait和notify必须配套使用，即必须使用同一把锁调用；</li>
<li>wait和notify必须放在一个同步块中调用wait和notify的对象必须是他们所处同步块的锁对象。</li>
</ul>

<h2 id="toc_3">并行和并发</h2>

<p>并发:指应用能够交替执行不同的任务,比如单 CPU 核心下执行多线程并非是 同时执行多个任务,如果你开两个线程执行,就是在你几乎不可能察觉到的速度不 断去切换这两个任务,已达到&quot;同时执行效果&quot;,其实并不是的,只是计算机的速度太 快,我们无法察觉到而已.</p>

<p>并行:指应用能够同时执行不同的任务,例:吃饭的时候可以边吃饭边打电话, 这两件事情可以同时执行</p>

<p>两者区别:一个是交替执行,一个是同时执行.</p>

<h2 id="toc_4">进程与线程的区别</h2>

<p>线程具有许多传统进程锁具有的特征，故又称为轻型进程，而把传统的进程成为重型进程，它相当于只有一个线程的任务。在引入线程的操作系统中，通常一个进程都有若干个线程，至少包含一个线程。</p>

<ul>
<li>根本区别：进程是操作系统资源分配的基本单位，线程是处理器任务调度和执行的基本单位</li>
<li>资源开销： 每个进程都有独立的代码和数据空间，程序之间的切换会有较大的开销；线程可以看做轻量级进程，同一类线程共享代码和数据空间，每个线程都有自己独立的运行栈和程序计数器，线程之间切换开销较小</li>
<li>包含关系：如果一个进程内有多个线程，则执行过程不是一条线的，而是多线程共同完成的；线程是进程的一部分，所以线程也被成为轻量进程</li>
<li>内存分配：同一个进程的线程共享本进程的地址空间和资源，而进程之间的地址空间和资源是相互独立的。</li>
<li>影响关系： 一个进程崩溃后，在保护模式下不会对其它进程产生影响，但是一个线程崩溃整个进程都会crash，所以多进程要比多线程健壮</li>
<li>执行过程：每个独立的进程有程序运行入口、顺序执行序列和程序出口，但是线程不能独立运行，必须依存在应用程序中，有应用程序提供多个线程执行控制，两者均可并发执行。</li>
</ul>

<h2 id="toc_5">java中守护进程和本地进程的区别</h2>

<p>java中线程分为两种：守护线程和用户线程</p>

<p>任何线程都可以设置为守护线程和用户线程，通过方法 Thread.setDaemon(boolean on) true则把该线程设置为守护线程，反之则为用户线程(这个方法必须在线程 start 之前设置才会生效)</p>

<p>两者区别：</p>

<ul>
<li>当主线程结束时，结束其余的子线程（守护线程）自动关闭，就免去了还要继续关闭子线程的麻烦。</li>
<li>在 Daemon 线程中产生的新线程也是 Daemon 的。</li>
<li>Java自带的多线程框架，比如ExecutorService，会将守护线程转换为用户线程，所以如果要使用后台线程就不能用Java的线程池。</li>
<li>不是所有的应用都可以分配给Daemon线程来进行服务，比如读写操作或者计算逻辑。因为在Daemon Thread还没来的及进行操作时，虚拟机可能已经退出了。</li>
</ul>

<h2 id="toc_6">通常线程有哪几种使用方式?</h2>

<ul>
<li>Runnable 不带返回值的任务</li>
<li>Callable 带返回值的任务</li>
<li>Thread</li>
</ul>

<p>实现 Runnable 和 Callable 接口的类只能当做一个可以在线程中运行的任务，不是真正意义上的线程，因此最后还需要通过 Thread 来调用。可以说任务是通过线程驱动从而执行的。</p>

<h2 id="toc_7">死锁和活锁的区别，死锁与饥饿的区别</h2>

<p>死锁与活锁请参见 <a href="16017068710077.html">死锁与活锁</a></p>

<p>饥饿：一个或者多个线程因为种种原因无法获得所需的资源，导致一直无法执行的状态</p>

<p>java中导致饥饿的原因：</p>

<ul>
<li>高优先级的线程侵占所有低优先级线程的 CPU 时间</li>
<li>线程被永久堵塞在一个等待进入同步块的状态，因为其他线程总是能在它之前持续的对该同步代码块进行访问</li>
<li>线程在等待一个本身也处于永久等待完成的对象（比如调用这个对象的 wait 方法，因为其它线程总是被持续的获得唤醒</li>
</ul>

<h2 id="toc_8">怎么唤醒一个阻塞的线程？</h2>

<p>如果线程是因为调用了wait()、sleep()或者join()方法而导致的阻塞，可以中断线程，并且通过抛出InterruptedException来唤醒它；如果线程遇到了IO阻塞，无能为力，因为IO是操作系统实现的，Java代码并没有办法直接接触到操作系统。</p>

<h2 id="toc_9">不可变对象对多线程有什么帮助</h2>

<p>不可变对象保证了对象的内存可见性，对不可变对象的读取不需要进行额外的同步操作，提升了代码的执行效率</p>

<h2 id="toc_10">什么是线程的上下文切换</h2>

<p>多线程的上下文切换是指cpu控制权由一个正在运行的线程切换到另一个就绪并等待获取 cpu 执行权的线程的过程。cpu在切换线程的时候，要保留上一个线程的车祸现场，一遍下次轮到它的时候，可以恢复车祸线程方便警察叔叔执行公务。</p>

<h2 id="toc_11">java 中用到的线程调度算法是什么</h2>

<p>操作系统中线程调度是系统为线程分配 cpu 资源的过程，主要调度方式有两种</p>

<ul>
<li><p>协同式线程调度<br/>
使用协同式线程调度的多线程系统，线程执行的时间由线程本身来控制，线 程把自己的工作执行完之后，要主动通知系统切换到另外一个线程上。</p></li>
<li><p>抢占式线程调度<br/>
使用抢占式线程调度的多线程系统，每个线程执行的时间以及是否切换都由 系统决定。</p></li>
</ul>

<p>Java 中的线程是通过映射到操作系统的原生线程上实现的，所以线程的调度最终取决于操作系统，而操作系统级别，OS 是以抢占式调度线程，我们可以认为线程是抢占式的。Java 虚拟机采用抢占式调度模型，是指优先让可运行池中优先级高的线程占用 CPU，如果可运行池中的线程优先级相同，那么就随机选择一个线程，使其占用 CPU。处于运行状态的线程会一直运行，直至它不得不放弃 CPU。而且操作系统中线程的优先级有时并不能和 Java 中的一一对应，所以 Java 优先级并不是特别靠谱。</p>

<h2 id="toc_12">Thread.sleep(0)的作用是什么？</h2>

<p>由于Java采用抢占式的线程调度算法，因此可能会出现某条线程常常获取到CPU控制权的情况，为了让某些优先级比较低的线程也能获取到CPU控制权，可以使用Thread.sleep(0)手动触发一次操作系统分配时间片的操作，这也是平衡CPU控制权的一种操作。</p>

<h2 id="toc_13">Runnable接口和Callable接口的区别？</h2>

<p>Runnable接口中的run()方法的返回值是void，它做的事情只是纯粹地去执行run()方法中的代码而已；Callable接口中的call()方法是有返回值的，是一个泛型，和Future、FutureTask配合可以用来获取异步执行的结果。</p>

<p>这其实是很有用的一个特性，因为多线程相比单线程更难、更复杂的一个重要原因就是因为多线程充满着未知性，某条线程是否执行了？某条线程执行了多久？某条线程执行的时候我们期望的数据是否已经赋值完毕？无法得知，我们能做的只是等待这条多线程的任务执行完毕而已。而Callable+Future/FutureTask却可以获取多线程运行的结果，可以在等待时间太长没获取到需要的数据的情况下取消该线程的任务，真的是非常有用。</p>

<h2 id="toc_14">什么是线程组，为什么在 Java 中不推荐使用?</h2>

<p>线程组 ThreadGroup 对象中比较有用的方法是 stop、resume、suspend 等 方法，由于这几个方法会导致线程的安全问题(主要是死锁问题)，已经被官方 废弃掉了，所以线程组本身的应用价值就大打折扣了。</p>

<p>线程组 ThreadGroup 不是线程安全的，这在使用过程中获取的信息并不全 是及时有效的，这就降低了它的统计使用价值。</p>

<h2 id="toc_15">多线程同步和互斥有几种实现方法？</h2>

<p>线程同步是指线程之间所具有的一种制约关系，一个线程的执行依赖另一个 线程的消息，当它没有得到另一个线程的消息时应等待，直到消息到达时才被唤醒。</p>

<p>线程互斥是指对于共享的进程系统资源，在各单个线程访问时的排它性。当有若干个线程都要使用某一共享资源时，任何时刻最多只允许一个线程去使用，其它要使用该资源的线程必须等待，直到占用资源者释放该资源。线程互斥可以 看成是一种特殊的线程同步。</p>

<p>线程间的同步方法大体可分为两类: 用户模式和内核模式。顾名思义，内核模式就是指利用系统内核对象的单一性来进行同步，使用时需要切换内核态与用户态，而用户模式就是不需要切换到内核态，只在用户态完成操作。</p>

<p>用户模式下的方法有: 原子操作(例如一个单一的全局变量)，临界区。内核模式下的方法有: 事件，信号量，互斥量。</p>

<h2 id="toc_16">什么是java内存模型？</h2>

<p>详情请参见 ：<a href="16019058386155.html">java 内存模型之初识</a><br/>
<a href="16020511412975.html">java内存模型之 volatile 同步原语详解</a><br/>
<a href="16082130374121.html">java内存模型之 final 同步原语详解</a><br/>
<a href="16005248987908.html">java内存模型之 Synchronized 同步原语详解</a></p>

<h2 id="toc_17">volatile关键字的作用？</h2>

<p>理解volatile关键字的作用的前提是要理解Java内存模型，这里就不讲Java内存模型了，可以参见<a href="16019058386155.html">java 内存模型</a><br/><br/>
这里简要说明一下 volatile关键字的作用，主要有两个：</p>

<ul>
<li><p>多线程主要围绕可见性和原子性两个特性而展开，使用volatile关键字修饰的变量，保证了其在多线程之间的可见性，即每次读取到volatile变量，一定是最新的数据</p></li>
<li><p>代码底层执行不像我们看到的高级语言----Java程序这么简单，它的执行是Java代码--&gt;字节码--&gt;根据字节码执行对应的C/C++代码--&gt;C/C++代码被编译成汇编语言--&gt;和硬件电路交互，现实中，为了获取更好的性能JVM可能会对指令进行重排序，多线程下可能会出现一些意想不到的问题。使用volatile则会对禁止语义重排序，当然这也一定程度上降低了代码执行效率。</p></li>
</ul>

<p>从实践角度而言，volatile的一个重要作用就是和CAS结合，保证了原子性，详细的可以参见java.util.concurrent.atomic包下的类，比如AtomicInteger。</p>

<h2 id="toc_18">什么是 CAS</h2>

<p>详情请参见 ：<a href="15874857577712.html">原子操作CAS 无锁操作</a></p>

<h2 id="toc_19">ThreadLocal 有什么作用</h2>

<p>简单说ThreadLocal就是一种以空间换时间的做法，在每个Thread里面维护了一个以开地址法实现的ThreadLocal.ThreadLocalMap，把数据进行隔离，数据不共享，自然就没有线程安全方面的问题了。详情请参见：<a href="ThreadLocal.html">ThreadLocal源码分析以及使用</a></p>

<h2 id="toc_20">为什么 wait, notify 和 notifyAll 这些方法不在 thread 类里面?</h2>

<p>JAVA 提供的锁是对象级的而不是线程级的，每个对象都有锁，通过线程获 得。如果线程需要等待某些锁那么调用对象中的 wait()方法就有意义了。如果 wait()方法定义在Thread类中，线程正在等待的是哪个锁就不明显了。简单的说， 由于 wait，notify 和 notifyAll 都是锁级别的操作，所以把他们定义在 Object 类中 因为锁属于对象。</p>

<h3 id="toc_21">为什么 wait 和 notify 方法要在同步块中调用?</h3>

<p>主要是因为 Java API 强制要求这样做，如果你不这么做，你的代码会抛出 IllegalMonitorStateException 异常。</p>

<h2 id="toc_22">怎么检测一个线程是否持有对象监视器</h2>

<p>Thread 类提供了一个 holdsLock(Object obj)方法，当且仅当对象obj的监视器被当前线程持有的时候才会返回true</p>

<h2 id="toc_23">同步方法和同步块，哪个是更好的选择</h2>

<p>同步块，这意味着同步块之外的代码是异步执行的，这比同步整个方法更提升代码的效率。请知道一条原则：同步的范围越小越好。</p>

<p>借着这一条，我额外提一点，虽说同步的范围越少越好，但是在Java虚拟机中还是存在着一种叫做锁粗化的优化方法，这种方法就是把同步范围变大。这是有用的，比方说StringBuffer，它是一个线程安全的类，自然最常用的append()方法是一个同步方法，我们写代码的时候会反复append字符串，这意味着要进行反复的加锁-&gt;解锁，这对性能不利，因为这意味着Java虚拟机在这条线程上要反复地在内核态和用户态之间进行切换，因此Java虚拟机会将多次append方法调用的代码进行一个锁粗化的操作，将多次的append的操作扩展到append方法的头尾，变成一个大的同步块，这样就减少了加锁--&gt;解锁的次数，有效地提升了代码执行的效率。</p>

<h2 id="toc_24">乐观锁和悲观锁的理解及如何实现，有哪些实现方式?</h2>

<ul>
<li>悲观锁:总是假设最坏的情况，每次去拿数据的时候都认为别人会修改，所 以每次在拿数据的时候都会上锁，这样别人想拿这个数据就会阻塞直到它拿到锁。 Java 里面的同步原语 synchronized 关键字的实现是悲观锁。</li>
<li>乐观锁:顾名思义，就是很乐观，每次去拿数据的时候都认为别人不会修改， 所以不会上锁，但是在更新的时候会判断一下在此期间别人有没有去更新这个数 据，可以使用版本号等机制。在 Java 中 j 原子变量类就是使用了乐观锁的一种实现方式 CAS 实现的。 乐观锁的实现方式:
<ul>
<li>使用版本标识来确定读到的数据与提交时的数据是否一致。提交 后修改版本标识，不一致时可以采取丢弃和再次尝试的策略。</li>
<li>java 中的 Compare and Swap 即 CAS ，当多个线程尝试使用 CAS 同时更新同一个变量时，只有其中一个线程能更新变量的值，而其它线程 都失败，失败的线程并不会被挂起，而是被告知这次竞争中失败，并可以 再次尝试。</li>
</ul></li>
</ul>

<h2 id="toc_25">什么是线程安全</h2>

<p>如果你的代码在多线程下执行和在单线程下执行永远都能获得一样的结果，那么你的代码就是线程安全的。</p>

<h2 id="toc_26">线程类的构造方法、静态块是被哪个线程调用的？</h2>

<p>线程类的构造方法、静态块是被new这个线程类所在的线程所调用的，而run方法里面的代码才是被线程自身所调用的。</p>

<p>假设Thread2中new了Thread1，main函数中new了Thread2，那么：</p>

<ul>
<li>Thread2的构造方法、静态块是main线程调用的，Thread2的run()方法是Thread2自己调用的</li>
<li>Thread1的构造方法、静态块是Thread2调用的，Thread1的run()方法是Thread1自己调用的</li>
</ul>

<h2 id="toc_27">Java中如何获取到线程dump文件？</h2>

<p>死循环、死锁、阻塞、页面打开慢等问题，打线程dump是最好的解决问题的途径。所谓线程dump也就是线程堆栈，获取到线程堆栈有两步：</p>

<ul>
<li>获取到线程的pid，可以通过使用jps命令，在Linux环境下还可以使用ps -ef | grep java</li>
<li>打印线程堆栈，可以通过使用jstack pid命令，在Linux环境下还可以使用kill -3 pid</li>
</ul>

<p>另外提一点，Thread类提供了一个getStackTrace()方法也可以用于获取线程堆栈。这是一个实例方法，因此此方法是和具体线程实例绑定的，每次获取获取到的是具体某个线程当前运行的堆栈，</p>

<h2 id="toc_28">一个线程如果出现了运行时异常会怎么样？</h2>

<p>如果这个线程的异常没有被捕获的话，这个线程会停止执行，如果这个线程持有某个对象的监视器，那么这个对象监视器也会被立即释放</p>

<h2 id="toc_29">如何在两个线程之间共享数据？</h2>

<p>通过共享对象，然后通过wait/notify/notifyAll、await/signal/signalAll进行唤起和等待，比方说阻塞队列BlockingQueue就是为线程之间共享数据而设计的。</p>

<h2 id="toc_30">sleep方法和wait方法有什么区别 ？</h2>

<p>这个问题常问，sleep方法和wait方法都可以用来放弃CPU一定的时间，不同点在于如果线程持有某个对象的监视器，sleep方法不会放弃这个对象的监视器，wait方法会放弃这个对象的监视器</p>

<h2 id="toc_31">Java中interrupted 和 isInterrupted方法的区别？</h2>

<p>interrupt方法用于中断线程。调用该方法的线程的状态为将被置为”中断”状态。<br/>
注意：线程中断仅仅是置线程的中断状态位，不会停止线程。需要用户自己去监视线程的状态为并做处理。支持线程中断的方法（也就是线程中断后会抛出interruptedException的方法）就是在监视线程的中断状态，一旦线程的中断状态被置为“中断状态”，就会抛出中断异常。</p>

<p>interrupted查询当前线程的中断状态，并且清除原状态。如果一个线程被中断了，第一次调用interrupted则返回true，第二次和后面的就返回false了。</p>

<p>isInterrupted仅仅是查询当前线程的中断状态</p>

<h2 id="toc_32">单例模式的线程安全性？</h2>

<p><a href="15896032913368.html">单例模式</a></p>

<h2 id="toc_33">Hashtable的size()方法中明明只有一条语句&quot;return count&quot;，为什么还要做同步？</h2>

<p>同一时间只能有一条线程执行固定类的同步方法，但是对于类的非同步方法，可以多条线程同时访问。所以，这样就有问题了，可能线程A在执行Hashtable的put方法添加数据，线程B则可以正常调用size()方法读取Hashtable中当前元素的个数，那读取到的值可能不是最新的，可能线程A添加了完了数据，但是没有对size++，线程B就已经读取size了，那么对于线程B来说读取到的size一定是不准确的。而给size()方法加了同步之后，意味着线程B调用size()方法只有在线程A调用put方法完毕之后才可以调用，这样就保证了线程安全性</p>

<h2 id="toc_34">介绍一下Executor 框架</h2>

<p>Executor 框架是一个根据一组执行策略调用，调度，执行和控制的异步任务 的框架。它管理多个异步任务的执行，而无需程序员显式的管理线程的生命周期。这里的异步是指多个任务的执行互不干扰，不需要进行同步操作。</p>

<p>Executor主要有三种: </p>

<ul>
<li>CachedThreadPool: 一个任务创建一个线程； </li>
<li>FixedThreadPool: 所有任务只能使用固定大小的线程； </li>
<li>SingleThreadExecutor: 相当于大小为 1 的 FixedThreadPool。</li>
</ul>

<h3 id="toc_35">为什么使用 Executor 框架</h3>

<ul>
<li>第一:降低资源消耗。通过重复利用已创建的线程降低线程创建和销毁造成的消耗。</li>
<li>第二:提高响应速度。当任务到达时，任务可以不需要等到线程创建就能立 即执行。</li>
<li>第三:提高线程的可管理性。线程是稀缺资源，如果无限制地创建，不仅会消耗系统资源，还会降低系统的稳定性，使用线程池可以进行统一分配、调优和监控。</li>
</ul>

<h2 id="toc_36">Java 线程池中 submit() 和 execute()方法有什么区别?</h2>

<p>两个方法都可以向线程池提交任务，execute()方法的返回类型是 void，它定 义在 Executor 接口中。<br/>
而 submit()方法可以返回持有计算结果的 Future 对象，它定义在 ExecutorService 接口中，它扩展了 Executor 接口</p>

<h2 id="toc_37">如果你提交任务时，线程池队列已满，这时会发生什么？</h2>

<p>这里区分一下：</p>

<p>如果使用的是无界队列LinkedBlockingQueue，也就是无界队列的话，没关系，继续添加任务到阻塞队列中等待执行，因为LinkedBlockingQueue可以近乎认为是一个无穷大的队列，可以无限存放任务</p>

<p>如果使用的是有界队列比如ArrayBlockingQueue，任务首先会被添加到ArrayBlockingQueue中，ArrayBlockingQueue满了，会根据maximumPoolSize的值增加线程数量，如果增加了线程数量还是处理不过来，ArrayBlockingQueue继续满，那么则会使用拒绝策略RejectedExecutionHandler处理满了的任务，默认是AbortPolicy</p>

<h2 id="toc_38">高并发、任务执行时间短的业务怎样使用线程池？并发不高、任务执行时间长的业务怎样使用线程池？并发高、业务执行时间长的业务怎样使用线程池？</h2>

<p>这是我在并发编程网上看到的一个问题，把这个问题放在最后一个，希望每个人都能看到并且思考一下，因为这个问题非常好、非常实际、非常专业。关于这个问题，个人看法是：</p>

<p>（1）高并发、任务执行时间短的业务，线程池线程数可以设置为CPU核数+1，减少线程上下文的切换</p>

<p>（2）并发不高、任务执行时间长的业务要区分开看：</p>

<p>　　a）假如是业务时间长集中在IO操作上，也就是IO密集型的任务，因为IO操作并不占用CPU，所以不要让所有的CPU闲下来，可以加大线程池中的线程数目，让CPU处理更多的业务</p>

<p>　　b）假如是业务时间长集中在计算操作上，也就是计算密集型任务，这个就没办法了，和（1）一样吧，线程池中的线程数设置得少一些，减少线程上下文的切换</p>

<p>（3）并发高、业务执行时间长，解决这种类型任务的关键不在于线程池而在于整体架构的设计，看看这些业务里面某些数据是否能做缓存是第一步，增加服务器是第二步，至于线程池的设置，设置参考其他有关线程池的文章。最后，业务执行时间长的问题，也可能需要分析一下，看看能不能使用中间件对任务进行拆分和解耦。</p>

<h2 id="toc_39">什么是 FutureTask</h2>

<p>FutureTask表示一个异步运算的任务。FutureTask里面可以传入一个Callable的具体实现类，可以对这个异步运算的任务的结果进行等待获取、判断是否已经完成、取消任务等操作。当然，由于FutureTask也是Runnable接口的实现类，所以FutureTask 也可以放入线程池中。</p>

<h2 id="toc_40">什么是阻塞队列？阻塞队列的实现原理是什么？如何使用阻塞队列来实现生产者-消费者模型？</h2>

<p>阻塞队列是一个支持两个附加操作的队列。在队列为空时，获取元素的线程会等待队列变为非空。当队列满时，存储元素的线程会等待队列可用。</p>

<p>阻塞队列常用于生产者和消费者的场景，生产者是往队列里添加元素的线程，消费者是从队列里拿元素的线程。阻塞队列就是生产者存放元素的容器，而消费者也只从容器里拿元素。</p>

<p>JDK7提供了7个阻塞队列。分别是：</p>

<ul>
<li><p>ArrayBlockingQueue ：一个由数组结构组成的有界阻塞队列。</p></li>
<li><p>LinkedBlockingQueue ：一个由链表结构组成的有界阻塞队列。</p></li>
<li><p>PriorityBlockingQueue ：一个支持优先级排序的无界阻塞队列。</p></li>
<li><p>DelayQueue：一个使用优先级队列实现的无界阻塞队列。</p></li>
<li><p>SynchronousQueue：一个不存储元素的阻塞队列。</p></li>
<li><p>LinkedTransferQueue：一个由链表结构组成的无界阻塞队列。</p></li>
<li><p>LinkedBlockingDeque：一个由链表结构组成的双向阻塞队列。</p></li>
</ul>

<p>BlockingQueue接口是Queue的子接口，它的主要用途并不是作为容器，而是作为线程同步的的工具，因此他具有一个很明显的特性，当生产者线程试图向BlockingQueue放入元素时，如果队列已满，则线程被阻塞，当消费者线程试图从中取出一个元素时，如果队列为空，则该线程会被阻塞，正是因为它所具有这个特性，所以在程序中多个线程交替向BlockingQueue中放入元素，取出元素，它可以很好的控制线程之间的通信。</p>

<p>阻塞队列使用最经典的场景就是socket客户端数据的读取和解析，读取数据的线程不断将数据放入队列，然后解析线程不断从队列取数据解析</p>

<h2 id="toc_41">什么是 AQS</h2>

<p>详情请参见 ： <a href="15867876320249.html">AQS--AbstractQueuedSynchronizer 详解</a></p>

<h2 id="toc_42">CyclicBarrier和CountDownLatch的区别？</h2>

<p>两个看上去有点像的类，都在java.util.concurrent下，都可以用来表示代码运行到某个点上，二者的区别在于：</p>

<ul>
<li><p>CyclicBarrier的某个线程运行到某个点上之后，该线程即停止运行，直到所有的线程都到达了这个点，所有线程才重新运行；CountDownLatch则不是，某线程运行到某个点上之后，只是给某个数值-1而已，该线程继续运行</p></li>
<li><p>CyclicBarrier只能唤起一个任务，CountDownLatch可以唤起多个任务</p></li>
<li><p>CyclicBarrier可重用，CountDownLatch不可重用，计数值为0该CountDownLatch就不可再用了</p></li>
</ul>

<h2 id="toc_43">Semaphore 有什么作用</h2>

<p>详情请参见 <a href="16017007365424.html">Semaphore</a></p>

<h2 id="toc_44">写时复制容器可以用于什么应用场景</h2>

<p>CopyOnWrite 并发容器用于对于绝大部分访问都是读，且只是偶尔写的并发场景。比如白名单，黑名单，商品类目的访问和更新场景。</p>

<p>CopyOnWriteArrayList(免锁容器) 的好处之一是当多个迭代器同时遍历和修改这个列表时，不会抛出ConcurrentModificationException。</p>

<p>在CopyOnWriteArrayList中，写入将导致创建整个底层数组的副本，而源数组将保留在原地，使得复制的数组在被修改时，读取操作可以安全地执行。</p>

<ul>
<li><p>由于写操作的时候，需要拷贝数组，会消耗内存，如果原数组的内容比较多的情况下，可能导致young gc或者full gc；</p></li>
<li><p>不能用于实时读的场景，像拷贝数组、新增元素都需要时间，所以调用一个set操作后，读取到数据可能还是旧的,虽然CopyOnWriteArrayList 能做到最终一致性,但是还是没法满足实时性要求；</p></li>
</ul>

<p>CopyOnWriteArrayList透露的思想：读写分离，读和写分开、最终一致性、使用另外开辟空间的思路，来解决并发冲突</p>

<h2 id="toc_45">生产者消费者模型的作用是什么？</h2>

<ul>
<li>通过平衡生产者的生产能力和消费者的消费能力来提升整个系统的运行效率，这是生产者消费者模型最重要的作用</li>
<li>解耦，这是生产者消费者模型附带的作用，解耦意味着生产者和消费者之间的联系少，联系越少越可以独自发展而不需要收到相互的制约</li>
</ul>

<h2 id="toc_46">什么是可重入锁(ReentrantLock)?谈谈它的实现</h2>

<p>线程可以重复进入任何一个它已经拥有的锁所同步着的代码块， synchronized、ReentrantLock 都是可重入的锁。在实现上，就是线程每次获取锁时判定如果获得锁的线程是它自己时，简单将计数器累积即可，每释放一次锁， 进行计数器累减，直到计算器归零，表示线程已经彻底释放锁。</p>

<h2 id="toc_47">synchronized 和 ReentrantLock 的对比</h2>

<p>synchronized是和if、else、for、while一样的关键字，ReentrantLock是类，这是二者的本质区别。既然ReentrantLock是类，那么它就提供了比synchronized更多更灵活的特性，可以被继承、可以有方法、可以有各种各样的类变量，ReentrantLock比synchronized的扩展性体现在几点上：</p>

<ul>
<li>ReentrantLock可以对获取锁的等待时间进行设置，这样就避免了死锁</li>
<li>ReentrantLock可以获取各种锁的信息</li>
<li>ReentrantLock可以灵活地实现多路通知</li>
</ul>

<h2 id="toc_48">说一下 synchronized 的实现原理</h2>

<p>详情请参见： <a href="16005248987908.html">Synchronized关键字解析</a></p>

<h2 id="toc_49">什么是自旋？</h2>

<p>很多synchronized里面的代码只是一些很简单的代码，执行时间非常快，此时等待的线程都加锁可能是一种不太值得的操作，因为线程阻塞涉及到用户态和内核态切换的问题。既然synchronized里面的代码执行得非常快，不妨让等待锁的线程不要被阻塞，而是在synchronized的边界做忙循环，这就是自旋。如果做了多次忙循环发现还没有获得锁，再阻塞，这样可能是一种更好的策略。</p>

<h2 id="toc_50">ReadWriteLock 是什么</h2>

<p>首先我们得直到 ReentrantLock 的局限性。</p>

<p>如果使用ReentrantLock，可能本身是为了防止线程A在写数据、线程B在读数据造成的数据不一致，但这样，如果线程C在读数据、线程D也在读数据，读数据是不会改变数据的，没有必要加锁，但是还是加锁了，降低了程序的性能。</p>

<p>因为这个，才诞生了读写锁 ReadWriteLock。ReadWriteLock 是一个读写锁接口，ReentrantReadWriteLock 是 ReadWriteLock 接口的一个具体实现，实现了读写的分离，读锁是共享的，写锁是独占的，读和读之间不会互斥，读和写、写和读、写和写之间才会互斥，提升了读写的性能。</p>

<h2 id="toc_51">ConcurrentHashMap的并发度是什么？</h2>

<p>ConcurrentHashMap把实际map划分成若干部分来实现它的可扩展性和线程安全。这种划分是使用并发度获得的，它是ConcurrentHashMap类构造函数的一个可选参数，默认值为16，这样在多线程情况下就能避免争用。</p>

<p>在JDK8后，它摒弃了Segment（锁段）的概念，而是启用了一种全新的方式实现,利用CAS算法。同时加入了更多的辅助变量来提高并发度，具体内容还是查看源码吧。</p>

<h2 id="toc_52">Linux环境下如何查找哪个线程使用CPU最长</h2>

<ul>
<li>获取项目的pid，jps 或者 ps -ef | grep java</li>
<li>top -H -p pid，顺序不能改变</li>
</ul>

<p>这样就可以打印出当前的项目，每条线程占用CPU时间的百分比。</p>

<p>注意这里打出的是LWP，也就是操作系统原生线程的线程号。</p>

<p>使用&quot;top -H -p pid&quot; + &quot;jps pid&quot; 可以很容易地找到某条占用CPU高的线程的线程堆栈，从而定位占用CPU高的原因，一般是因为不当的代码操作导致了死循环。</p>

<p>最后提一点，&quot;top -H -p pid&quot;打出来的LWP是十进制的，&quot;jps pid&quot;打出来的本地线程号是十六进制的，转换一下，就能定位到占用CPU高的线程的当前线程堆栈了。</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[spring 面试题]]></title>
    <link href="http://www.throne4j.com/16078475422536.html"/>
    <updated>2020-12-13T16:19:02+08:00</updated>
    <id>http://www.throne4j.com/16078475422536.html</id>
    <content type="html"><![CDATA[
<h2 id="toc_0">什么是控制反转(IOC)？什么是依赖注入？</h2>

<h2 id="toc_1">BeanFactory 和 ApplicationContext 有什么区别？</h2>

<h2 id="toc_2">请解释 Spring Bean 的生命周期？</h2>

<h2 id="toc_3">请举例解释@Autowired 注解？</h2>

<h2 id="toc_4">Spring 框架中有哪些不同类型的事件？</h2>

<ul>
<li><p>1.上下文更新事件ContextRefreshedEvent： 该事件会在ApplicationContext 被初始化或者更新时发布。也可以在调用 ConfigurableApplicationContext 接口中的 refresh()方法时被触发。</p></li>
<li><p>2.上下文开始事件ContextStartedEvent： 当容器调用ConfigurableApplicationContext的 Start()方法开始/重新开始容器时触发该事件。</p></li>
<li><p>3.上下文停止事件ContextStoppedEvent： 当容器调用 ConfigurableApplicationContext的 Stop()方法停止容器时触发该事件。</p></li>
<li><p>4.上下文关闭事件ContextClosedEvent： 当 ApplicationContext 被 关闭时触发该事件。容器被关闭时，其管理的所有单例 Bean 都被销毁。</p></li>
<li><p>5.请求处理事件RequestHandledEvent： 在 Web 应用中，当一个 http 请求（request）结束触发该事件。除了上面介绍的事件以外，还可以通过扩展 ApplicationEvent 类来开发自定义的事件。</p></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[]]></title>
    <link href="http://www.throne4j.com/16077870652991.html"/>
    <updated>2020-12-12T23:31:05+08:00</updated>
    <id>http://www.throne4j.com/16077870652991.html</id>
    <content type="html"><![CDATA[

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Netty 之 Channel]]></title>
    <link href="http://www.throne4j.com/16075310570444.html"/>
    <updated>2020-12-10T00:24:17+08:00</updated>
    <id>http://www.throne4j.com/16075310570444.html</id>
    <content type="html"><![CDATA[
<p><figure><img src="media/16075310570444/NioSocketChannel.jpg" alt="NioSocketChannel"/><figcaption>NioSocketChannel</figcaption></figure><br/>
<figure><img src="media/16075310570444/NioServerSocketChannel.jpg" alt="NioServerSocketChannel"/><figcaption>NioServerSocketChannel</figcaption></figure></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Netty 之 EventLoop 相关源码分析]]></title>
    <link href="http://www.throne4j.com/16074361744050.html"/>
    <updated>2020-12-08T22:02:54+08:00</updated>
    <id>http://www.throne4j.com/16074361744050.html</id>
    <content type="html"><![CDATA[
<p><figure><img src="media/16074361744050/NioEventLoop.jpg" alt="NioEventLoop"/><figcaption>NioEventLoop</figcaption></figure><br/>
<figure><img src="media/16074361744050/NioEventLoopGroup.jpg" alt="NioEventLoopGroup"/><figcaption>NioEventLoopGroup</figcaption></figure></p>

<h2 id="toc_0">EventLoopGroup 和 EventLoop 代码分析</h2>

<pre><code class="language-text">EventLoopGroup group = new NioEventLoopGroup();
</code></pre>

<p>执行这行代码时会发生什么?，由 NioEventLoopGroup 开始，一路调用，到达 MultithreadEventLoopGroup，如果没有指定创建的线程数量，则默认创建的线程个数为 DEFAULT_EVENT_LOOP_THREADS，该数值为:处理器数量 x2。</p>

<pre><code class="language-java">private static final int DEFAULT_EVENT_LOOP_THREADS;

static {
    DEFAULT_EVENT_LOOP_THREADS = Math.max(1, SystemPropertyUtil.getInt(
            &quot;io.netty.eventLoopThreads&quot;, NettyRuntime.availableProcessors() * 2));
}

protected MultithreadEventLoopGroup(int nThreads, Executor executor, Object... args) {
    super(nThreads == 0 ? DEFAULT_EVENT_LOOP_THREADS : nThreads, executor, args);
}
</code></pre>

<p>最终由 MultithreadEventExecutorGroup 实例化</p>

<p>由此可见，每个 NioEventLoop 的执行器为 ThreadPerTaskExecutor，ThreadPerTaskExecutor 实现了 Executor 接口，并会在 execute 方法中启动真正的线程，但是要和 NioEventLoop 的线 程挂钩则在 SingleThreadEventExecutor 的 doStartThread 方法里</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[spring AOP]]></title>
    <link href="http://www.throne4j.com/16073961967770.html"/>
    <updated>2020-12-08T10:56:36+08:00</updated>
    <id>http://www.throne4j.com/16073961967770.html</id>
    <content type="html"><![CDATA[
<h2 id="toc_0">execution 表达式</h2>

<pre><code class="language-text">execution(* com.loongshawn.method..*.*(..))
</code></pre>

<p>execution 表达式主体</p>

<ul>
<li>第一个 * 表示任何返回对象</li>
<li>com.loongshawn.method 包</li>
<li>后来的 .. 表示当前包以及子包</li>
<li>第二个 * 表示类名</li>
<li>*.(..) 表示任何方法名，括号表示参数，两个点表示任何参数类型</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[redis 常见问题与解决方案]]></title>
    <link href="http://www.throne4j.com/16073594276390.html"/>
    <updated>2020-12-08T00:43:47+08:00</updated>
    <id>http://www.throne4j.com/16073594276390.html</id>
    <content type="html"><![CDATA[
<p>考虑到绝大部分写业务的程序员，在实际开发中使用 Redis 的时候，只会 Set Value 和 Get Value 两个操作，对 Redis 整体缺乏一个认知。</p>

<p>本文围绕以下几点进行阐述：</p>

<ul>
<li><p>为什么使用 Redis</p></li>
<li><p>使用 Redis 有什么缺点</p></li>
<li><p>单线程的 Redis 为什么这么快</p></li>
<li><p>Redis 的数据类型，以及每种数据类型的使用场景</p></li>
<li><p>Redis 的过期策略以及内存淘汰机制</p></li>
<li><p>Redis 和数据库双写一致性问题</p></li>
<li><p>如何应对缓存穿透和缓存雪崩问题</p></li>
<li><p>如何解决 Redis 的并发竞争 Key 问题</p></li>
</ul>

<h2 id="toc_0">为什么使用 Redis？</h2>

<p>我觉得在项目中使用 Redis，主要是从两个角度去考虑：性能和并发。</p>

<p>当然，Redis 还具备可以做分布式锁等其他功能，但是如果只是为了分布式锁这些其他功能，完全还有其他中间件，如 ZooKpeer 等代替，并不是非要使用 Redis。因此，这个问题主要从性能和并发两个角度去答。</p>

<ul>
<li>性能：</li>
</ul>

<p>如下图所示，我们在碰到需要执行耗时特别久，且结果不频繁变动的 SQL，就特别适合将运行结果放入缓存。这样，后面的请求就去缓存中读取，使得请求能够迅速响应。</p>

<p><figure><img src="media/16073594276390/16073594942888.jpg" alt=""/></figure></p>

<ul>
<li>并发：<br/>
如下图所示，在大并发的情况下，所有的请求直接访问数据库，数据库会出现连接异常。</li>
</ul>

<p><figure><img src="media/16073594276390/16073595299526.jpg" alt=""/></figure></p>

<p>这个时候，就需要使用 Redis 做一个缓冲操作，让请求先访问到 Redis，而不是直接访问数据库。</p>

<h2 id="toc_1">使用 Redis 有什么缺点？</h2>

<p>大家用 Redis 这么久，这个问题是必须要了解的，基本上使用 Redis 都会碰到一些问题，常见的也就几个。</p>

<p>回答主要是四个问题：</p>

<ul>
<li>缓存和数据库双写一致性问题</li>
<li>缓存雪崩问题</li>
<li>缓存击穿问题</li>
<li>缓存的并发竞争问题</li>
</ul>

<p>这四个问题，我个人觉得在项目中是常遇见的，具体解决方案，后文给出。</p>

<h2 id="toc_2">单线程的 Redis 为什么这么快？</h2>

<p>这个问题是对 Redis 内部机制的一个考察。根据我的面试经验，很多人都不知道 Redis 是单线程工作模型。所以，这个问题还是应该要复习一下的。</p>

<p>回答主要是以下三点：</p>

<ul>
<li>纯内存操作</li>
<li>单线程操作，避免了频繁的上下文切换</li>
<li>采用了非阻塞 I/O 多路复用机制</li>
</ul>

<p>多路复用在 Redis 中的应用，如图所示：</p>

<p><figure><img src="media/16073594276390/16073599141241.jpg" alt=""/></figure></p>

<p>简单来说，就是我们的 redis-client 在操作的时候，会产生具有不同事件类型的 Socket。</p>

<p>在服务端，有一段 I/O 多路复用程序，将其置入队列之中。然后，文件事件分派器，依次去队列中取，转发到不同的事件处理器中。</p>

<p>需要说明的是，这个 I/O 多路复用机制，Redis 还提供了 select、epoll、evport、kqueue 等多路复用函数库，大家可以自行去了解。</p>

<h2 id="toc_3">Redis 的数据类型以及每种数据类型的使用场景</h2>

<p>是不是觉得这个问题很基础？我也这么觉得。然而根据面试经验发现，至少百分之八十的人答不上这个问题。</p>

<p>建议，在项目中用到后，再类比记忆，体会更深，不要硬记。基本上，一个合格的程序员，五种类型都会用到。</p>

<ul>
<li>String</li>
</ul>

<p>这个没啥好说的，最常规的 set/get 操作，Value 可以是 String 也可以是数字。一般做一些复杂的计数功能的缓存。</p>

<ul>
<li>Hash</li>
</ul>

<p>这里 Value 存放的是结构化的对象，比较方便的就是操作其中的某个字段。</p>

<p>我在做单点登录的时候，就是用这种数据结构存储用户信息，以 CookieId 作为 Key，设置 30 分钟为缓存过期时间，能很好的模拟出类似 Session 的效果。</p>

<ul>
<li>List</li>
</ul>

<p>使用 List 的数据结构，可以做简单的消息队列的功能。另外还有一个就是，可以利用 lrange 命令，做基于 Redis 的分页功能，性能极佳，用户体验好。</p>

<ul>
<li>Set</li>
</ul>

<p>因为 Set 堆放的是一堆不重复值的集合。所以可以做全局去重的功能。为什么不用 JVM 自带的 Set 进行去重？<br/>
因为我们的系统一般都是集群部署，使用 JVM 自带的 Set，比较麻烦，难道为了一个做一个全局去重，再起一个公共服务，太麻烦了。</p>

<p>另外，就是利用交集、并集、差集等操作，可以计算共同喜好，全部的喜好，自己独有的喜好等功能。</p>

<ul>
<li>Sorted Set</li>
</ul>

<p>Sorted Set多了一个权重参数 Score，集合中的元素能够按 Score 进行排列。</p>

<p>可以做排行榜应用，取 TOP N 操作。Sorted Set 可以用来做延时任务。最后一个应用就是可以做范围查找。</p>

<h2 id="toc_4">Redis 的过期策略以及内存淘汰机制</h2>

<p>这个问题相当重要，到底 Redis 有没用到家，这个问题就可以看出来。</p>

<p>比如你 Redis 只能存 5G 数据，可是你写了10G，那会删 5G 的数据。怎么删的，这个问题思考过么？</p>

<p>还有，你的数据已经设置了过期时间，但是时间到了，内存占用率还是比较高，有思考过原因么?</p>

<h3 id="toc_5">Redis 采用的是定期删除+惰性删除策略。</h3>

<p>为什么不用定时删除策略</p>

<p>定时删除，用一个定时器来负责监视 Key，过期则自动删除。虽然内存及时释放，但是十分消耗 CPU 资源。</p>

<p>在大并发请求下，CPU 要将时间应用在处理请求，而不是删除 Key，因此没有采用这一策略。</p>

<p>定期删除 + 惰性删除是如何工作</p>

<p>定期删除，Redis 默认每个 100ms 检查，是否有过期的 Key，有过期 Key 则删除。<br/>
需要说明的是，Redis 不是每个 100ms 将所有的 Key 检查一次，而是随机抽取进行检查(如果每隔 100ms，全部 Key 进行检查，Redis 岂不是卡死)。</p>

<p>因此，如果只采用定期删除策略，会导致很多 Key 到时间没有删除。于是，惰性删除派上用场。<br/>
也就是说在你获取某个 Key 的时候，Redis 会检查一下，这个 Key 如果设置了过期时间，那么是否过期了？如果过期了此时就会删除。</p>

<p>采用定期删除+惰性删除就没其他问题了么?</p>

<p>不是的，如果定期删除没删除 Key。然后你也没即时去请求 Key，也就是说惰性删除也没生效。这样，Redis的内存会越来越高。那么就应该采用内存淘汰机制。</p>

<p>在 redis.conf 中有一行配置：</p>

<pre><code class="language-text"># maxmemory-policy volatile-lru
</code></pre>

<p>该配置就是配内存淘汰策略的(什么，你没配过？好好反省一下自己)：</p>

<ul>
<li><p>noeviction：当内存不足以容纳新写入数据时，新写入操作会报错。应该没人用吧。</p></li>
<li><p>allkeys-lru：当内存不足以容纳新写入数据时，在键空间中，移除最近最少使用的 Key。推荐使用，目前项目在用这种。</p></li>
<li><p>allkeys-random：当内存不足以容纳新写入数据时，在键空间中，随机移除某个 Key。应该也没人用吧，你不删最少使用 Key，去随机删。</p></li>
<li><p>volatile-lru：当内存不足以容纳新写入数据时，在设置了过期时间的键空间中，移除最近最少使用的 Key。这种情况一般是把 Redis 既当缓存，又做持久化存储的时候才用。不推荐。</p></li>
<li><p>volatile-random：当内存不足以容纳新写入数据时，在设置了过期时间的键空间中，随机移除某个 Key。依然不推荐。</p></li>
<li><p>volatile-ttl：当内存不足以容纳新写入数据时，在设置了过期时间的键空间中，有更早过期时间的 Key 优先移除。不推荐。</p></li>
</ul>

<p>PS：如果没有设置 expire 的 Key，不满足先决条件(prerequisites)；那么 volatile-lru，volatile-random 和 volatile-ttl 策略的行为，和 noeviction(不删除) 基本上一致。</p>

<h2 id="toc_6">Redis 和数据库双写一致性问题</h2>

<p>一致性问题是分布式常见问题，还可以再分为最终一致性和强一致性。数据库和缓存双写，就必然会存在不一致的问题。</p>

<p>答这个问题，先明白一个前提。就是如果对数据有强一致性要求，不能放缓存。我们所做的一切，只能保证最终一致性。</p>

<p>另外，我们所做的方案从根本上来说，只能说降低不一致发生的概率，无法完全避免。因此，有强一致性要求的数据，不能放缓存。</p>

<p>回答：首先，采取正确更新策略，先更新数据库，再删缓存。其次，因为可能存在删除缓存失败的问题，提供一个补偿措施即可，例如利用消息队列。</p>

<h2 id="toc_7">如何应对缓存穿透和缓存雪崩问题</h2>

<p>这两个问题，说句实在话，一般中小型传统软件企业，很难碰到这个问题。如果有大并发的项目，流量有几百万左右。这两个问题一定要深刻考虑。</p>

<p>缓存穿透，即黑客故意去请求缓存中不存在的数据，导致所有的请求都怼到数据库上，从而数据库连接异常。</p>

<h3 id="toc_8">缓存穿透解决方案：</h3>

<ul>
<li><p>利用互斥锁，缓存失效的时候，先去获得锁，得到锁了，再去请求数据库。没得到锁，则休眠一段时间重试。</p></li>
<li><p>采用异步更新策略，无论 Key 是否取到值，都直接返回。Value 值中维护一个缓存失效时间，缓存如果过期，异步起一个线程去读数据库，更新缓存。需要做缓存预热(项目启动前，先加载缓存)操作。</p></li>
<li><p>提供一个能迅速判断请求是否有效的拦截机制，比如，利用布隆过滤器，内部维护一系列合法有效的 Key。迅速判断出，请求所携带的 Key 是否合法有效。如果不合法，则直接返回。</p></li>
</ul>

<p>缓存雪崩，即缓存同一时间大面积的失效，这个时候又来了一波请求，结果请求都怼到数据库上，从而导致数据库连接异常。</p>

<h3 id="toc_9">缓存雪崩解决方案：</h3>

<ul>
<li><p>给缓存的失效时间，加上一个随机值，避免集体失效。</p></li>
<li><p>使用互斥锁，但是该方案吞吐量明显下降了。</p></li>
<li><p>双缓存。我们有两个缓存，缓存 A 和缓存 B。缓存 A 的失效时间为 20 分钟，缓存 B 不设失效时间。自己做缓存预热操作。</p></li>
</ul>

<p>然后细分以下几个小点：从缓存 A 读数据库，有则直接返回；A 没有数据，直接从 B 读数据，直接返回，并且异步启动一个更新线程，更新线程同时更新缓存 A 和缓存 B。</p>

<h2 id="toc_10">如何解决 Redis 的并发竞争 Key 问题</h2>

<p>这个问题大致就是，同时有多个子系统去 Set 一个 Key。这个时候大家思考过要注意什么呢？</p>

<p>需要说明一下，我提前百度了一下，发现答案基本都是推荐用 Redis 事务机制。</p>

<p>我并不推荐使用 Redis 的事务机制。因为我们的生产环境，基本都是 Redis 集群环境，做了数据分片操作。</p>

<p>你一个事务中有涉及到多个 Key 操作的时候，这多个 Key 不一定都存储在同一个 redis-server 上。因此，Redis 的事务机制，十分鸡肋。</p>

<p>如果对这个 Key 操作，不要求顺序</p>

<p>这种情况下，准备一个分布式锁，大家去抢锁，抢到锁就做 set 操作即可，比较简单。</p>

<p>如果对这个 Key 操作，要求顺序</p>

<p>假设有一个 key1，系统 A 需要将 key1 设置为 valueA，系统 B 需要将 key1 设置为 valueB，系统 C 需要将 key1 设置为 valueC。</p>

<p>期望按照 key1 的 value 值按照 valueA &gt; valueB &gt; valueC 的顺序变化。这种时候我们在数据写入数据库的时候，需要保存一个时间戳。</p>

<p>假设时间戳如下：</p>

<p>系统A key1 {valueA 3:00}<br/>
系统B key1 {valueB 3:05}<br/>
系统C key1 {valueC 3:10}</p>

<p>那么，假设这会系统 B 先抢到锁，将 key1 设置为{valueB 3:05}。接下来系统 A 抢到锁，发现自己的 valueA 的时间戳早于缓存中的时间戳，那就不做 set 操作了，以此类推。</p>

<p>其他方法，比如利用队列，将 set 方法变成串行访问也可以。总之，灵活变通。</p>

<h2 id="toc_11">总结</h2>

<p>本文对 Redis 的常见问题做了一个总结。大部分是自己在工作中遇到，以及之前面试别人的时候，爱问的一些问题。</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[epoll 高效的原理和底层机制]]></title>
    <link href="http://www.throne4j.com/16073533164404.html"/>
    <updated>2020-12-07T23:01:56+08:00</updated>
    <id>http://www.throne4j.com/16073533164404.html</id>
    <content type="html"><![CDATA[
<p>当某一进程调用 epoll_create 方法时，Linux 内核会创建一个 eventpoll 结构体，在内核 cache 里建了个红黑树用于存储以后 epoll_ctl 传来的 socket 外，还会再建立一个 rdllist 双向链表，用于存储准备就绪的事件，当 epoll_wait 调用时，仅仅观察这个 rdllist 双向链表里有没有数据即可。有数据就返回，没有数据就 sleep，等到 timeout 时间到后即使链表没数据也返回。</p>

<p>同时，所有添加到 epoll 中的事件都会与设备(如网卡)驱动程序建立回调关系，也就是 说相应事件的发生时会调用这里的回调方法。这个回调方法在内核中叫做 ep_poll_callback， 它会把这样的事件放到上面的 rdllist 双向链表中。</p>

<p>当调用 epoll_wait 检查是否有发生事件的连接时，只是检查 eventpoll 对象中的 rdllist 双向链表是否有 epitem 元素而已，如果 rdllist 链表不为空，则这里的事件复制到用户态内 存(使用共享内存提高效率)中，同时将事件数量返回给用户。因此 epoll_wait 效率会非常高，可以轻易地处理百万级别的并发连接。</p>

<h2 id="toc_0">从网卡接收数据开始</h2>

<p>一个典型的计算机结构图，计算机有 CPU、存储器、网络接口等部件组成，了解 epoll 本质的第一步，要从硬件的角度看计算机怎样接收网络数据。<br/>
<a href="16073516476681.html">Linux 网络包接收过程</a></p>

<p>了解一下计算机的结构：</p>

<p><figure><img src="media/16073533164404/16073539371404.jpg" alt="计算机结构"/><figcaption>计算机结构</figcaption></figure></p>

<p>网卡接收数据的过程：网卡收到网线传来的数据;经过硬件电路的传输;最终将数据写入到内存中的某个地址上。这个过程涉及到 DMA 传输、IO 通路选择等硬件有关的知识， 但我们只需知道，网卡会把接收到的数据写入内存，操作系统就可以去读取它们。</p>

<h3 id="toc_1">如何知道接收了数据</h3>

<p>CPU 的如何知道网络上有数据要接收? 可以详细了解一下 中断机制</p>

<h3 id="toc_2">进程阻塞</h3>

<p>问题1：进程阻塞为什么不占用 cpu 资源？<br/>
问题2：那么阻塞的原理是什么?</p>

<p>了解 epoll 本质，要从操作系统进程调度的角度来看数据接收。阻塞是进程调度的关键一环，指的是进程在等待某事件(如接收到网络数据)发生之前的等待状态，recv、select 和 epoll 都是阻塞方法。</p>

<p>我们看下如下伪代码</p>

<pre><code class="language-text">//创建 socket
int s = socket(AF_INET, SOCK_STREAM, 0);
//绑定
bind(s, ...)
//监听
listen(s, ...) 
//接受客户端连接 
int c = accept(s, ...) 
//接收客户端数据 
recv(c, ...); 
//将数据打印出来 
printf(...)
</code></pre>

<p>这是一段最基础的网络编程代码，先新建 socket 对象，依次调用 bind、listen、accept， 最后调用 recv 接收数据。recv 是个阻塞方法，当程序运行到 recv 时，它会一直等待，直到接收到数据才往下执行。</p>

<p>现代操作系统为了支持多任务，实现了进程调度的功能，会把进程分为“运行”和“等待” 等几种状态。运行状态是进程获得 cpu 使用权，正在执行代码的状态;等待状态是阻塞状态， 比如上述程序运行到 recv 时，程序会从运行状态变为等待状态，接收到数据后又变回运行 状态。操作系统会分时执行各个运行状态的进程，由于速度很快，看上去就像是同时执行多个任务。</p>

<p>现在我们假设的计算机中运行着 A、B、C 三个进程，其中进程 A 执行着上述基础网络程序， 一开始，这 3 个进程都被操作系统的工作队列所引用，处于运行状态，会分时执行。</p>

<p>当进程 A 执行到创建 socket 的语句时，操作系统会创建一个由文件系统管理的 socket 对象。这个 socket 对象包含了发送缓冲区、接收缓冲区、等待队列等成员。等待队列是个非常重要的结构，它指向所有需要等待该 socket 事件的进程</p>

<p>当程序执行到 recv 时，操作系统会将进程 A 从工作队列移动到该 socket 的等待队列中 。由于工作队列只剩下了进程 B 和 C，依据进程调度，cpu 会轮流执行这两个进 程的程序，不会执行进程 A 的程序。所以进程 A 被阻塞，不会往下执行代码，也不会占用 cpu 资源。</p>

<p>操作系统添加等待队列只是添加了对这个“等待中”进程的引用，以便在接收到数据时 获取进程对象、将其唤醒，而非直接将进程管理纳入自己之下。</p>

<p>当 socket 接收到数据后，操作系统将该 socket 等待队列上的进程重新放回到工作队列， 该进程变成运行状态，继续执行代码。也由于 socket 的接收缓冲区已经有了数据，recv 可以返回接收到的数据。</p>

<h3 id="toc_3">内核接收网络数据全过程</h3>

<p>进程在 recv 阻塞期间，计算机收到了对端传送的数据(步骤1)。数据经由网卡传送 到内存(步骤2)，然后网卡通过中断信号通知 cpu 有数据到达，cpu 执行中断程序(步骤3)。此处的中断程序主要有两项功能，先将网络数据写入到对应 socket 的接收缓冲区里 面(步骤4)，再唤醒进程 A(步骤5)，重新将进程 A 放入工作队列中</p>

<p><figure><img src="media/16073533164404/16073547682930.jpg" alt="Linux内核接收数据的过程"/><figcaption>Linux内核接收数据的过程</figcaption></figure></p>

<p>通过上面的步骤可能有小伙伴会想，操作系统是怎么直到网络数据对应于哪个 socket的呢？</p>

<p>因为一个 socket 对应着一个端口号，而网络数据包中包含了 ip 和端口的信息，内核可 以通过端口号找到对应的 socket。当然，为了提高处理速度，操作系统会维护端口号到 socket 的索引结构，以快速读取。</p>

<p>那么操作系统是如何同时监视多个 socket 的数据的呢？</p>

<p>服务端需要管理多个客户端连接，而 recv 只能监视单个 socket，因此编写操作系统的大神们开始寻找监视多个 socket 的方法。epoll 的目的就是高效的监视多个socket，但是万事都有一个循序渐进的过程，我们就从最简陋的监视多个 socket 的方法开始吧</p>

<p>假如能够预先传入一个 socket 列表，如果列表中的 socket 都没有数据，挂起进程，直到有一个 socket 收到数据，将进程唤醒。这种方法很直接，也是 select 的设计思想。</p>

<p>为方便理解，我们先复习 select 的用法。</p>

<pre><code class="language-text">int fds[] = 存放需要监听的 socket 
while(1){
    int n = select(..., fds, ...) 
    for(int i=0; i &lt; fds.count; i++) {
        if(FD_ISSET(fds[i], ...)) { 
            //fds[i]的数据处理
    
 
        }
    }
}
</code></pre>

<p>在上面的伪代码中，先准备一个数组(伪代码中的 fds)，让 fds 存放着所有需要监视的 socket。然后调用 select，如果 fds 中的所有 socket 都没有数据，select 会阻塞，直到有一个 socket 接收到数据，select 返回，唤醒进程。用户 可以遍历 fds，通过 FD_ISSET 判断具体哪个 socket 收到数据，然后做出处理。</p>

<p>select 的实现思路很直接。假如程序同时监视 sock1、sock2 和 sock3 三个 socket，那么在调用 select 之后，操作系统把进程 A 分别加入这三个 socket 的等待队列中。<br/>
<figure><img src="media/16073533164404/16073578349616.jpg" alt="调用select之后"/><figcaption>调用select之后</figcaption></figure></p>

<p>当任何一个socket 接收到数据之后，中断程序将唤起进程。所唤起进程就是将进程从所有的等待队列中移除，加入到工作队列里面</p>

<p>经由这些步骤，当进程 A 被唤醒之后，它知道至少有一个 socket 接收了数据，程序只需要遍历一遍 socket 列表，就可以得到就绪的 socket。</p>

<p>这种 Select 方式比较简单且行之有效，几乎所有的操作系统都有对应的实现。</p>

<p>但是简单的方法通常是有不足之处的，主要不足如下：</p>

<ul>
<li><p>每次调用 select 都需要将进程加入到的所有被监视 socket 的等待队列，每次唤醒都需要从每个队列中移除，都必须进行遍历。而且每次都要将整个 fds 列表传递给内核，开销比较大。正式因为遍历操作开销比较大，才会规定 select 的最大监视数量，默认32位机器只能监视 1024 个 socket</p></li>
<li><p>进程被唤醒之后，程序并不知道哪些socket收到数据，还需要再次遍历一遍 fds</p></li>
</ul>

<p>那么，有没有减少遍历的方法?有没有保存就绪 socket 的方法?这两个问题便是 epoll 技术要解决的。</p>

<h2 id="toc_4">epoll 的设计思路</h2>

<p>epoll 是在 select 出现 N 多年后才被发明的，是 select 和 poll 的增强版本。epoll 通过以下一些措施来改进效率。</p>

<h3 id="toc_5">措施一：功能分离</h3>

<p>select 低效的原因之一是将 维护等待队列 和 阻塞队列 两步合二为一了，每次调用 select 都需要这两步操作，然而大多数应用场景中，需要监视的 socket 相对固定，并不需要每次都修改。epoll 将这两个操作分开，先用 epoll_ctl 维护 等待队列，再调用 epoll_wait 阻塞进程。相比 select，epoll 拆分了功能。</p>

<p>为方便理解后续的内容，我们先复习下 epoll 的用法。</p>

<pre><code class="language-text">int epfd = epoll_create(...);
//将所有需要监听的 socket 添加到 epfd 中 
epoll_ctl(epfd, ...); 
while(1){
    int n = epoll_wait(...) 
    for(接收到数据的 socket) {
        //处理 
    }
}
</code></pre>

<p>在上面的代码中，先用 epoll_create 创建一个 epoll 对象 epfd，再通过 epoll_ctl 将需要监视的 socket 添加到 epfd 中，最后调用 epoll_wait 等待数据。epoll 的功能分离，使得 epoll 有了优化的可能。</p>

<h3 id="toc_6">就绪列表</h3>

<p>select 低效的另一个原因在于 程序不知道哪些 socket 收到了数据，只能一个个遍历。如果内核维护一个 就绪列表，引用收到数据的 socket， 就能避免遍历整个等待队列操作了</p>

<h2 id="toc_7">epoll 的原理和流程</h2>

<p>当某个进程 调用 epoll_create 方法时，内核会创建一个 eventpoll 对象(也就是程序中 epfd 所代表的对象)。eventpoll 对象也是文件系统中的一员，和 socket 一样，它也会有 等待队列。</p>

<p>创建 epoll 对象后，可以用 epoll_ctl 添加或删除所要监听的 socket。以添加 socket 为例， 如果通过 epoll_ctl 添加 sock1、sock2 和 sock3 的监视，内核将会把 eventpoll 添加到 socket 的等待队列中</p>

<p><figure><img src="media/16073533164404/16073579818343.jpg" alt="epoll"/><figcaption>epoll</figcaption></figure></p>

<p>当 socket 收到数据后，中断程序会操作 eventpoll 对象，而不是直接操作进程。中断程 序会给 eventpoll 的“就绪列表”添加 socket 引用。如下图展示的是 sock2 和 sock3 收到数据后，中断程序让 rdlist 引用这两个 socket。</p>

<p><figure><img src="media/16073533164404/16073581194057.jpg" alt=""/></figure></p>

<p>eventpoll 对象相当于是 socket 和进程之间的中介，socket 的数据接收并不直接影响进程，而是通过改变 eventpoll 的就绪列表来改变进程状态。</p>

<p>当程序执行到 epoll_wait 时，如果 rdlist 已经引用了 socket，那么 epoll_wait 直接返回， 如果 rdlist 为空，阻塞进程。</p>

<p>现在我们假设计算机中正在运行进程 A 和进程 B，在某时刻进程 A 运行到了 epoll_wait 语句。如 下图所示，内核会将进程 A 放入 eventpoll 的等待队列中，阻塞进程。</p>

<p><figure><img src="media/16073533164404/16073582227125.jpg" alt=""/></figure></p>

<p>当 socket 接收到数据，中断程序一方面修改 rdlist，另一方面唤醒 eventpoll 等待队列中 的进程，进程 A 再次进入运行状态。也因为 rdlist 的存在，进程 A 可以知道哪些 socket 发生了变化。</p>

<h2 id="toc_8">epoll 实现细节</h2>

<h3 id="toc_9">eventpoll</h3>

<p>上面我们看到 eventpoll 对象对于 epoll 来说是非常重要的<br/>
首先思考两个问题</p>

<ul>
<li>就绪队列应该应使用什么数据结构?</li>
<li>eventpoll 应使用什么数据结构来管理通过 epoll_ctl 添加或删除的 socket?</li>
</ul>

<p>下面我们来看下 eventpoll 的数据结构</p>

<pre><code class="language-c">struct eventpoll {
    // 用于锁定这个eventpoll数据结构，
    // 在用户空间多线程操作这个epoll结构，比如调用epoll_ctl作add, mod, del时，用户空间不需要加锁保护
    // 内核用这个mutex帮你搞定
    struct mutex mtx;

    // 等待队列，epoll_wait时如果当前没有拿到有效的事件，将当前task加入这个等待队列后作进程切换，等待被唤醒
    wait_queue_head_t wq;

    /* Wait queue used by file-&gt;poll() */
    // eventpoll对象在使用时都会对应一个struct file对象，赋值到其private_data，
    // 其本身也可以被 poll， 那也就需要一个wait queue
    wait_queue_head_t poll_wait;

    // 所有有事件触发的被监控的fd都会加入到这个列表
    struct list_head rdllist;

    /* Lock which protects rdllist and ovflist */
    rwlock_t lock;

    // 所有被监控的 fd 使用红黑树来存储
    struct rb_root_cached rbr;

    //  当将ready的fd复制到用户进程中，会使用上面的 lock锁锁定rdllist,
    //  此时如果有新的ready状态fd, 则临时加入到 ovflist表示的单链表中
    struct epitem *ovflist;

    // 会autosleep准备的唤醒源
    struct wakeup_source *ws;

    /* The user that created the eventpoll descriptor */
    struct user_struct *user;

    // linux下一切皆文件，epoll实例被创建时，同时会创建一个file, file的private_data
    // 指向当前这个eventpoll结构
    struct file *file;

    /* used to optimize loop detection check */
    int visited;
    struct list_head visited_list_link;

#ifdef CONFIG_NET_RX_BUSY_POLL
    /* used to track busy poll napi_id */
    unsigned int napi_id;
#endif
};
</code></pre>

<p>就绪列表引用着就绪的 socket，所以它应能够快速的插入数据。</p>

<p>程序可能随时调用 epoll_ctl 添加监视 socket，也可能随时删除。当删除时，若该 socket已经存放在就绪列表中，它也应该被移除。</p>

<p>所以就绪列表应是一种能够快速插入和删除的数据结构。双向链表就是这样一种数据结 构，epoll 使用双向链表来实现就绪队列，也就是 Linux 源码中的 rdllist</p>

<p>既然 epoll 将“维护监视队列”和“进程阻塞”分离，也意味着需要有个数据结构来保 存监视的 socket。至少要方便的添加和移除，还要便于搜索，以避免重复添加。红黑树是一 种自平衡二叉查找树，搜索、插入和删除时间复杂度都是 O(log(N))，效率较好。epoll 使用 了红黑树作为索引结构，也就是 Linux 源码中的 rbr。</p>

<p>因为操作系统要兼顾多种功能，以及由更多需要保存的数据，rdlist 并非直接引用 socket， 而是通过 epitem 间接引用，红黑树的节点也是 epitem 对象。</p>

<h3 id="toc_10">epitem 结构</h3>

<p>每一个被 epoll监控的句柄都会保存在eventpoll内部的红黑树上（eventpoll-&gt;rbr），ready状态的句柄也会保存在eventpoll内部的一个链表上（eventpoll-&gt;rdllist）, 实现时会将每个句柄封装在一个结构中，这就是 epitem</p>

<pre><code class="language-c">struct epitem {
    // 用于构建红黑树
    union {
        /* RB tree node links this structure to the eventpoll RB tree */
        struct rb_node rbn;
        /* Used to free the struct epitem */
        struct rcu_head rcu;
    };

    // 用于将当前epitem链接到eventpoll-&gt;rdllist中
    struct list_head rdllink;

    //用于将当前epitem链接到&quot;struct eventpoll&quot;-&gt;ovflist这个单链表中
    struct epitem *next;

    /* The file descriptor information this item refers to */
    struct epoll_filefd ffd;

    /* Number of active wait queue attached to poll operations */
    int nwait;

    /* List containing poll wait queues */
    struct list_head pwqlist;

    // 对应的eventpoll对象
    struct eventpoll *ep;

    /* List header used to link this item to the &quot;struct file&quot; items list */
    struct list_head fllink;

    /* wakeup_source used when EPOLLWAKEUP is set */
    struct wakeup_source __rcu *ws;

    // 需要关注的读，写事件等
    struct epoll_event event;
};
</code></pre>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Linux 网络包接收过程]]></title>
    <link href="http://www.throne4j.com/16073516476681.html"/>
    <updated>2020-12-07T22:34:07+08:00</updated>
    <id>http://www.throne4j.com/16073516476681.html</id>
    <content type="html"><![CDATA[
<p>首先看下网络包接收大概流程图</p>

<p><figure><img src="media/16073516476681/Linux%20%E7%BD%91%E7%BB%9C%E5%8C%85%E6%8E%A5%E6%94%B6%E8%BF%87%E7%A8%8B.jpg" alt="Linux 网络包接收过程"/><figcaption>Linux 网络包接收过程</figcaption></figure></p>

<ul>
<li>SKB：struct sk_buffer 的简写<br/>
Struct sk_buffer 是 linux TCP/IP stack 中，用于管理Data Buffer的结构。Sk_buffer 在数据包的发送和接收中起着重要的作用。为了提高网络处理的性能，应尽量避免数据包的拷贝。Linux 内核开发者们在设计 sk_buffer 结构的时候，充分考虑到这一点。目前 Linux 协议栈在接收数据的时候，需要拷贝两次：数据包进入网卡驱动后拷贝一次，从内核空间递交给用户空间的应用时再拷贝一次。</li>
</ul>

<p>当网卡上收到数据以后，Linux 中第一个工作的模块是网络驱动。 网络驱动会以 DMA 的 方式把网卡上收到的帧写到内存里。再向 CPU 发起一个中断，以通知 CPU 有数据到达。第 二，当 CPU 收到中断请求后，会去调用网络驱动注册的中断处理函数。 网卡的中断处理函 数并不做过多工作，发出软中断请求，然后尽快释放 CPU。ksoftirqd 检测到有软中断请求到 达，调用 poll 开始轮询收包，收到后交由各级协议栈处理。最后会被放到用户 socket 的接 收队列中。</p>

<h2 id="toc_0">准备工作</h2>

<p>1、创建 ksoftirqd 内核线程<br/>
Linux 的软中断都是在专门的内核线程(ksoftirqd)中进行的，因此我们非常有必要看一 下这些进程是怎么初始化的，这样我们才能在后面更准确地了解收包过程。该进程数量不是 1 个，而是 N 个，其中 N 等于你的机器的核数。</p>

<p>系统初始化的时候会创建出 softirqd 进程。</p>

<p>当 ksoftirqd 被创建出来以后，它就会进入自己的线程循环函数不停地判断有没有软中断需要被处理。这里需要注意的一点是，软中断不仅仅只有网络软中断，还有其它类型。</p>

<p>2、网络子系统初始化</p>

<p>linux 内核通过 net/core/dev.c 中 net_dev_init 函数进行网络子系统的初始化，在这个函数里，会为每个 CPU 都申请一个 softnet_data 数据结构。</p>

<p>同时会给每一种软中断都注册一个处理函数。比如网络处理中 NET_TX_SOFTIRQ 中断的处 理函数为 net_tx_action，NET_RX_SOFTIRQ 中断的为 net_rx_action。</p>

<p>3、协议栈注册<br/>
内核实现了网络层的 ip 协议，也实现了传输层的 tcp 协议和 udp 协议。 </p>

<p>这些协议对应的实现函数分别是 ip_rcv()，tcp_v4_rcv()和 udp_rcv()。和我们平时写代码的方式不一样的是， 内核是通过注册的方式来实现的。 在 net/ipv4/af_inet.c 中的 inet_init(void)负责网络协议栈 注册。 通过 inet_init，将这些函数注册到了 inet_protos 和 ptype_base 两个数据结构中</p>

<p>inet_protos记录着udp，tcp的处理函数地址，ptype_base存储着ip_rcv()函数的处理地址。 软中断中会通过 ptype_base 找到 ip_rcv 函数地址，进而将 ip 包正确地送到 ip_rcv()中执行。 在 ip_rcv 中将会通过 inet_protos 找到 tcp 或者 udp 的处理函数，再而把包转发给 udp_rcv() 或 tcp_v4_rcv()函数。</p>

<p>4、网卡驱动初始化和启动网卡，中间的工作包括分配内存、注册中断处理函数，注册供 内核使用的函数、开启硬中断等等工作。</p>

<h2 id="toc_1">Socket 的创建</h2>

<p>调用链：<br/>
net/Socket.c:sys_socket()-&gt;sock_create()-&gt;__sock_create()-&gt;sock_alloc();</p>

<p>主要的工作则是:<br/>
1、和 Linux 文件系统挂钩;<br/>
2、使用协议族来初始化 socket，在前面的初始化过程中，我们知道有个协议栈注册过程， 找到内核初始化时注册的协议域，然后调用其 create 方法进行 socket 实例的创建;<br/>
3、初始化和 socket 通信有关的缓冲区、队列等等;</p>

<h2 id="toc_2">接受数据的到来</h2>

<p>1、首先当数据帧从网线到达网卡上的时候，第一站是网卡的接收队列。网卡在分配给自己的RingBuffer中寻找可用的内存位置，找到后 DMA 引擎会把数据 DMA 到网卡之前关联的内存里，这个时候 CPU 都是无感的。</p>

<p>当 DMA 操作完成以后，网卡会向 CPU 发起一个硬中断，通知 CPU 有数据到达。CPU 调用网卡注册的硬中断处理函数，这个处理过程非常短。只是记录了一个寄存器，修改了一下 CPU 的 softnet_data 中的列表，告诉 CPU 当前有设备有数据需要处理，然后发出个软中断， 硬中断工作就算是完成了。</p>

<p>2、在网络子系统初始化小节，我们看到为 NET_RX_SOFTIRQ 注册了处理函数 net_rx_action， 这个函数中的核心逻辑执行就是执行网卡驱动注册到 softnet_data 中的函数，把数据帧从 RingBuffer 上取下来，被送到协议栈中，从数据包中取出协议信息，然后遍历注册在这个协议上的回调函数列表，调用到了协议层注册的处理函数了。</p>

<p>对于 ip 包来讲，就会进入到 ip_rcv，IP 层处理完成后如协议注册小节看到 inet_protos 中 保存着 tcp_rcv()和 udp_rcv()的函数地址。</p>

<p>这里将会根据包中的协议类型选择进行分发，在这里 skb 包将会进一步被派送到更上层的协议中，udp 和 tcp。</p>

<p>udp 和 tcp 层根据 skb 来寻找对应的 socket，当找到以后将数据包放到 socket 的接收队列中。<br/>
而我们一般网络编程里的读取数据方法，就是访问这个接收队列</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Java 生态圈中的零拷贝]]></title>
    <link href="http://www.throne4j.com/16072730714201.html"/>
    <updated>2020-12-07T00:44:31+08:00</updated>
    <id>http://www.throne4j.com/16072730714201.html</id>
    <content type="html"><![CDATA[
<p>Linux 提供的零拷贝技术 Java 并不是全支持，支持 2种(内存映射 mmap、sendfile);</p>

<h2 id="toc_0">NIO 提供的内存映射 MappedByteBuffer</h2>

<p>NIO 中的 FileChannel.map()方法其实就是采用了操作系统中的内存映射方式，底层就是 调用 Linux mmap()实现的。</p>

<p>将内核缓冲区的内存和用户缓冲区的内存做了一个地址映射。这种方式适合读取大文件， 同时也能对文件内容进行更改，但是如果其后要通过 SocketChannel 发送，还是需要 CPU 进 行数据的拷贝。</p>

<h2 id="toc_1">NIO 提供的 sendfile</h2>

<p>Java NIO 中提供的 FileChannel 拥有 transferTo 和 transferFrom 两个方法，可直接把 FileChannel 中的数据拷贝到另外一个 Channel，或者直接把另外一个 Channel 中的数据拷 贝到 FileChannel。</p>

<p>该接口常被用于高效的 网络/文件 的数据传输和大文件拷贝。在操作系统支持的情况下，通过该方法传输数据并不需要将源数据从内核态拷贝到用户态，再从用户态拷贝到目标通道的内核态，同时也避免了两次用户态和内核态间的上下文切换，也即使用了“零拷贝”，所以其性能一般高于 Java IO 中提供的方法。</p>

<h2 id="toc_2">Kafka 中的零拷贝</h2>

<p>Kafka 两个重要过程都使用了零拷贝技术，且都是操作系统层面的狭义零拷贝，</p>

<ul>
<li>Producer 生产的数据存到 broker<br/>
Producer 生产的数据持久化到 broker，采用 mmap 文件映射，实现顺序的快速写入</li>
<li>Consumer 从 broker 读取数据。<br/>
Customer 从 broker 读取数据，采用 sendfile，将磁盘文件读到 OS 内核缓冲区后，直接转到 socket buffer 进行网络发送</li>
</ul>

<h2 id="toc_3">Netty 的零拷贝实现</h2>

<p>Netty 的零拷贝主要包含几个方面:</p>

<p>在网络通信上，Netty 的接收和发送 ByteBuffer 采用 DIRECT BUFFERS，使用堆外直接 内存进行 Socket 读写，不需要进行字节缓冲区的二次拷贝。如果使用传统的堆内存(HEAP BUFFERS)进行 Socket 读写，JVM 会将堆内存 Buffer 拷贝一份到直接内存中，然后才写 入 Socket 中。相比于堆外直接内存，消息在发送过程中多了一次缓冲区的内存拷贝。</p>

<p>在缓存操作上，Netty 提供了 CompositeByteBuf 类，它可以将多个 ByteBuf 合并为一个逻辑上的 ByteBuf，避免了各个 ByteBuf 之间的拷贝。</p>

<p>通过 wrap 操作，我们可以将 byte[]数组、ByteBuf、 ByteBuffer 等包装成一个 Netty ByteBuf 对象，进而避免了拷贝操作。</p>

<p>ByteBuf支持slice 操作，因此可以将ByteBuf分解为多个共享同一个存储区域的ByteBuf， 避免了内存的拷贝。</p>

<p>在文件传输上，Netty 的通过 FileRegion 包装的 FileChannel.tranferTo 实现文件传输，它可以直接将文件缓冲区的数据发送到目标 Channel，避免了传统通过循环 write 方式导致的内存拷贝问题。</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Linux 零拷贝技术]]></title>
    <link href="http://www.throne4j.com/16072680119143.html"/>
    <updated>2020-12-06T23:20:11+08:00</updated>
    <id>http://www.throne4j.com/16072680119143.html</id>
    <content type="html"><![CDATA[
<h2 id="toc_0">第一个问题：什么是零拷贝？</h2>

<p>零拷贝技术是指计算机执行操作时， CPU 不需要先将数据从某处内存复制到另一个特定区域。这种技术通常用于通过网络传输文件时节省 CPU 周期和内存带宽。</p>

<ul>
<li>零拷贝技术可以减少数据拷贝和共享总线操作次数，消除传输数据在存储器之间不必要的中间拷贝次数，从而有效的提高数据传输效率</li>
<li>零拷贝技术减少了用户进程地址空间和内核地址空间之间因为上下文切换而带来的开销</li>
</ul>

<p>广泛的说就是减少不必要的拷贝即被称为零拷贝</p>

<h2 id="toc_1">Linux 的 IO 机制与 DMA</h2>

<p>在早期的计算机中，用户进程需要读取磁盘数据，需要 CPU 中断和 CPU 参与，因此效率比较低，发起 IO 请求，每次的 IO 中断，都带来了 CPU 的上下文切换，因此出现了 DMA 技术</p>

<p>DMA(Direct Memory Access，直接内存存取)是所有现代电脑的重要特色，它允许不同速度的硬件装置来沟通，而不需要依赖于 CPU 的大量中断负载。</p>

<p>DMA 控制器接管了数据读写请求，减少了 CPU 的负担，这样一来 CPU 可以高效工作了。现代硬盘基本都支持 DMA</p>

<p>实际 IO 读取涉及两个过程：</p>

<ul>
<li>DMA 等待数据准备好，把磁盘数据读取到操作系统内核缓冲区</li>
<li>用户进程将内核缓冲区的数据 拷贝到用户空间</li>
</ul>

<p>这两个过程都是阻塞的。</p>

<h2 id="toc_2">传统数据传送机制</h2>

<p>比如：读取文件，再用 socket 发送出去，实际经过四次 copy。</p>

<p>伪代码如下：</p>

<pre><code class="language-text">buffer = File.read()
Socket.send(buffer)
</code></pre>

<ul>
<li>第一次：将磁盘文件读取到操作系统内核缓冲区</li>
<li>第二次：将内核缓冲区的数据拷贝到应用程序的 buffer</li>
<li>第三次：将应用程序 buffer 中的数据拷贝到 socket 网络发送缓冲区(属于操作系统内核的缓冲区)</li>
<li>第四次：将 socket buffer 的数据拷贝到网卡，由网卡进行网络传输</li>
</ul>

<p>图示如下：<br/>
<figure><img src="media/16072680119143/16072705633200.jpg" alt="传统数据传送机制"/><figcaption>传统数据传送机制</figcaption></figure></p>

<p>分析上述的过程，虽然引入 DMA 来接管 CPU 的中断请求，但四次 copy 是存在“不必 要的拷贝”的。实际上并不需要第二个和第三个数据副本。应用程序除了缓存数据并将其传输回套接字缓冲区之外什么都不做。相反，数据可以直接从读缓冲区传输到套接字缓冲区</p>

<p>显然，第二次和第三次数据 copy 其实在这种场景下没有什么帮助反而带来开销，这也 正是零拷贝出现的背景和意义。</p>

<p>传统的数据传送所消耗的成本: 4 次拷贝，4 次上下文切换。</p>

<h2 id="toc_3">Linux 支持的零拷贝</h2>

<p>零拷贝的目的：减少 IO 流程中不必要的拷贝，当然零拷贝需要操作系统的支持。</p>

<h3 id="toc_4">mmap内存映射</h3>

<p>首先了解一下 <a href="15972869856630.html">mmap 是什么，能做什么 </a></p>

<p>硬盘上文件的位置和应用程序缓冲区(application buffers)进行映射(建立一种一一对应 关系)，由于 mmap()将文件直接映射到用户空间，所以实际文件读取时根据这个映射关系， 直接将文件从硬盘拷贝到用户空间，只进行了一次数据拷贝，不再有文件内容从硬盘拷贝到 内核空间的一个缓冲区。</p>

<p><figure><img src="media/16072680119143/16072711819766.jpg" alt="mmap零拷贝"/><figcaption>mmap零拷贝</figcaption></figure></p>

<p>再次看下mmap的数据传输所消耗的成本：3次拷贝(1次 CPU 拷贝，2次 DMA 拷贝)和 4 次上下文切换</p>

<h3 id="toc_5">sendfile</h3>

<p>linux 2.1 支持的 sendfile，当调用 sendfile()时，DMA 将磁盘数据复制到 kernel buffer，然后将内核中的 kernel buffer 直接拷贝到 socket buffer; </p>

<p>但是数据并未被真正复制到 socket 关联的缓冲区内。取而代之的是，只有记录数据位置和长度的描述符被加入到 socket 缓冲区中。DMA 模块将数据直接从内核缓冲区传递给协议引擎，从而消除了遗留的最后一次复制。</p>

<p>一旦数据全都拷贝到 socket buffer，sendfile()系统调用将会 return、代表数据转化的完成。socket buffer 里的数据就能在网络传输了。</p>

<p><figure><img src="media/16072680119143/16072720825690.jpg" alt="sendfile"/><figcaption>sendfile</figcaption></figure></p>

<p>sendfile 会经历:3 次拷贝(1 次 CPU 拷贝,2 次 DMA 拷贝)和2次上下文切换</p>

<h3 id="toc_6">splice</h3>

<p>Linux 从 2.6.17 支持 splice<br/>
数据从磁盘读取到 OS 内核缓冲区后，在内核缓冲区直接可将其转成内核空间其他数据 buffer，而不需要拷贝到用户空间。</p>

<p>如下图所示，从磁盘读取到内核 buffer 后，在内核空间直接与 socket buffer 建立 pipe 管道。</p>

<p>和 sendfile()不同的是，splice()不需要硬件支持。</p>

<p>注意 splice 和 sendfile 的不同，sendfile 是将磁盘数据加载到 kernel buffer 后，需要一次 CPU 拷贝，拷贝到 socket buffer。而 splice 是更进一步，连这个 CPU 拷贝 也不需要了，直接将两个内核空间的 buffer 进行 pipe。</p>

<p><figure><img src="media/16072680119143/16072728727089.jpg" alt="splice"/><figcaption>splice</figcaption></figure></p>

<p>splice 会经历 2 次拷贝: 0 次 cpu 拷贝 2 次 DMA 拷贝;</p>

<h2 id="toc_7">总结 Linux 中零拷贝</h2>

<p>最早的零拷贝定义来源于 Linux 2.4 内核新增 sendfile 系统调用，提供了零拷贝。</p>

<p>磁盘数据通过 DMA 拷贝到内核 态 Buffer 后，直接通过 DMA 拷贝到 NIO Buffer(socket buffer)，无需 CPU 拷贝。这也是零拷贝这一说法的来源。这是真正操作系统意义上的零拷贝(也就是狭义零拷贝)。</p>

<p>但是我们知道，由 OS 内核提供的 操作系统意义上的零拷贝，发展到目前也并没有很 多种，也就是这样的零拷贝并不是很多;</p>

<p>随着发展，零拷贝的概念得到了延伸，就是目前的减少不必要的数据拷贝都算作零拷贝 的范畴。</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[直接内存]]></title>
    <link href="http://www.throne4j.com/16072655636252.html"/>
    <updated>2020-12-06T22:39:23+08:00</updated>
    <id>http://www.throne4j.com/16072655636252.html</id>
    <content type="html"><![CDATA[
<p>在所有的网络通信和应用程序中，每个 TCP 的 Socket 的内核中都有一个发送缓冲区 (SO_SNDBUF)和一个接收缓冲区(SO_RECVBUF)，可以使用相关套接字选项来更改该缓冲区大 小。</p>

<p><figure><img src="media/16072655636252/16072674138718.jpg" alt=""/></figure></p>

<p>当某个应用进程调用 write 时，内核从该应用进程的缓冲区中复制所有数据到所写套接 字的发送缓冲区。如果该套接字的发送缓冲区容不下该应用进程的所有数据(或是应用进程 的缓冲区大于套接字的发送缓冲区，或是套接字的发送缓冲区中已有其他数据)，假设该套 接字是阻塞的，则该应用进程将被投入睡眠。</p>

<p>内核将不从 write 系统调用返回，直到应用进程缓冲区中的所有数据都复制到套接字发送缓冲区。因此，从写一个 TCP 套接字的 write 调用成功返回仅仅表示我们可以重新使用原来的应用进程缓冲区，并不表明对端的 TCP 或应用进程已接收到数据。</p>

<p><figure><img src="media/16072655636252/16072675181930.jpg" alt=""/></figure></p>

<p>Java 程序自然也要遵守上述的规则。</p>

<p>但在 Java 中存在着堆、垃圾回收等特性，所以在实际的 IO 中，在 JVM 内部的存在着这样一种机制: 在 IO 读写上，如果是使用堆内存，JDK 会先创建一个 DirectBuffer，再去执行真正的写操作。</p>

<p>这是因为，当我们把一个地址通过 JNI 传递给底层的 C 库的时候，有一个基本的要求， 就是这个地址上的内容不能失效。然而，在 GC 管理下的对象是会在 Java 堆中移动的。也就是说，有可能我把一个地址传给底层的 write，但是这段内存却因为 GC 整理内存而失效了。 所以必须要把待发送的数据放到一个 GC 管不着的地方。这就是调用 native 方法之前，数据—定要在堆外内存的原因。</p>

<p>可见，DirectBuffer 并没有节省什么内存拷贝，只是因为 HeapBuffer 必须多做一次拷贝， 使用 DirectBuffer 就会少一次内存拷贝。相比没有使用堆内存的 Java 程序，使用直接内存的 Java 程序当然更快一点。</p>

<p>从垃圾回收的角度而言，直接内存不受 GC(新生代的 Minor GC) 影响，只有当执行老 年代的 Full GC 时候才会顺便回收直接内存，整理内存的压力也比数据放到 HeapBuffer 要小。</p>

<h2 id="toc_0">堆外内存的优点和缺点</h2>

<p>堆外内存相比于堆内内存有几个优势:</p>

<ul>
<li>减少了垃圾回收的工作，因为垃圾回收会暂停其他的工作线程</li>
<li>加快了复制的速度。因为堆内在 flush 到远程时，会先复制到直接内存(非堆内存)，然后再发送，而堆外内存相当于省略了这个操作。</li>
</ul>

<p>相对来说堆外内存的劣势如下：</p>

<ul>
<li>堆外内存难以控制，如果内存泄漏，很难排查</li>
<li>堆外内存相对来说，不适合存储很复杂的对象。一般简单的对象或者扁平化的对象比较合适</li>
</ul>

]]></content>
  </entry>
  
</feed>
