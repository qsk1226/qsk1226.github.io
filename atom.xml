<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[大爷来玩儿啊~]]></title>
  <link href="http://www.throne4j.com/atom.xml" rel="self"/>
  <link href="http://www.throne4j.com/"/>
  <updated>2020-10-27T21:06:09+08:00</updated>
  <id>http://www.throne4j.com/</id>
  <author>
    <name><![CDATA[]]></name>
    
  </author>
  <generator uri="http://www.mweb.im/">MWeb</generator>
  
  <entry>
    <title type="html"><![CDATA[]]></title>
    <link href="http://www.throne4j.com/16027453677170.html"/>
    <updated>2020-10-15T15:02:47+08:00</updated>
    <id>http://www.throne4j.com/16027453677170.html</id>
    <content type="html"><![CDATA[
<h1 id="toc_0">redis rehash</h1>

<p>作为一个内存数据库，redis内部采用了字典的数据结构实现了键值对的存储，字典也就是我们平时所说的哈希表。随着数据量的不断增加，数据必然会产生hash碰撞，而redis采用链地址法解决hash冲突。我们知道如果哈希表数据量达到了一个很大的量级，那么冲突的链的元素数量就会很大，这时查询效率就会变慢，因为取值的时候redis会遍历链表。而随着数据量的缩减，也会产生一定的内存浪费。redis在设计时充分考虑了字典的增加和缩减，为了优化数据量增加时的查询效率和缩减时的内存利用率，redis进行了一系列操作，而处理的这个过程被称作rehash。</p>

<h2 id="toc_1">redis 哈希表结构</h2>

<p><figure><img src="media/16027453677170/16027455213851.jpg" alt=""/></figure></p>

<pre><code class="language-c">// 哈希表定义

typedef struct dictht {

    dictEntry **table;

    unsigned long size;

    unsigned long sizemask;

    unsigned long used; 

} dictht;



// 字典定义

typedef struct dict {

    dictType *type;

    void *privdata;    dictht ht[2]; /* 两个hashtable */

    long rehashidx; /* rehashing 如果没有进行则 rehashidx == -1  否则 rehash则表示rehash进行到的索引位置 */ 

    unsigned long iterators; /* number of iterators currently running */

} dict;
</code></pre>

<p>从结构上看每个字典中都包含了两个hashtable。那么为什么一个字典会需要两个hashtable？首先redis在正常读写时会用到一个hashtable，而另一个hashtable的作用实际上是作为字典在进行rehash时的一个临时载体。我们可以这么理解，redis开始只会用一个hashtable去读写，如果这个hashtable的数据量增加或者缩减到某个值，到达了rehash的条件，redis便会开始根据数据量和链（bucket）的个数初始化那个备用的hashtable，来使这个hashtable从容量上满足后续的使用，并开始把之前的hashtable的数据迁移到这个新的hashtable上来，当然这种迁移是对每个节点值进行一次hash运算。等到数据全部迁移完成，再进行一次hashtable的地址更名，把这个备用的hashtable为正式的hashtable，同时清空另一个hashtable以供下一次rehash使用。</p>

<h2 id="toc_2">rehash 条件</h2>

<p>hashtable元素总个数 / 字典的链个数 = 每个链平均存储的元素个数(load_factor)</p>

<ul>
<li><p>服务器目前没有在执行BGSAVE命令或者BGREWRITEAOF命令，load_factor &gt;= 1，dict就会触发扩大操作rehash</p></li>
<li><p>服务器目前正在执行BGSAVE命令或者BGREWRITEAOF命令，load_factor &gt;= 5，dict就会触发扩大操作rehash</p></li>
<li><p>load_factor &lt; 0.1，dict就会触发缩减操作rehash</p></li>
</ul>

<h2 id="toc_3">rehash 过程</h2>

<p>我们假设 ht[0]为正在使用的hashtable，ht[1]为rehash之后的备用hashtable<br/>
步骤如下：</p>

<ul>
<li><p>为字典的备用哈希表分配空间：</p></li>
<li><p>如果执行的是扩展操作，那么备用哈希表的大小为第一个大于等于(已用节点个数)*2的2n（2的n次方幂）</p></li>
<li><p>如果执行的是收缩操作，那么备用哈希表的大小为第一个大于等于(已用节点个数)的2n</p></li>
<li><p>在字典中维持一个索引计数器变量rehashidx，并将它的值设置为0，表示rehash工作正式开始（为-1时表示没有进行rehash）。</p></li>
<li><p>rehash进行期间，每次对字典执行添加、删除、查找或者更新操作时，程序除了执行指定的操作以外，还会顺带将ht[0]哈希表在rehashidx索引上的所有键值对rehash到ht[1]，当一次rehash工作完成之后，程序将rehashidx属性的值+1。同时在serverCron中调用rehash相关函数，在1ms的时间内，进行rehash处理，每次仅处理少量的转移任务(100个元素)。</p></li>
<li><p>随着字典操作的不断执行，最终在某个时间点上，ht[0]的所有键值对都会被rehash至ht[1]，这时程序将rehashidx属性的值设为-1，表示rehash操作已完成。</p></li>
</ul>

<h2 id="toc_4">rehash 总结</h2>

<p><strong><em>在Hash 表扩容或者收缩的时候，程序需要将现有的哈希表中的所有键值对rehash 到新的 Hash表里面，此rehash 过程不是一次性完成的，而是渐进式的完成。</em></strong></p>

<p>这种渐进式的 rehash 避免了集中式rehash带来的庞大计算量和内存操作，但是需要注意的是redis在进行rehash的时候，正常的访问请求可能需要做多要访问两次hashtable（ht[0]， ht[1]），例如键值被rehash到新ht[1]，则需要先访问ht[0]，如果ht[0]中找不到，则去ht[1]中找。</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[CPU缓存一致性协议MESI]]></title>
    <link href="http://www.throne4j.com/16026440050146.html"/>
    <updated>2020-10-14T10:53:25+08:00</updated>
    <id>http://www.throne4j.com/16026440050146.html</id>
    <content type="html"><![CDATA[
<h2 id="toc_0">CPU高速缓存（Cache Memory）</h2>

<h3 id="toc_1">CPU为何要有高速缓存</h3>

<p>CPU在摩尔定律的指导下以每18个月翻一番的速度在发展，然而内存和硬盘的发展速度远远不及CPU。这就造成了高性能能的内存和硬盘价格及其昂贵。然而CPU的高度运算需要高速的数据。为了解决这个问题，CPU厂商在CPU中内置了少量的高速缓存以解决I\O速度和CPU运算速度之间的不匹配问题。</p>

<p>在CPU访问存储设备时，无论是存取数据抑或存取指令，都趋于聚集在一片连续的区域中，这就被称为局部性原理。</p>

<p>时间局部性（Temporal Locality）：如果一个信息项正在被访问，那么在近期它很可能还会被再次访问。</p>

<p>比如循环、递归、方法的反复调用等。</p>

<p>空间局部性（Spatial Locality）：如果一个存储器的位置被引用，那么将来他附近的位置也会被引用。</p>

<p>比如顺序执行的代码、连续创建的两个对象、数组等。</p>

<h3 id="toc_2">带有高速缓存的CPU执行计算的流程</h3>

<p>程序以及数据被加载到主内存</p>

<p>指令和数据被加载到CPU的高速缓存</p>

<p>CPU执行指令，把结果写到高速缓存</p>

<p>高速缓存中的数据写回主内存</p>

<p><figure><img src="media/16026440050146/16026440570104.jpg" alt=""/></figure></p>

<h3 id="toc_3">目前流行的多级缓存结构</h3>

<p>由于CPU的运算速度超越了1级缓存的数据I\O能力，CPU厂商又引入了多级的缓存结构。</p>

<p>多级缓存结构<br/>
<figure><img src="media/16026440050146/16026440832590.jpg" alt=""/></figure></p>

<h2 id="toc_4">多核CPU多级缓存一致性协议MESI</h2>

<p>多核CPU的情况下有多个一级缓存，如何保证缓存内部数据的一致,不让系统数据混乱。这里就引出了一个一致性的协议MESI。</p>

<h3 id="toc_5">MESI协议缓存状态</h3>

<p>MESI 是指4中状态的首字母。每个 缓存行Cache line (缓存存储数据的单元)有4个状态，可用2个bit表示，它们分别是：</p>

<table>
<thead>
<tr>
<th>状态</th>
<th>描述</th>
<th>监听任务</th>
</tr>
</thead>

<tbody>
<tr>
<td>M 修改 (Modified)</td>
<td>该Cache line有效，数据被修改了，和内存中的数据不一致，数据只存在于本Cache中。</td>
<td>缓存行必须时刻监听所有试图读该缓存行相对就主存的操作，这种操作必须在缓存将该缓存行写回主存并将状态变成S（共享）状态之前被延迟执行。</td>
</tr>
<tr>
<td>E 独享、互斥 (Exclusive)</td>
<td>该Cache line有效，数据和内存中的数据一致，数据只存在于本Cache中。</td>
<td>缓存行也必须监听其它缓存读主存中该缓存行的操作，一旦有这种操作，该缓存行需要变成S（共享）状态。</td>
</tr>
<tr>
<td>S 共享 (Shared)</td>
<td>该Cache line有效，数据和内存中的数据一致，数据存在于很多Cache中。</td>
<td>缓存行也必须监听其它缓存使该缓存行无效或者独享该缓存行的请求，并将该缓存行变成无效（Invalid）。</td>
</tr>
<tr>
<td>I 无效 (Invalid)</td>
<td>该Cache line无效。</td>
<td>无</td>
</tr>
</tbody>
</table>

<p>注意：<br/>
<strong>对于M和E状态而言总是精确的，他们在和该缓存行的真正状态是一致的，而S状态可能是非一致的</strong>。如果一个缓存将处于S状态的缓存行作废了，而另一个缓存实际上可能已经独享了该缓存行，但是该缓存却不会将该缓存行升迁为E状态，这是因为其它缓存不会广播他们作废掉该缓存行的通知，同样由于缓存并没有保存该缓存行的copy的数量，因此（即使有这种通知）也没有办法确定自己是否已经独享了该缓存行。</p>

<p>从上面的意义看来E状态是一种投机性的优化：如果一个CPU想修改一个处于S状态的缓存行，总线事务需要将所有该缓存行的copy变成invalid状态，而修改E状态的缓存不需要使用总线事务。</p>

<h3 id="toc_6">MESI 状态转换</h3>

<p><figure><img src="media/16026440050146/16026444223626.jpg" alt=""/></figure></p>

<p>理解该图的前置说明：</p>

<h4 id="toc_7">触发事件</h4>

<table>
<thead>
<tr>
<th>触发事件</th>
<th>描述</th>
</tr>
</thead>

<tbody>
<tr>
<td>本地读取（Local read）</td>
<td>本地cache读取本地cache数据</td>
</tr>
<tr>
<td>本地写入（Local write）</td>
<td>本地cache写入本地cache数据</td>
</tr>
<tr>
<td>远端读取（Remote read）</td>
<td>其他cache读取本地cache数据</td>
</tr>
<tr>
<td>远端写入（Remote write）</td>
<td>其他cache写入本地cache数据</td>
</tr>
</tbody>
</table>

<h4 id="toc_8">cache分类</h4>

<p>前提：所有的cache共同缓存了主内存中的某一条数据。</p>

<p>本地cache:指当前cpu的cache。<br/>
触发cache:触发读写事件的cache。<br/>
其他cache:指既除了以上两种之外的cache。<br/>
注意：本地的事件触发 本地cache和触发cache为相同。</p>

<p><figure><img src="media/16026440050146/16026616087435.jpg" alt="" style="width:1121px;"/></figure></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[]]></title>
    <link href="http://www.throne4j.com/16024819837796.html"/>
    <updated>2020-10-12T13:53:03+08:00</updated>
    <id>http://www.throne4j.com/16024819837796.html</id>
    <content type="html"><![CDATA[

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[mesi]]></title>
    <link href="http://www.throne4j.com/16024149950927.html"/>
    <updated>2020-10-11T19:16:35+08:00</updated>
    <id>http://www.throne4j.com/16024149950927.html</id>
    <content type="html"><![CDATA[
<p>循环getBeanPostProcessors</p>

<p>org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory#populateBean  </p>

<p>org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory#initializeBean(java.lang.String, java.lang.Object, org.springframework.beans.factory.support.RootBeanDefinition)---&gt;org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory#applyBeanPostProcessorsBeforeInitialization</p>

<p>org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory#determineConstructorsFromBeanPostProcessors</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Dubbo]]></title>
    <link href="http://www.throne4j.com/16024075158165.html"/>
    <updated>2020-10-11T17:11:55+08:00</updated>
    <id>http://www.throne4j.com/16024075158165.html</id>
    <content type="html"><![CDATA[
<h2 id="toc_0">Dubbo 简介</h2>

<p>在分布式服务架构下，各个服务间的相互 rpc 调用会越来越复杂。最终形成网状结构，此时服务的治理极为关键。</p>

<p>Dubbo 是一个带有服务治理功能的 RPC 框架，提供了一套较为完整的服务治理方案，其底层直接实现了 rpc 调用的全过程，并尽力做事 rpc 远程对 使用者透明。下图展示了 Dubbo 服务治理的功能。</p>

<p><figure><img src="media/16024075158165/16024075991851.jpg" alt="" style="width:751px;"/></figure></p>

<p>简单的说，Dubbo 就是个服务调用的框架，如果没有分布式的需求，其实是不需要用的，只有在分布式的时候，才有使用 Dubbo 这样的分布式服务框架的需求，并且本质上是个服务调用的东东。</p>

<p>其核心部分包括：</p>

<ul>
<li>远程通讯:提供对多种基于长连接的 NIO 框架抽象封装，包括多种线程 模型、序列化以及“请求-响应”模式的信息交换方式。</li>
<li>集群容错:提供基于接口方法的透明远程过程调用，包括多协议支持以 及软负载均衡，失败容错、地址路由、动态配置等集群支持。</li>
<li>自动发现:基于注册中心目录服务，使服务消费方能动态的查某服务的提供方，使地址透明，使服务提供方可以平滑增加或减少机器。</li>
</ul>

<h2 id="toc_1">dubbo 的架构</h2>

<p>dubbo 的整体结构如下图所示：</p>

<p><figure><img src="media/16024075158165/16024078552797.jpg" alt=""/></figure></p>

<ul>
<li>图中左边淡蓝背景的为服务消费方使用的接口，右边淡绿色背景的为服务提供方使用的接口，位于中轴线上的为双方都用到的接口。</li>
<li>图中从下至上分为十层，各层均为单向依赖，右边的黑色箭头代表层之间的依赖关系，每一层都可以剥离上层被复用，其中，Service 和 Config 层为 API，其它各层均为 SPI。</li>
<li>图中绿色小块的为扩展接口，蓝色小块为实现类，图中只显示用于关联各层的实现类。</li>
<li>图中蓝色虚线为初始化过程，即启动时组装链，红色实线为方法调用过程，即运行时调时链，紫色三角箭头为继承，可以把子类看作父类的同一个节点，线上的文字为调用的方法。</li>
</ul>

<p>Dubbo总体架构设计一共划分了10层：</p>

<ul>
<li><p>服务接口层(Service)<br/>
该层是与实际业务逻辑相关的，根据服务提供方 和 服务消费方的业务设计对应的接口和实现。</p></li>
<li><p>配置层(Config)<br/>
对外配置接口，以 ServiceConfig 和 ReferenceConfig 为中 心，可以直接 new 配置类，也可以通过 Spring 解析配置生成配置类。</p></li>
<li><p>服务代理层(Proxy)<br/>
服务接口透明代理，生成服务的客户端 Stub 和服务 器端 Skeleton，以 ServiceProxy 为中心，扩展接口为 ProxyFactory。</p></li>
<li><p>服务注册层(Registry)<br/>
封装服务地址的注册与发现，以服务 URL 为中心， 扩展接口为RegistryFactory、Registry 和 RegistryService。可能没有服务注册中心，此时服务提供方直接暴露服务。</p></li>
<li><p>集群层(Cluster)<br/>
封装多个提供者的路由及负载均衡，并桥接注册中心，以 Invoker 为中心，扩展接口为 Cluster、Directory、Router 和 LoadBalance。 将多个服务提供方组合为一个服务提供方，实现对服务消费方透明，只 需要与一个服务提供方进行交互。</p></li>
<li><p>监控层(Monitor)<br/>
RPC 调用次数和调用时间监控，以 Statistics 为中心， 扩展接口为 MonitorFactory、Monitor 和 MonitorService。</p></li>
<li><p>远程调用层(Protocol)<br/>
封将 RPC 调用，以 Invocation 和 Result 为中心， 扩展接口为 Protocol、Invoker 和 Exporter。Protocol 是服务域，它是 Invoker 暴露和引用的主功能入口，它负责 Invoker 的生命周期管理。Invoker 是 实体域，它是 Dubbo 的核心模型，其他模型都向它靠扰，或转换成它，它代表一个可执行体，可向它发起invoke 调用。它有可能是一个本地的实现，也可能是一个远程的实现，也可能是一个集群实现。</p></li>
<li><p>信息交换层(Exchange)<br/>
封装请求响应模式，同步转异步，以 Request 和 Response 为中心，扩展接口为 Exchanger、ExchangeChannel、ExchangeClient 和 ExchangeServer。</p></li>
<li><p>网络传输层(Transport)<br/>
抽象 mina 和 netty 为统一接口，以 Message 为中心，扩展接口为 Channel、Transporter、Client、Server 和 Codec。</p></li>
<li><p>数据序列化层(Serialize)<br/>
可复用的一些工具，扩展接口为 Serialization、ObjectInput、ObjectOutput 和 ThreadPool。</p></li>
</ul>

<p>各层之间的关系：</p>

<ul>
<li>在 RPC 中，Protocol 是核心层，也就是只要有 Protocol + Invoker + Exporter 就可以完成非透明的 RPC 调用，然后在 Invoker 的主过程上 Filter 拦截点。</li>
<li>图中的 Consumer 和 Provider 是抽象概念，只是想让看图者更直观的了解哪些类分属于客户端与服务器端，不用 Client 和 Server 的原因是 Dubbo 在很多场景下都使用 Provider, Consumer, Registry, Monitor 划分逻辑拓普节点，保持统一概念。</li>
<li>而 Cluster 是外围概念，所以 Cluster 的目的是将多个 Invoker 伪装成一个 Invoker，这样其它人只要关注 Protocol 层 Invoker 即可，加上 Cluster 或者去掉 Cluster 对其它层都不会造成影响，因为只有一个提供者时，是不需要 Cluster 的。</li>
<li>Proxy 层封装了所有接口的透明化代理，而在其它层都以 Invoker 为中心，只有到了暴露给用户使用时，才用 Proxy 将 Invoker 转成接口，或将接口实现转成 Invoker，也就是去掉 Proxy 层 RPC 是可以 Run 的，只是不那么透明，不那么看起来像调本地服务一样调远程服务。</li>
<li>而 Remoting 实现是 Dubbo 协议的实现，如果你选择 RMI 协议，整个 Remoting 都不会用上，Remoting 内部再划为 Transport 传输层和 Exchange 信息交换层，Transport 层只负责单向消息传输，是对 Mina, Netty, Grizzly 的抽象，它也可以扩展 UDP 传输，而 Exchange 层是在传输层之上封装了 Request-Response 语义。</li>
<li>Registry 和 Monitor 实际上不算一层，而是一个独立的节点，只是为了全局概览，用层的方式画在一起。</li>
</ul>

<h2 id="toc_2">Dubbo 架构具有以下几个特点，分别是连通性、健壮性、伸缩性、以及向未来架构的升级性。</h2>

<h3 id="toc_3">连通性</h3>

<ul>
<li>注册中心负责服务地址的注册与查找，相当于目录服务，服务提供者和消费者只在启动时与注册中心交互，注册中心不转发请求，压力较小</li>
<li>监控中心负责统计各服务调用次数，调用时间等，统计先在内存汇总后每分钟一次发送到监控中心服务器，并以报表展示</li>
<li>服务提供者向注册中心注册其提供的服务，并汇报调用时间到监控中心，此时间不包含网络开销</li>
<li>服务消费者向注册中心获取服务提供者地址列表，并根据负载算法直接调用提供者，同时汇报调用时间到监控中心，此时间包含网络开销</li>
<li>注册中心，服务提供者，服务消费者三者之间均为长连接，监控中心除外</li>
<li>注册中心通过长连接感知服务提供者的存在，服务提供者宕机，注册中心将立即推送事件通知消费者</li>
<li>注册中心和监控中心全部宕机，不影响已运行的提供者和消费者，消费者在本地缓存了提供者列表</li>
<li>注册中心和监控中心都是可选的，服务消费者可以直连服务提供者</li>
</ul>

<h3 id="toc_4">健壮性</h3>

<ul>
<li>监控中心宕掉不影响使用，只是丢失部分采样数据</li>
<li>数据库宕掉后，注册中心仍能通过缓存提供服务列表查询，但不能注册新服务</li>
<li>注册中心对等集群，任意一台宕掉后，将自动切换到另一台</li>
<li>注册中心全部宕掉后，服务提供者和服务消费者仍能通过本地缓存通讯</li>
<li>服务提供者无状态，任意一台宕掉后，不影响使用</li>
<li>服务提供者全部宕掉后，服务消费者应用将无法使用，并无限次重连等待服务提供者恢复</li>
</ul>

<h3 id="toc_5">伸缩性</h3>

<ul>
<li>注册中心为对等集群，可动态增加机器部署实例，所有客户端将自动发现新的注册中心</li>
<li>服务提供者无状态，可动态增加机器部署实例，注册中心将推送新的服务提供者信息给消费者</li>
</ul>

<h2 id="toc_6">Dubbo 服务的角色关系</h2>

<p>服务提供方 和 服务消费方 之间的调用关系，如下图所示：<br/>
<figure><img src="media/16024075158165/16024093724823.jpg" alt=""/></figure></p>

<h3 id="toc_7">节点角色说明：</h3>

<table>
<thead>
<tr>
<th>节点</th>
<th>角色说明</th>
</tr>
</thead>

<tbody>
<tr>
<td>Provider</td>
<td>暴露服务的服务提供方</td>
</tr>
<tr>
<td>Consumer</td>
<td>调用远程服务的服务消费方</td>
</tr>
<tr>
<td>Registry</td>
<td>服务注册与发现的注册中心</td>
</tr>
<tr>
<td>Monitor</td>
<td>统计服务的调用次数和调用时间的监控中心</td>
</tr>
<tr>
<td>Container</td>
<td>服务运行容器</td>
</tr>
</tbody>
</table>

<h3 id="toc_8">调用关系说明</h3>

<ul>
<li>0：服务容器负责启动，加载，运行服务提供者。</li>
<li>1：服务提供者在启动时，向注册中心注册自己提供的服务。</li>
<li>2：服务消费者在启动时，向注册中心订阅自己所需的服务。</li>
<li>3：注册中心返回服务提供者地址列表给消费者，如果有变更，注册中心将基于长连接推送变更数据给消费者。</li>
<li>4：服务消费者，从提供者地址列表中，基于软负载均衡算法，选一台提供者进行调用，如果调用失败，再选另一台调用。</li>
<li>5：服务消费者和提供者，在内存中累计调用次数和调用时间，定时每分钟发送一次统计数据到监控中心。</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[解答 Kafka]]></title>
    <link href="http://www.throne4j.com/16022540100685.html"/>
    <updated>2020-10-09T22:33:30+08:00</updated>
    <id>http://www.throne4j.com/16022540100685.html</id>
    <content type="html"><![CDATA[
<h2 id="toc_0">kafka 中的 zookeeper 起到什么作用，可以不用zookeeper 么?(初级)</h2>

<p>zookeeper 是一个分布式的协调组件，早期版本的 kafka 用 zk 做 meta 信息存储， consumer 的消费状态，group 的管理以及 offset 的值。考虑到 zk 本身的一些因素以及整个 架构较大概率存在单点问题，新版本中逐渐弱化了 zookeeper 的作用。新的 consumer 使用 了 kafka 内部的 group coordination 协议，也减少了对 zookeeper 的依赖，但是 broker 依然依 赖于 ZK，zookeeper 在 kafka 中还用来选举和检测 broker 是否存活等等。</p>

<h2 id="toc_1">kafka 中 consumer group 是什么概念(初级)</h2>

<p>同样是逻辑上的概念，是 Kafka 实现单播和广播两种消息模型的手段。同一个 topic 的 数据，会广播给不同的 group;同一个 group 中的 worker，只有一个 worker 能拿到这个数 据。换句话说，对于同一个 topic，每个 group 都可以拿到同样的所有数据，但是数据进入 group 后只能被其中的一个 worker 消费。group 内的 worker 可以使用多线程或多进程来实 现，也可以将进程分散在多台机器上，worker 的数量通常不超过 partition 的数量，且二者 最好保持整数倍关系，因为Kafka在设计时假定了一个partition只能被一个worker消费(同 一 group 内)。</p>

<h2 id="toc_2">kafka 为什么那么快?(中级)</h2>

<p>系统缓存，页面缓存技术。<br/>
顺序写:由于现代的操作系统提供了预读和写技术，磁盘的顺序写大多数情况下比随机 写内存还要快。<br/>
Zero-copy 零拷技术减少拷贝次数。 批量量处理。合并小的请求，然后以流的方式进行交互，直顶网络上限。 Pull 拉模式 使用拉模式进行消息的获取消费，与消费端处理能力相符。</p>

<h2 id="toc_3">Kafka 中是怎么体现消息顺序性的?(中级) kafka 每个 partition 中的消息在写入时都是有序的，消费时，每个 partition 只能被每一</h2>

<p>个 group 中的一个消费者消费，保证了消费时也是有序的。<br/>
整个 topic 不保证有序。如果为了保证 topic 整个有序，那么将 partition 调整为 1.</p>

<h2 id="toc_4">kafka follower 如何与 leader 同步数据(高级)</h2>

<p>Kafka 的复制机制既不是完全的同步复制，也不是单纯的异步复制。完全同步复制要求 All Alive Follower 都复制完，这条消息才会被认为 commit，这种复制方式极大的影响了吞吐 率。而异步复制方式下，Follower 异步的从 Leader 复制数据，数据只要被 Leader 写入 log 就被认为已经 commit，这种情况下，如果 leader 挂掉，会丢失数据，kafka 使用 ISR 的方式 很好的均衡了确保数据不丢失以及吞吐率。</p>

<p>kafka producer 如何优化生产速度 增加线程<br/>
提高 batch.size<br/>
增加更多 producer 实例<br/>
增加 partition 数<br/>
设置 acks=-1 时，如果延迟增大:可以增大 num.replica.fetchers(follower 同步数据的<br/>
线程数)来调解;<br/>
跨数据中心的传输:增加 socket 缓冲区设置以及 OS tcp 缓冲区设置。</p>

<h2 id="toc_5">为什么 Kafka 不支持读写分离?(高级)</h2>

<p>在 Kafka 中，生产者写入消息、消费者读取消息的操作都是与 leader 副本进行交互的，<br/>
从而实现的是一种主写主读的生产消费模型。<br/>
Kafka 并不支持主写从读，因为主写从读有 2 个很明显的缺点:<br/>
(1)数据一致性问题。数据从主节点转到从节点必然会有一个延时的时间窗口，这个时 间 窗口会导致主从节点之间的数据不一致。某一时刻，在主节点和从节点中 A 数据的值 都为 X， 之后将主节点中 A 的值修改为 Y，那么在这个变更通知到从节点之前，应用读 取从节点中的 A 数据的值并不为最新的 Y，由此便产生了数据不一致的问题。<br/>
(2)延时问题。类似 Redis 这种组件，数据从写入主节点到同步至从节点中的过程需要 经 历网络→主节点内存→网络→从节点内存这几个阶段，整个过程会耗费一定的时间。而 在 Kafka 中，主从同步会比 Redis 更加耗时，它需要经历网络→主节点内存→主节点磁盘 →网络→从节 点内存→从节点磁盘这几个阶段。对延时敏感的应用而言，主写从读的功能 并不太适用。</p>

<h2 id="toc_6">有几百万消息持续积压几小时怎么解决?(高级)</h2>

<p>发生了线上故障，几千万条数据在 MQ 里积压很久。是修复 consumer 的问题，让他恢 复消费速度，然后等待几个小时消费完毕?这是个解决方案。不过有时候我们还会进行临时 紧急扩容。</p>

<p>一个消费者一秒是 1000 条，一秒 3 个消费者是 3000 条，一分钟是 18 万条。1000 多万 条，所以如果积压了几百万到上千万的数据，即使消费者恢复了，也需要大概 1 小时的时间 才能恢复过来。</p>

<p>一般这个时候，只能操作临时紧急扩容了，具体操作步骤和思路如下:</p>

<p>先修复 consumer 的问题，确保其恢复消费速度，然后将现有 consumer 都停掉。<br/>
新建一个 topic，partition 是原来的 10 倍，临时建立好原先 10 倍或者 20 倍的 queue 数 量。然后写一个临时的分发数据的 consumer 程序，这个程序部署上去消费积压的数据，消 费之后不做耗时的处理，直接均匀轮询写入临时建立好的 10 倍数量的 queue。</p>

<p>接着临时征用 10 倍的机器来部署 consumer，每一批 consumer 消费一个临时 queue 的 数据。</p>

<p>这种做法相当于是临时将 queue 资源和 consumer 资源扩大 10 倍，以正常的 10 倍速度 来消费数据。</p>

<p>等快速消费完积压数据之后，再恢复原先部署架构，重新用原先的 consumer 机器来消费消息。</p>

<h2 id="toc_7">Kafka 是如何实现高性能的?(高级)</h2>

<p>宏观架构层面利用 Partition 实现并行处理</p>

<p>Kafka 中每个 Topic 都包含一个或多个 Partition，不同 Partition 可位于不同节点。同时 Partition 在物理上对应一个本地文件夹，每个 Partition 包含一个或多个 Segment，每个 Segment 包含一个数据文件和一个与之对应的索引文件。在逻辑上，可以把一个 Partition 当 作一个非常长的数组，可通过这个“数组”的索引(offset)去访问其数据。</p>

<p>一方面，由于不同 Partition 可位于不同机器，因此可以充分利用集群优势，实现机器间 的并行处理。另一方面，由于 Partition 在物理上对应一个文件夹，即使多个 Partition 位于同 一个节点，也可通过配置让同一节点上的不同 Partition 置于不同的 disk drive 上，从而实现 磁盘间的并行处理，充分发挥多磁盘的优势。</p>

<p>利用多磁盘的具体方法是，将不同磁盘mount到不同目录，然后server.properties中， 将 log.dirs 设置为多目录(用逗号分隔)。Kafka 会自动将所有Partition 尽可能均匀分配到不 同目录也即不同目录(也即不同 disk)上。</p>

<p>Partition 是最小并发粒度，Partition 个数决定了可能的最大并行度。。 </p>

<h3 id="toc_8">ISR 实现可用性与数据一致性的动态平衡</h3>

<p>常用数据复制及一致性方案</p>

<ul>
<li><p>Master-Slave</p>
<ul>
<li>RDBMS 的读写分离即为典型的 Master-Slave 方案</li>
<li>同步复制可保证强一致性但会影响可用性</li>
<li>异步复制可提供高可用性但会降低一致性</li>
</ul></li>
<li><p>WNR</p>
<ul>
<li>主要用于去中心化的分布式系统中。</li>
<li>N 代表总副本数，W 代表每次写操作要保证的最少写成功的副本数，R 代表每次读至少要读取的副本数</li>
<li>当 W+R&gt;N 时，可保证每次读取的数据至少有一个副本拥有最新的数据</li>
<li>多个写操作的顺序难以保证，可能导致多副本间的写操作顺序不一致。Dynamo 通过 向量时钟保证最终一致性</li>
</ul></li>
<li><p>Paxos 及其变种</p>
<ul>
<li>Google 的 Chubby，Zookeeper 的原子广播协议(Zab)，RAFT 等</li>
</ul></li>
</ul>

<h3 id="toc_9">基于 ISR 的数据复制方案</h3>

<p>Kafka 的数据复制是以 Partition 为单位的。而多个备份间的数据复制，通过 Follower 向 Leader 拉取数据完成。从一这点来讲，Kafka 的数据复制方案接近于上文所讲的 Master-Slave 方案。不同的是，Kafka 既不是完全的同步复制，也不是完全的异步复制，而是基于 ISR 的 动态复制方案。</p>

<p>ISR，也即 In-sync Replica。每个 Partition 的 Leader 都会维护这样一个列表，该列表中， 包含了所有与之同步的 Replica(包含 Leader 自己)。每次数据写入时，只有 ISR 中的所有 Replica 都复制完，Leader 才会将其置为 Commit，它才能被 Consumer 所消费。</p>

<p>这种方案，与同步复制非常接近。但不同的是，这个 ISR 是由 Leader 动态维护的。如果 Follower 不能紧“跟上”Leader，它将被 Leader 从 ISR 中移除，待它又重新“跟上”Leader 后，会被 Leader 再次加加 ISR 中。每次改变 ISR 后，Leader 都会将最新的 ISR 持久化到 Zookeeper 中。</p>

<p>由于 Leader 可移除不能及时与之同步的 Follower，故与同步复制相比可避免最慢的 Follower 拖慢整体速度，也即 ISR 提高了系统可用性。</p>

<p>ISR 中的所有 Follower 都包含了所有 Commit 过的消息，而只有 Commit 过的消息才会被 Consumer 消费，故从 Consumer 的角度而言，ISR 中的所有Replica 都始终处于同步状态，从 而与异步复制方案相比提高了数据一致性。</p>

<p>ISR 可动态调整，极限情况下，可以只包含 Leader，极大提高了可容忍的宕机的 Follower 的数量。与 Majority Quorum 方案相比，容忍相同个数的节点失败，所要求的总节点数少了 近一半。</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[MySql 事务]]></title>
    <link href="http://www.throne4j.com/16022490047332.html"/>
    <updated>2020-10-09T21:10:04+08:00</updated>
    <id>http://www.throne4j.com/16022490047332.html</id>
    <content type="html"><![CDATA[
<h2 id="toc_0">数据库事务具备ACID特性</h2>

<ul>
<li>原子性(A)：要执行的事务是一个独立的操作单元，要么全部执行，要么全部不执行</li>
<li>一致性(C)：事务的一致性是指事务的执行不能破坏数据库的一致性，一致性也称为完整性。一个事务在执行后，数据库必须从一个一致性状态转变为另一个一致性状态</li>
<li>隔离性(I)：多个事务并发执行时，一个事务的执行不应影响其他事务的执行</li>
<li>持久性(D)：是事务的保证，事务终结的标志(内存的数据持久到硬盘文件中)</li>
</ul>

<h2 id="toc_1">无隔离性会出现的问题：</h2>

<ul>
<li><p>丢失更新<br/>
A 事务撤销时，把已经提交的 B 事务的更新数据覆盖了。这种错误可能造成很严重的问 题，通过下面的账户取款转账就可以看出来，MySQL 通过三级封锁协议的第一级解决了丢 失更新，事务 T 要修改数据 A 时必须加 X 锁，直到 T 结束才释放锁。</p></li>
<li><p>脏读<br/>
脏读主要是读取到了其他事务的数据，而其他事务随后发生回滚。MySQL 通过三级封锁 协议的第二级解决了脏读，在一级的基础上，要求读取数据 A 时必须加 S 锁，读取完马上 释放 S 锁。</p></li>
<li><p>不可重复度<br/>
不可重复读是读取到数据后，随后其他事务对数据发生了修改，无法再次读取。MySQL 通过三级封锁协议的第三级解决了不可重复读。在二级的基础上，要求读取数据 A 时必须 加 S 锁，直到事务结束了才能释放 S 锁。</p></li>
<li><p>幻读 <br/>
幻读是读取到数据后，随后其他事务对数据发生了新增，无法再次读取。在 InnoDB 引擎 Repeatable Read 的隔离级别下，MySQL 通过 Next-Key Lock 以及 MVCC 解决了幻读，事务中 分为当前读以及快照读。</p></li>
</ul>

<h2 id="toc_2">事务隔离性</h2>

<p>SQL92规范中对隔离性定义了 4 种隔离级别（隔离性从上向下依次增强，但是导致的问题是并发能力的减弱）：</p>

<ul>
<li>读未提交(READ UNCOMMITED)
<ul>
<li>事物A和事物B，事物A未提交的数据，事物B可以读取到</li>
<li>这种隔离级别最低，这种级别一般是在理论上存在，数据库隔离级别一般都高于该级别</li>
<li>这里读取到的数据叫做“脏数据”</li>
</ul></li>
<li>读已提交(READ COMMITTED)
<ul>
<li>事物A和事物B，事物A提交的数据，事物B才能读取到</li>
<li>这种隔离级别高于读未提交</li>
<li>换句话说，对方事物提交之后的数据，我当前事物才能读取到</li>
<li>这种级别可以避免“脏数据”</li>
<li>这种隔离级别会导致“不可重复读取”</li>
</ul></li>
<li>可重复读(REPEATABLE READ)
<ul>
<li>事务A和事务B，事务A提交之后的数据，事务B读取不到</li>
<li>事务B是可重复读取数据</li>
<li>这种隔离级别高于读已提交</li>
<li>换句话说，对方提交之后的数据，我还是读取不到</li>
<li>这种隔离级别可以避免“不可重复读取”，达到可重复读取</li>
<li>比如1点和2点读到数据是同一个</li>
<li>MySQL默认级别</li>
<li>虽然可以达到可重复读取，但是会导致“幻读”</li>
</ul></li>
<li>序列化(SERIALIZABLE)。
<ul>
<li>事务A和事务B，事务A在操作数据库时，事务B只能排队等待</li>
<li>这种隔离级别很少使用，吞吐量太低，用户体验差</li>
<li>这种级别可以避免“幻像读”，每一次读取的都是数据库中真实存在数据，事务A与事务B串行，而不并发</li>
</ul></li>
</ul>

<table>
<thead>
<tr>
<th>隔离级别</th>
<th>脏读</th>
<th>不可重复读</th>
<th>幻读</th>
<th>概念</th>
</tr>
</thead>

<tbody>
<tr>
<td>READ UNCOMMITED</td>
<td>√</td>
<td>√</td>
<td>√</td>
<td>事务能够看到其他事务没有提交的修改，当另一个事务又回滚了修改后的情况，又被称为脏读dirty read</td>
</tr>
<tr>
<td>READ COMMITTED</td>
<td>×</td>
<td>√</td>
<td>√</td>
<td>事务能够看到其他事务提交后的修改，这时会出现一个事务内两次读取数据可能因为其他事务提交的修改导致不一致的情况，称为不可重复读</td>
</tr>
<tr>
<td>REPEATABLE READ</td>
<td>×</td>
<td>×</td>
<td>√</td>
<td>事务在两次读取时读取到的数据的状态是一致的</td>
</tr>
<tr>
<td>SERIALIZABLE</td>
<td>×</td>
<td>×</td>
<td>×</td>
<td>可重复读中可能出现第二次读读到第一次没有读到的数据，也就是被其他事务插入的数据，这种情况称为幻读phantom read, 该级别中不能出现幻读</td>
</tr>
</tbody>
</table>

<p>大多数数据库系统的默认隔离级别都是READ COMMITTED（但MySQL不是)，InnoDB存储引擎默认隔离级别REPEATABLE READ，通过多版本并发控制（MVCC，Multiversion Concurrency Control）解决了幻读的问题。</p>

<h3 id="toc_3">隔离级别的实现原理</h3>

<p>为了解决更新丢失、脏读、不可重复读、幻读的问题，MySQL事务提出了4个不同的隔离级别，而这些隔离级别的实现本质上就是通过加锁，解锁来实现的。</p>

<p>那么我们该何时加锁，占锁多长时间，何时解锁呢？这就是我们今天的主题三级封锁协议。三级封锁协议顾名思义是3个不同级别的封锁协议，它们是以何时加锁，何时解锁来区分的。下面我们看一下具体的定义：</p>

<ul>
<li><p>一级封锁协议<br/>
事务T在修改数据R之前必须先对其加X锁，直到事务结束才释放。事务结束包括正常结束（COMMIT）和非正常结束（ROLLBACK）。 一级封锁协议可以防止丢失修改，并保证事务T是可恢复的。使用一级封锁协议可以解决丢失修改问题。在一级封锁协议中，如果仅仅是读数据不对其进行修改，是不需要加锁的，它不能保证可重复读和不读“脏”数据。</p></li>
<li><p>二级封锁协议<br/>
在一级封锁协议之上，事务T在读取数据R之前必须先对其加S锁，读完后方可释放S锁。 二级封锁协议除防止了丢失修改，还可以进一步防止读“脏”数据。但在二级封锁协议中，由于读完数据后即可释放S锁，所以它不能保证可重复读。</p></li>
<li><p>三级封锁协议<br/>
在一级封锁协议之上，事务T在读取数据R之前必须先对其加S锁，直到事务结束才释放。 三级封锁协议除防止了丢失修改和不读“脏”数据外，还进一步防止了不可重复读。</p></li>
</ul>

<p>在继续往下之前，我们需要先搞清楚什么是X锁，S锁。如果我们直接在网上搜索，我们能得到这样的一个关系：</p>

<p>排它锁 == 写锁 == X锁 ， 共享锁 == 读锁 == S锁</p>

<p>简单理解就是如果我对资源A加上了排它锁，那么我既可以读取资源A，也可以插入或更新资源A，而其他人都无法对资源A再加排它锁或共享锁。 如果我对资源A加上了共享锁，那么所有人都不能再对资源A加上排它锁 （包括我自己），而其他人也都可以对资源A再加共享锁。</p>

<p>关于X锁和S锁，我们需要了解，普通的select语句是不需要加锁的，而insert，update，delete，select ... for update 需要加X锁，select ... lock in share mode 这样的语句会加S锁。</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[解答RabbitMQ]]></title>
    <link href="http://www.throne4j.com/16022464930141.html"/>
    <updated>2020-10-09T20:28:13+08:00</updated>
    <id>http://www.throne4j.com/16022464930141.html</id>
    <content type="html"><![CDATA[
<h2 id="toc_0">为什么使用消息队列</h2>

<p>首先 消息队列关注于数据的发送和接收，利用高效可靠的异步消息传递机制对分布式系统中的其余各个子系统进行集成。但是它拥有以下优点：</p>

<ul>
<li>高效:对于消息的处理处理速度快。 </li>
<li>可靠:一般消息中间件都会有消息持久化机制和其他的机制确保消息不丢失。 </li>
<li>异步:指发送完一个请求，不需要等待返回，随时可以再发送下一个请求，既不需要等待。</li>
</ul>

<p>使用消息队列可以实现应用之间的解耦、异步调用和削峰填谷。</p>

<p>但是引入消息队列也有如下一些问题：</p>

<ul>
<li>系统的可用性降低</li>
<li>系统的复杂度变高</li>
<li>一致性问题</li>
</ul>

<h2 id="toc_1">使用了消息中间件之后消息可能重复的原因，如何解决？</h2>

<h3 id="toc_2">重复的原因：</h3>

<ul>
<li><p>消息发送端应用重复发送</p>
<ul>
<li>消息发送端发送消息给消息中间件,消息中间件收到消息并成功存储,而这时消息中 间件出现了问题,导致应用端没有收到消息发送成功的返回因而进行重试产生了重 复。</li>
<li>消息中间件因为负载高响应变慢,成功把消息存储到消息存储中后,返回“成功”这 个结果时超时。</li>
<li>消息中间件将消息成功写入消息存储,在返回结果时网络出现问题,导致应用发送端 重试,而重试时网络恢复,由此导致重复。</li>
</ul></li>
<li><p>消息到达了消息存储，由消息中间件进行向外的投递时产生重复</p>
<ul>
<li>消息被投递到消息接收者进行处理，处理完毕后应用出现问题，消息中间件不知道消息的处理结果，会再次投递消息</li>
<li>消息被投递到消息接收者进行处理，处理完毕后网络出现问题，消息中间件不知道消息的处理结果，会再次投递消息</li>
<li>消息被投递到消息接收者进行处理，处理时间比较长，消息中间件因为消息超时会再次投递</li>
<li>消息被投递到消息接收者进行处理，处理完毕后消息中间件出现问题没能收到消息结果并处理，会再次投递消息</li>
<li>消息被投递到消息接收者进行处理，处理完毕消息中间件收到结果，但是遇到消息存储故障，没能更新投递状态，会再次投递消息</li>
</ul></li>
</ul>

<h3 id="toc_3">如何解决消息重复的问题？</h3>

<p>主要是要求消息接收者来处理这种重复的情况,也就是要 求消息接收者的消息处理是幂等操作。</p>

<h4 id="toc_4">什么是幂等性</h4>

<p>对于消息接收端的情况,幂等的含义是采用同样的输入多次调用处理函数,得到同样的结 果。</p>

<h4 id="toc_5">常见幂等的方法</h4>

<p>因此应对消息重复的办法是,使消息接收端的处理是一个幂等操作。这样的做法降低了 消息中间件的整体复杂性,不过也给使用消息中间件的消息接收端应用带来了一定的限制和门槛。</p>

<h5 id="toc_6">MVCC</h5>

<p>多版本并发控制，乐观锁的一种实现，在生产者发送消息时进行数据更新时需要带上数据的版本号，消费者去更新时需要去比较持有的数据版本号，版本号不一致的操作无法成功。</p>

<h5 id="toc_7">去重表</h5>

<p>利用数据库表单的特性来实现幂等，常用的一个思路是在表上构建唯一性索引，保证某<br/>
一类数据一旦执行完毕，后续同样的请求不再重复处理了</p>

<h2 id="toc_8">RabbitMQ中 channel、exchange、queue概念及作用</h2>

<p>Queue 就是消息队列，用于存储消息，具有自己的 erlang 进程。</p>

<p>exchange 内部实现为 保存 binding 关系的查找表;</p>

<p>channel 是实际进行路由工作的实体，即负责按照 routing_key 将 message 投递给 queue 。</p>

<p>在 RabbitMQ 中所有客户端与 RabbitMQ 之间的通讯都是在 channel 上，channel 是真实 TCP 连接之上的虚拟连接，所有 AMQP 命令都是通过 channel 发送的。</p>

<h2 id="toc_9">RabbitMQ 中的元数据有哪些？</h2>

<p>元数据主要分为 </p>

<ul>
<li>Queue 元数据(queue 名字和属性等)</li>
<li>Exchange 元数据(exchange 名字、类型和属性等)</li>
<li>Binding 元数据(存放路由关系的查找表)</li>
<li>Vhost 元数据(vhost 范围内针对前三者的名字空间约束和安全属性设置)，另外在集群中，元数据都是在一个 broker 中都是全局复制的。</li>
</ul>

<h2 id="toc_10">RabbitMQ中的vhost 是什么?起什么作用?</h2>

<p>vhost 可以理解为虚拟 broker ，即一个迷你版的 RabbitMQ server。其内部均含有独立 的 queue、exchange 和 binding 等，但最最重要的是，其拥有独立的权限系统，可以做到 vhost 范围的用户控制。当然，从 RabbitMQ 的全局角度，vhost 可以作为不同权限隔离的手段。</p>

<h2 id="toc_11">RabbitMQ 上的一个 queue 中存放的 message 是 否有数量限制?</h2>

<p>默认情况下一般是无限制，因为限制取决于机器的内存，但是消息过多会导致处理效率 的下降。同时可以通过参数来限制， x-max-length :对队列中消息的条数进行限制 ， x-max-length-bytes :对队列中消息的总量进行限制。</p>

<h2 id="toc_12">为什么对所有的 message 都使用持久化机制?</h2>

<p>首先，必然导致性能的下降，因为写磁盘比写内存慢的多，Rabbit 的吞吐量有 10 倍的差距。</p>

<p>其次，message 的持久化机制用在 RabbitMQ 的集群时会出现“坑爹”问题。矛盾点 在于，要实现持久化的话，必须消息、消息队列、交换器三者持久化，如果集群中不同机器 中三者属性有差异，会发生不可预料的问题。</p>

<p>所以一般处理原则是:仅对关键消息作持久化 处理(根据业务重要程度)，且应该保证关键消息的量不会导致性能瓶颈。 </p>

<h2 id="toc_13">RAM node 和 Disk node 的区别？</h2>

<p>RAM node 就是内存节点，Rabbit 中的 queue、exchange 和 binding 等 RabbitMQ 基础 构件中相关元数据保存到内存中，</p>

<p>Disk node 是磁盘节点，上述数据会在内存和磁盘中均进 行存储。</p>

<p>一般在 RabbitMQ 集群中至少存在一个 Disk node.</p>

<h2 id="toc_14">RabbitMQ 如何确保消息的可靠性传输</h2>

<p>因为 MQ 中涉及到了 MQ 本身，生产者和消费，所以需要从三个角度来看</p>

<h3 id="toc_15">生产者</h3>

<p>生产者将数据发送到 RabbitMQ 的时候，可能数据就在半路给搞丢了，因为网络充斥着不稳定性，有以下几种方案：</p>

<h4 id="toc_16">选择RabbitMQ 提供的事务功能</h4>

<p>选择RabbitMQ的事务功能 就是生产者发送数据之前开启 RabbitMQ 事务(channel.txSelect)，然后发送消息，如果消息没有成功被 RabbitMQ 接收到， 那么生产者会收到异常报错，此时就可以回滚事务(channel.txRollback)，然后重试发送消 息;如果收到了消息，那么可以提交事务(channel.txCommit)。但是问题是，RabbitMQ 事 务机制一搞，基本上吞吐量会下来，因为太耗性能。</p>

<h4 id="toc_17">开启confirm 模式</h4>

<p>在生产者 那里设置开启 confirm 模式之后，你每次写的消息都会分配一个唯一的 id，然后如果写入了 RabbitMQ 中，RabbitMQ 会给你回传一个 ack 消息，告诉你说这个消息 ok 了。</p>

<p>如果 RabbitMQ 没能处理这个消息，会回调你一个 nack 接口，告诉你这个消息接收失败，你可以重试。而 且你可以结合这个机制自己在内存里维护每个消息 id 的状态，如果超过一定时间还没接收 到这个消息的回调，那么你可以重发。</p>

<p>事务机制和 cnofirm 机制最大的不同在于，事务机制是同步的，你提交一个事务之后会 阻塞在那儿，但是 confirm 机制是异步的，你发送个消息之后就可以发送下一个消息，然后 那个消息 RabbitMQ 接收了之后会异步回调你一个接口通知你这个消息接收到了。</p>

<p>所以一般在生产者这块避免数据丢失，都是用 confirm 机制的。</p>

<h3 id="toc_18">RabbitMQ 自身</h3>

<p>RabbitMQ 自己丢数据，这个时候我们就必须开启 RabbitMQ 的持久化，结合confirm模式，等到消息持久化到磁盘之后才会通知生产者，就算这时RabbitMQ挂掉了，我们也可以自己重发。</p>

<h3 id="toc_19">消费端</h3>

<p>RabbitMQ 如果丢失了数据，主要是因为你消费的时候，刚消费到，还没处理，结果进程挂了，比如重启了，那么就尴尬了，RabbitMQ 认为你都消费了，这数据就丢了。</p>

<p>这个时候得用 RabbitMQ 提供的 ack 机制，简单来说，就是你关闭 RabbitMQ 自动 ack，进行手动确认，只有程序手动确认消息已消费才会在RabbitMQ中删除消息，这样消息就不会丢啦。</p>

<h2 id="toc_20">RabbitMQ 如何保证消息的顺序性</h2>

<p>从根本上说，异步消息是不应该有顺序依赖的。在 MQ 上估计是没法解决。要实现严格 的顺序消息，简单且可行的办法就是:保证生产者 - MQServer - 消费者是一对一对一的关系。</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[RabbitMQ--队列]]></title>
    <link href="http://www.throne4j.com/16022443352683.html"/>
    <updated>2020-10-09T19:52:15+08:00</updated>
    <id>http://www.throne4j.com/16022443352683.html</id>
    <content type="html"><![CDATA[
<p>RabbitMQ 中消费行为主要跟队列有直接关系，那么我们接下来深入的分析队列。</p>

<h2 id="toc_0">临时队列</h2>

<p>临时队列对应的是没有持久化的队列，也就是如果 RabbitMQ 服务器重启，那么这些队列就不会存在，所以我们称之为临时队列。</p>

<h2 id="toc_1">自动删除队列</h2>

<p>自动删除队列和普通队列在使用上没有什么区别，唯一的区别是，当消费者断开连接时，队列将会被删除。</p>

<p>自动删除队列允许的消费者没有限制， 也就是说当这个队列上最后一个消费者断开连接才会执行删除。</p>

<p>自动删除队列只需要在声明队列时，设置属性 auto-delete 标识为 true 即可</p>

<h2 id="toc_2">单消费者队列</h2>

<p>普通队列允许的消费者没有限制，多个消费者绑定到多个队列时，RabbitMQ 会采用轮询进行投递。如果需要消费者独占队列，在队列创建的时候， 设定属性参数exclusive 为 true。</p>

<h2 id="toc_3">自动过期队列</h2>

<p>指队列在超过一定时间没使用，队列会从 RabbitMQ 中被删除。</p>

<p>什么是没使用?</p>

<ul>
<li>一定时间内没有 Get 操作发生。</li>
<li>没有 Consumer 连接在队列上</li>
</ul>

<p>就算一直有消息进入队列，也不算队列在被使用。 通过声明队列时，设定x-expires 参数即可，单位毫秒。</p>

<h2 id="toc_4">队列的持久性</h2>

<p>持久化队列和非持久化队列的区别是，持久化队列会被保存在磁盘中，固定并持久的存储，当 RabbitMQ 服务重启后，该队列会保持原来的状态在 RabbitMQ 中被管理，而非持久化队列不会被保存在磁盘中，RabbitMQ 服务重启后队列就会消失。</p>

<h2 id="toc_5">队列级别消息过期</h2>

<p>就是为每个队列设置消息的超时时间。只要给队列设置 x-message-ttl 参数，就设定了该队列所有消息的存活时间，时间单位是毫秒。如果声明队列时指定了死信交换器，则过期消息会成为死信消息。</p>

<h2 id="toc_6">队列参数列表</h2>

<table>
<thead>
<tr>
<th>参数</th>
<th>目的</th>
</tr>
</thead>

<tbody>
<tr>
<td>x-dead-letter-exchange</td>
<td>死信交换机</td>
</tr>
<tr>
<td>x-dead-letter-routing-key</td>
<td>死信消息的可选路由键</td>
</tr>
<tr>
<td>x-expires</td>
<td>队列在指定毫秒数后被删除</td>
</tr>
<tr>
<td>x-ha-policy</td>
<td>创建 HA 队列</td>
</tr>
<tr>
<td>x-ha-nodes</td>
<td>HA 队列的分布节点</td>
</tr>
<tr>
<td>x-max-length</td>
<td>队列的最大消息条数</td>
</tr>
<tr>
<td>x-max-length-bytes</td>
<td>消息的最大总量</td>
</tr>
<tr>
<td>x-message-ttl</td>
<td>毫秒为单位的消息过期时间，队列级别</td>
</tr>
<tr>
<td>x-max-prority</td>
<td>最大优先值为255的队列优先排序功能</td>
</tr>
</tbody>
</table>

<h2 id="toc_7">消息的属性</h2>

<p>按照 AMQP 的协议单个最大的消息大小为 16EB(2 的 64 次方)，但是 RabbitMQ 将消息大小限定为 2GB(2的31次方)。<br/>
<figure><img src="media/16022443352683/16022456945696.jpg" alt="" style="width:882px;"/></figure></p>

<p><figure><img src="media/16022443352683/16022457151235.jpg" alt=""/></figure></p>

<h3 id="toc_8">消息存活时间</h3>

<p>当队列消息的 TTL 和消息 TTL 都被设置，时间短的 TTL 设置生效。<br/>
如果将一个过期消息发送给 RabbitMQ，该消息不会路由到任何队列，而是直接丢弃。</p>

<p>为消息设置 TTL 有一个问题:RabbitMQ 只对处于队头的消息判断是否过期(即不会扫描队列)，所以，很可能队列中已存在死消息，但是队列并不 知情。这会影响队列统计数据的正确性，妨碍队列及时释放资源。</p>

<h3 id="toc_9">消息的持久化</h3>

<p>默认情况下，队列和交换器在服务器重启后都会消失，消息当然也是。将队列和交换器的 durable 属性设为 true，缺省为 false，但是消息要持久化还 不够，还需要将消息在发布前，将投递模式设置为 2。消息要持久化，必须要有持久化的队列、交换器和投递模式都为 2 。</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[RabbitMQ--死信交换器 DLX]]></title>
    <link href="http://www.throne4j.com/16022411349774.html"/>
    <updated>2020-10-09T18:58:54+08:00</updated>
    <id>http://www.throne4j.com/16022411349774.html</id>
    <content type="html"><![CDATA[
<p><figure><img src="media/16022411349774/16022427035514.jpg" alt="" style="width:851px;"/></figure></p>

<p>如果使用消息拒绝机制，同时 requeue 参数设置为 false 时，消息丢失了，这点作为程序员我们不能忍。</p>

<p>所以 RabbitMQ 作为一个高级消息中间件，提出了死信交换器的概念，死信，意思就是死了的信息。这种交换器专门处理死了的信息(被拒绝可以重新投递的信息不能算死的)。</p>

<p>死信交换器是 RabbitMQ 对 AMQP 规范的一个扩展，往往用在对问题消息的诊断上(主要针对消费者)，还有延时队列的功能。</p>

<p>消息变成死信一般是以下三种情况:</p>

<ul>
<li>消息被拒绝，并且设置 requeue 参数为 false</li>
<li>消息过期(默认情况下 Rabbit 中的消息不过期，但是可以设置队列的过期时间和消息的过期时间以达到消息过期的效果) </li>
<li>队列达到最大长度(一般当设置了最大队列长度或大小并达到最大值时)</li>
</ul>

<p>死信交换器仍然只是一个普通的交换器，创建时并没有特别要求和操作。在创建队列的时候，声明该交换器将用作保存被拒绝的消息即可，即设置参数 x-dead-letter-exchange 指定哪个交换机为死信交换机。参数 x-dead-letter-routing-key 指定 死信routing-key。</p>

<p><figure><img src="media/16022411349774/16022418380839.jpg" alt=""/></figure></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[RabbitMQ--消息的消费]]></title>
    <link href="http://www.throne4j.com/16022373268306.html"/>
    <updated>2020-10-09T17:55:26+08:00</updated>
    <id>http://www.throne4j.com/16022373268306.html</id>
    <content type="html"><![CDATA[
<h2 id="toc_0">消息的获取方式</h2>

<h3 id="toc_1">拉取 Get</h3>

<p>属于一种轮询模型，发送一次 get 请求，获得一个消息。如果此时 RabbitMQ 中没有消息，会获得一个表示空的回复。总的来说，这种方式性能比较 差，很明显，每获得一条消息，都要和 RabbitMQ 进行网络通信发出请求。而且对 RabbitMQ 来说，RabbitMQ 无法进行任何优化，因为它永远不知道应用 程序何时会发出请求。对我们实现者来说，要在一个循环里，不断去服务器 get 消息。</p>

<h3 id="toc_2">推送 Consume</h3>

<p>属于一种推送模型。注册一个消费者后，RabbitMQ 会在消息可用时，自动将消息进行推送给消费者，这种模式我们已经使用过很多次。</p>

<h3 id="toc_3">消息的应答</h3>

<p>消费者收到的每一条消息都必须进行确认。消息确认后，RabbitMQ 才会从队列删除这条消息，RabbitMQ 不会为未确认的消息设置超时时间，它判断此消息是否需要重新投递给消费者的唯一依据是消费该消息的消费者连接是否已经断开。这么设计的原因是 RabbitMQ 允许消费者消费一条消 息的时间可以很久很久。</p>

<h3 id="toc_4">自动确认</h3>

<p>消费者在声明队列时，可以指定 autoAck 参数，当 autoAck=true 时，一旦消费者接收到了消息，就视为自动确认了消息。如果消费者在处理消息的过 程中，出了错，就没有什么办法重新处理这条消息，所以我们很多时候，需要在消息处理成功后，再确认消息，这就需要手动确认。</p>

<h3 id="toc_5">手动确认</h3>

<p>当 autoAck=false 时，RabbitMQ 会等待消费者显式发回 ack 信号后才从内存(和磁盘，如果是持久化消息的话)中移去消息。否则，RabbitMQ 会在队列 中消息被消费后立即删除它。</p>

<p>采用消息确认机制后，只要令 autoAck=false，消费者就有足够的时间处理消息(任务)，不用担心处理消息过程中消费者进程挂掉后消息丢失的问题， 因为 RabbitMQ 会一直持有消息直到消费者显式调用 basicAck 为止。</p>

<p>当 autoAck=false 时，对于 RabbitMQ 服务器端而言，队列中的消息分成了两部分:一部分是等待投递给消费者的消息;一部分是已经投递给消费者， 但是还没有收到消费者 ack 信号的消息。如果服务器端一直没有收到消费者的 ack 信号，并且消费此消息的消费者已经断开连接，则服务器端会安排该消 息重新进入队列，等待投递给下一个消费者(也可能还是原来的那个消费者)</p>

<h3 id="toc_6">QoS 预取模式</h3>

<p>在确认消息被接收之前，消费者可以预先要求接收一定数量的消息，在处理完一定数量的消息后，批量进行确认。如果消费者应用程序在确认消息之前崩溃，则所有未确认的消息将被重新发送给其他消费者。所以这里存在着一定程度上的可靠性风险。</p>

<p>这种机制一方面可以实现限速(将消息暂存到 RabbitMQ 内存中)的作用，一方面可以保证消息确认质量(比如确认了但是处理有异常的情况)。</p>

<p><strong>注意</strong>: 消费确认模式必须是非自动 ACK 机制(这个是使用 baseQos 的前提条件，否则会 Qos 不生效)，然后设置 basicQos 的值;另外，还可以基于 consume 和 channel 的粒度进行设置(global)。</p>

<p>basicQos 方法参数详细解释:</p>

<ul>
<li>prefetchSize:最多传输的内容的大小的限制，0 为不限制，但据说 prefetchSize 参数，rabbitmq 没有实现。</li>
<li>prefetchCount:会告诉 RabbitMQ 不要同时给一个消费者推送多于 N 个消息，即一旦有 N 个消息还没有 ack，则该 consumer 将 block 掉，直到有消息 ack</li>
<li>global:true\false 是否将上面设置应用于 channel，简单点说，就是上面限制是 channel 级别的还是 consumer 级别。</li>
</ul>

<p>如果同时设置 channel 和消费者，会怎么样?AMQP 规范没有解释如果使用不同的全局值多次调用 basic.qos 会发生什么。 RabbitMQ 将此解释为意味着两个预取限制应该彼此独立地强制执行; 消费者只有在未达到未确认消息限制时才会收到新消息。 </p>

<h2 id="toc_7">消费者中的事务</h2>

<p>使用方法和生产者一致 假设消费者模式中使用了事务，并且在消息确认之后进行了事务回滚，会是什么样的结果? 结果分为两种情况:</p>

<ul>
<li>autoAck=false 手动应对的时候是支持事务的，也就是说即使你已经手动确认了消息已经收到了，但 RabbitMQ 对消息的确认会等事务的 返回结果，再做最终决定是确认消息还是重新放回队列，如果你手动确认之后，又回滚了事务，那么以事务回滚为准，此条消息会重新放回队列;</li>
<li>autoAck=true 如果自动确认为 true 的情况是不支持事务的，也就是说你即使在收到消息之后在回滚事务也是于事无补的，队列已经把 消息移除了。</li>
</ul>

<h2 id="toc_8">消息的拒绝</h2>

<h3 id="toc_9">Reject 和 Nack</h3>

<p>消息确认可以让 RabbitMQ 知道消费者已经接受并处理完消息。但是如果消息本身或者消息的处理过程出现问题怎么办?需要一种机制通知 RabbitMQ 这个消息我无法处理，请让别的消费者处理。</p>

<p>这里就有两种机制，Reject 和 Nack。</p>

<ul>
<li>Reject 在拒绝消息时，可以使用 requeue 标识，告诉 RabbitMQ 是否需要重新发送给别的消费者。如果是 false 则不重新发送，一般这个消息就会被RabbitMQ 丢弃。Reject 一次只能拒绝一条消息。如果是 true 则消息发生了重新投递。</li>
<li>Nack 跟 Reject 类似，只是它可以一次性拒绝多个消息。也可以使用 requeue 标识，这是 RabbitMQ 对 AMQP 规范的一个扩展。</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[RabbitMQ--消息发布时的权衡]]></title>
    <link href="http://www.throne4j.com/16022331778802.html"/>
    <updated>2020-10-09T16:46:17+08:00</updated>
    <id>http://www.throne4j.com/16022331778802.html</id>
    <content type="html"><![CDATA[
<p>在 RabbitMQ 中，有不同的投递机制(生产者)，但是每一种机制都对性能有一定的影响。一般来讲速度快的可靠性低，可靠性好的性能差，具体怎 么使用需要根据你的应用程序来定，所以说没有最好的方式，只有最合适的方式。只有把你的项目和技术相结合，才能找到适合你的平衡。</p>

<p><figure><img src="media/16022331778802/16022332641946.jpg" alt=""/></figure></p>

<p>在 RabbitMQ 中实际项目中，生产者和消费者都是客户端，它们都可以完成申明交换器、申明队列和绑定关系，但是在我们的实战过程中，我们在生产者代码中申明交换器，在消费者代码中申明队列和绑定关系。</p>

<p>另外还要申明的就是，生产者发布消息时不一定非得需要消费者，对于 RabbitMQ 来说，如果是单纯的生产者你只需要生产者客户端、申明交换器、 申明队列、确定绑定关系，数据就能从生产者发送至 RabbitMQ。</p>

<h2 id="toc_0">无保障</h2>

<p>通过 basicPublish 发布你的消息并使用正确的交换器和路由信息，你的消息会被接收并发送到合适的队列中, 但是如果有网络问题，或者消息不可路由，或者RabbitMQ 自身有问题的话，这种方式就有风险。所以无保证的消息发送一般情况下不推荐。</p>

<h2 id="toc_1">失败确认</h2>

<p>在发送消息时设置 mandatory = true 标志，告诉 RabbitMQ，如果消息不可路由，应该将消息返回给发送者，并通知失败。可以这样认为，开启 mandatory 是开启故障检测模式。</p>

<p>注意:它只会让 RabbitMQ 向你通知失败，而不会通知成功。如果消息正确路由到队列，则发布者不会受到任何通知。带来的问题是无法确保发布消 息一定是成功的，因为通知失败的消息可能会丢失。</p>

<p>channel.addConfirmListener 则用来监听 RabbitMQ 发回的信息。</p>

<h2 id="toc_2">事务</h2>

<p>事务的实现主要是对信道(Channel)的设置，主要的方法有三个:</p>

<ul>
<li>channel.txSelect()声明启动事务模式;</li>
<li>channel.txComment()提交事务;</li>
<li>channel.txRollback()回滚事务;</li>
</ul>

<p>在发送消息之前，需要声明 channel 为事务模式，提交或者回滚事务即可。 开启事务后，客户端和 RabbitMQ 之间的通讯交互流程:</p>

<ul>
<li>客户端发送给服务器 Tx.Select(开启事务模式)</li>
<li>服务器端返回 Tx.Select-Ok(开启事务模式 ok)  推送消息</li>
<li>客户端发送给事务提交 Tx.Commit</li>
<li>服务器端返回 Tx.Commit-Ok</li>
</ul>

<p>以上就完成了事务的交互流程，如果其中任意一个环节出现问题，就会抛出 IoException，这样用户就可以拦截异常进行事务回滚，或决定要不要重<br/>
复消息。</p>

<p>既然已经有事务了，为何还要使用发送方确认模式呢，原因是因为事务的性能是非常差的。根据相关资料，事务会降低 2~10 倍的性能。</p>

<h2 id="toc_3">发送方确认模式</h2>

<p>基于事务的性能问题，RabbitMQ 团队为我们拿出了更好的方案，即采用发送方确认模式，该模式比事务更轻量，性能影响几乎可以忽略不计。</p>

<p>原理:生产者将信道设置成 confirm 模式，一旦信道进入 confirm 模式，所有在该信道上面发布的消息都将会被指派一个唯一的 ID(从 1 开始)，由这个 id 在生产者和 RabbitMQ 之间进行消息的确认。 </p>

<p>不可路由的消息，当交换器发现，消息不能路由到任何队列，会进行确认操作，表示收到了消息。如果发送方设置了 mandatory 模式,则会先调用 addReturnListener 监听器。</p>

<p>可路由的消息，要等到消息被投递到所有匹配的队列之后，broker 会发送一个确认给生产者(包含消息的唯一 ID)，这就使得生产者知道消息已经正确 到达目的队列了，如果消息和队列是可持久化的，那么确认消息会在将消息写入磁盘之后发出，broker 回传给生产者的确认消息中 delivery-tag 域包含了 确认消息的序列号。</p>

<p>confirm 模式最大的好处在于它可以是异步的，一旦发布一条消息，生产者应用程序就可以在等信道返回确认的同时继续发送下一条消息，当消息最终得到确认之后，生产者应用便可以通过回调方法来处理该确认消息，如果 RabbitMQ 因为自身内部错误导致消息丢失，就会发送一条 nack 消息，生产者应用程序同样可以在回调方法中处理该 nack 消息决定下一步的处理。</p>

<p>Confirm 的三种实现方式:</p>

<ul>
<li>channel.waitForConfirms() 普通发送方确认模式，消息到达交换机，机会返回 true</li>
<li>channel.waitForConfirmsOrDie() 批量确认模式，使用同步方式等所有的消息发送之后才会执行后面代码，只要有一个小心未到达交换器就会抛出 IOException 异常</li>
<li>channel.addConfirmListener()一步监听发送方确认模式</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[RabbitMQ--AMQP 概论]]></title>
    <link href="http://www.throne4j.com/16022252063226.html"/>
    <updated>2020-10-09T14:33:26+08:00</updated>
    <id>http://www.throne4j.com/16022252063226.html</id>
    <content type="html"><![CDATA[
<p>AMQP 是应用层协议的一个开放标准, 为面向消息的中间件设计。基于此协议的客户端与消息中间件可传递消息，并不受客户端/中间件不同产品，不同的开发语言等条件的限制。目标是实现一种在全行业广泛使用的标准消息中间件技术，以便降低企业和系统集成的开销，并且向大众提供工业级的集成服务。 主要实现有 RabbitMQ。</p>

<h2 id="toc_0">客户端与 RabbitMQ 的通讯</h2>

<h3 id="toc_1">连接</h3>

<p>首先作为客户端(无论是生产者还是消费者)，如果要与 RabbitMQ 通讯的话，客户端与服务端之间必须创建一条 TCP 连接，当然同时建立连接后，客户端还必须发送一条“问候语”让彼此知道我们都是符合 AMQP 的语言的，比如你跟别人打招呼一般会说“你好!”，你跟国外的美女一般会说“hello!”一样。 你们确认好“语言”之后，就相当于客户端和 RabbitMQ 通过“认证”了。你们之间可以创建一条 AMQP 的信道。</p>

<p>连接在 RabbitMQ 原生客户端(5.0.0)版本中默认使用 java 的原生 socket，但是也支持 NIO，需要手动设置修改。</p>

<h3 id="toc_2">信道</h3>

<p>信道是生产者/消费者与 RabbitMQ 通信的渠道。信道是建立在 TCP 连接上的虚拟连接，什么意思呢? 就是说 rabbitmq 在一条 TCP 上建立成百上千个信道来达到多个线程处理，这个 TCP 被多个线程共享，每个线程对应一个信道，信道在RabbitMQ 都有唯一的 ID ,保证了信道私有性，对应上唯一的线程使用。</p>

<p>疑问: 为什么不建立多个 TCP 连接呢? </p>

<p>原因是 rabbit 保证性能，系统为每个线程开辟一个 TCP 是非常消耗性能，每秒成百上千的建立销毁 TCP 会严重消耗系统。</p>

<p>所以 rabbitmq 选择建立多个信道(建立在 tcp 的虚拟连接)连接到 rabbit 上。<br/>
从技术上讲，这被称之为“多路复用”，对于执行多个任务的多线程或者异步应用程序来说，好使的很。</p>

<h2 id="toc_3">RabbitMQ 中使用 AMQP</h2>

<h3 id="toc_4">包括的要素</h3>

<ul>
<li>生产者<br/>
消息的创建者，发送到RabbitMQ</li>
<li>消费者<br/>
连接到 RabbitMQ 订阅到队列上，消费消息，持续订阅(basicConsumer)和单条订阅(basicGet)</li>
<li>包含有效载荷和标签，有效载荷指要传输的数据，标签描述了有效载荷，并且 RabbitMQ用它来决定谁获得消息，消费者只能拿到有效载荷，病不知道生产者是谁。</li>
</ul>

<h3 id="toc_5">交换器、队列、绑定、路由键</h3>

<p>队列通过路由键(routing key，某种确定的规则)绑定到交换器，生产者将消息发布到交换器，交换器根据绑定的路由键将消息路由到特定队列， 然后由订阅这个队列的消费者进行接收。（routing_key和 丙丁见 binding_key 的最大长度是 255 个字节）</p>

<p><figure><img src="media/16022252063226/16022280502106.jpg" alt="" style="width:882px;"/></figure></p>

<h3 id="toc_6">消息的确认</h3>

<p>消费者收到的每一条消息都必须进行确认(自动确认和自行确认)。</p>

<p>消费者在声明队列时，可以指定 autoAck 参数，当 autoAck=false 时，RabbitMQ 会等待消费者显式发回 ack 信号后才从内存(和磁盘，如果是持久化消<br/>
息的话)中移去消息。否则，RabbitMQ 会在队列中消息被消费后立即删除它。</p>

<p>采用消息确认机制后，只要令 autoAck=false，消费者就有足够的时间处理消息(任务)，不用担心处理消息过程中消费者进程挂掉后消息丢失的问题， 因为 RabbitMQ 会一直持有消息直到消费者显式调用 basicAck 为止。</p>

<p>当 autoAck=false 时，对于 RabbitMQ 服务器端而言，队列中的消息分成了两部分:一部分是等待投递给消费者的消息;一部分是已经投递给消费者， 但是还没有收到消费者 ack 信号的消息。如果服务器端一直没有收到消费者的 ack 信号，并且消费此消息的消费者已经断开连接，则服务器端会安排该消 息重新进入队列，等待投递给下一个消费者(也可能还是原来的那个消费者)。</p>

<p>RabbitMQ 不会为未 ack 的消息设置超时时间，它判断此消息是否需要重新投递给消费者的唯一依据是消费该消息的消费者连接是否已经断开。这么 设计的原因是 RabbitMQ 允许消费者消费一条消息的时间可以很久很久</p>

<h2 id="toc_7">虚拟主机</h2>

<p>虚拟消息服务器，vhost，本质上就是一个 mini 版的 mq 服务器，有自己的队列、交换器和绑定，最重要的，自己的权限机制。Vhost 提供了逻辑上的 分离，可以将众多客户端进行区分，又可以避免队列和交换器的命名冲突。Vhost 必须在连接时指定，rabbitmq 包含缺省 vhost:“/”，通过缺省用户和 口令 guest 进行访问。</p>

<p>rabbitmq 里创建用户，必须要被指派给至少一个 vhost，并且只能访问被指派内的队列、交换器和绑定。Vhost 必须通过 rabbitmq 的管理控制工具创建。</p>

<h2 id="toc_8">交换器类型</h2>

<p>共有四种 direct、fanout、topic、headers，其中headers和 direct 可以忽略。</p>

<h3 id="toc_9">fanout</h3>

<p>消息广播到绑定的队列，不管队列绑定了什么路由键，消息经过交换器，每个队列都有一份</p>

<h3 id="toc_10">Topic</h3>

<p><code>通过使用 “*”和“#”通配符进行处理，使来自不同源头的消息到达同一个队列，”.”将路由键分为了几个标识符，“*” 匹配 1 个，“#”匹配一个或多个。</code></p>

<p><figure><img src="media/16022252063226/16022297347390.jpg" alt="" style="width:799px;"/></figure></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[什么是消息中间件]]></title>
    <link href="http://www.throne4j.com/16021705401610.html"/>
    <updated>2020-10-08T23:22:20+08:00</updated>
    <id>http://www.throne4j.com/16021705401610.html</id>
    <content type="html"><![CDATA[
<h2 id="toc_0">消息中间件(MQ)的定义</h2>

<p>其实并没有标准定义。一般认为，消息中间件属于分布式系统中一个子系统，关注于数据的发送和接收，利用高效可靠的异步消息传递机制对分布式系统中的其余各个子系统进行集成。</p>

<ul>
<li>高效:对于消息的处理处理速度快。 </li>
<li>可靠:一般消息中间件都会有消息持久化机制和其他的机制确保消息不丢失。 </li>
<li>异步:指发送完一个请求，不需要等待返回，随时可以再发送下一个请求，既不需要等待。 </li>
</ul>

<p>一句话总结，我们消息中间件不生产消息，只是消息的搬运工。</p>

<h2 id="toc_1">为什么要用消息中间件?</h2>

<p>所以消息中间件主要解决分布式系统之间消息的传递，同时为分布式系统中其他子系统提供了松耦合的架构，同时还有以下好处。</p>

<ul>
<li><p>低耦合<br/>
低耦合，不管是程序还是模块之间，使用消息中间件进行间接通信。</p></li>
<li><p>异步通信能力<br/>
异步通信能力，使得子系统之间得以充分执行自己的逻辑而无需等待。</p></li>
<li><p>缓冲能力<br/>
缓冲能力，消息中间件像是一个巨大的蓄水池，将高峰期大量的请求存储下来慢慢交给后台进行处理，对于秒杀业务来说尤为重要。</p></li>
<li><p>伸缩性<br/>
伸缩性，是指通过不断向集群中加入服务器的手段来缓解不断上升的用户并发访问压力和不断增长的数据存储需求。就像弹簧一样挂东西一样，用 户多，伸一点，用户少，浅一点，啊，不对，缩一点。是伸缩，不是深浅。衡量架构是否高伸缩性的主要标准就是是否可用多台服务器构建集群，是否 容易向集群中添加新的服务器。加入新的服务器后是否可以提供和原来服务器无差别的服务。集群中可容纳的总的服务器数量是否有限制。</p></li>
<li><p>扩展性<br/>
扩展性，主要标准就是在网站增加新的业务产品时，是否可以实现对现有产品透明无影响，不需要任何改动或者很少改动既有业务功能就可以上线新产品。比如用户购买电影票的应用，现在我们要增加一个功能，用户买了铁血战士的票后，随机抽取用户送异形的限量周边。怎么做到不改动用户购票功能的基础上增加这个功能。熟悉设计模式的同学，应该很眼熟，这是设计模式中的开闭原则(对扩展开放，对修改关闭)在架构层面的一个原则。</p></li>
</ul>

<h2 id="toc_2">和 RPC 有何区别?</h2>

<p>RPC 和消息中间件的场景的差异很大程度上在于就是“依赖性”和“同步性”。</p>

<ul>
<li>依赖性:<br/>
比如短信通知服务并不是事交易环节必须的，并不影响下单流程，不是强依赖，所以交易系统不应该依赖短信服务。如果是 RPC 调用，短信通知服 务挂了，整个业务就挂了，这个就是依赖性导致的，而消息中间件则没有这个依赖性。</li>
</ul>

<p>消息中间件出现以后对于交易场景可能是调用库存中心等强依赖系统执行业务，之后发布一条消息(这条消息存储于消息中间件中)。像是短信通 知服务、数据统计服务等等都是依赖于消息中间件去消费这条消息来完成自己的业务逻辑。</p>

<ul>
<li>同步性:<br/>
RPC 方式是典型的同步方式，让远程调用像本地调用。消息中间件方式属于异步方式。</li>
</ul>

<h2 id="toc_3">消息中间件有些什么使用场景?</h2>

<h3 id="toc_4">异步处理</h3>

<p>场景说明: 用户注册后，需要发注册邮件和注册短信。</p>

<p>传统的做法有两种 </p>

<ul>
<li>串行的方式<br/>
将注册信息写入数据库成功后，发送注册邮件，再发送注册短信。以上三个任务全部完成后，返回给客户端。</li>
<li>并行方式<br/>
将注册信息写入数据库成功后，发送注册邮件的同时，发送注册短信。以上三个任务完成后，返回给客户端。与串行的差别是，并 行的方式可以提高处理的时间。
<figure><img src="media/16021705401610/16021728568093.jpg" alt="串行"/><figcaption>串行</figcaption></figure></li>
</ul>

<p><figure><img src="media/16021705401610/16021728464187.jpg" alt="并行"/><figcaption>并行</figcaption></figure></p>

<p>上案例描述，传统的方式系统的性能(并发量，吞吐量，响应时间)会有瓶颈。如何解决这个问题呢? </p>

<p>引入消息队列，将不是必须的业务逻辑，异步处理。</p>

<p><figure><img src="media/16021705401610/16021712978489.jpg" alt="" style="width:560px;"/></figure></p>

<p>按照以上约定，用户的响应时间相当于是注册信息写入数据库的时间，也就是 50 毫秒。注册邮件，发送短信写入消息队列后，直接返回，因此写入 消息队列的速度很快，基本可以忽略，因此用户的响应时间可能是 50 毫秒。因此架构改变后，系统的吞吐量提高到每秒 20 QPS。比串行提高了 3 倍，比 并行提高了两倍。</p>

<h3 id="toc_5">应用解耦</h3>

<p>场景说明:用户下单后，订单系统需要通知库存系统。传统的做法是，订单系统调用库存系统的接口。</p>

<p>传统模式的缺点:</p>

<ul>
<li>1) 假如库存系统无法访问，则订单减库存将失败，从而导致订单失败;</li>
<li>2) 订单系统与库存系统耦合;</li>
</ul>

<p>如何解决以上问题呢?引入应用消息队列后的方案</p>

<p>订单系统:用户下单后，订单系统完成持久化处理，将消息写入消息队列，返回用户订单下单成功。 </p>

<p>库存系统:订阅下单的消息，采用拉/推的方式，获取下单信息，库存系统根据下单信息，进行库存操作。</p>

<p>假如:在下单时库存系统不能正常使用。也不影响正常下单，因为下单后，订单系统写入消息队列就不再关心其他的后续操作了。实现订单系统与 库存系统的应用解耦。</p>

<h3 id="toc_6">流量削峰</h3>

<p>流量削峰也是消息队列中的常用场景，一般在秒杀或团抢活动中使用广泛。</p>

<p>应用场景:秒杀活动，一般会因为流量过大，导致流量暴增，应用挂掉。为解决这个问题，一般需要在应用前端加入消息队列:可以控制活动的人数;可以缓解短时间内高流量压垮应用。</p>

<h3 id="toc_7">日志处理</h3>

<p>日志处理是指将消息队列用在日志处理中，比如 Kafka 的应用，解决大量日志传输的问题。架构简化如下:<br/>
<figure><img src="media/16021705401610/16022199054947.jpg" alt="" style="width:623px;"/></figure></p>

<p>日志采集客户端，负责日志数据采集，定时写入 Kafka 队列:Kafka 消息队列，负责日志数据的接收，存储和转发;日志处理应用:订阅并消费 kafka 队列中的日志数据;</p>

<h3 id="toc_8">消息通讯</h3>

<p>消息通讯是指，消息队列一般都内置了高效的通信机制，因此也可以用在纯的消息通讯。比如实现点对点消息队列，或者聊天室等。 点对点通讯:客户端 A 和客户端 B 使用同一队列，进行消息通讯。<br/>
聊天室通讯:客户端 A，客户端 B，客户端 N 订阅同一主题，进行消息发布和接收。实现类似聊天室效果。</p>

<h2 id="toc_9">常见的消息中间件比较</h2>

<p><figure><img src="media/16021705401610/16022199788204.jpg" alt="" style="width:1025px;"/></figure></p>

<p>如果一般的业务系统要引入 MQ，怎么选型:</p>

<p>用户访问量在 ActiveMQ 的可承受范围内，而且确实主要是基于解耦和异步来用的，可以考虑 ActiveMQ，也比较贴近 Java 工程师的使用习惯，但是<br/>
ActiveMQ 现在停止维护了，同时 ActiveMQ 并发不高，所以业务量一定的情况下可以考虑使用。</p>

<p>RabbitMQ 作为一个纯正血统的消息中间件，有着高级消息协议 AMQP 的完美结合，在消息中间件中地位无可取代，但是 erlang 语言阻止了我们去深入研究和掌控，对公司而言，底层技术无法控制，但是确实是开源的，有比较稳定的支持，活跃度也高。</p>

<p>对自己公司技术实力有绝对自信的，可以用 RocketMQ，但是 RocketMQ 诞生比较晚，并且更新迭代很快，这个意味着在使用过程中有可能会遇到很多坑，所以如果你们公司 Java 技术不是很强，不推荐使用。</p>

<p>如果是大数据领域的实时计算、日志采集等场景，用 Kafka 是业内标准的，绝对没问题，社区活跃度很高，几乎是全世界这个领域的事实性规范。 </p>

<p>从性能上来看，使用文件系统的消息中间件(kafka、rokcetMq)性能是最好的，所以基于文件系统存储的消息中间件是发展趋势。(从存储方式和效率来看 文件系统&gt;KV存储&gt;关系型数据库)</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Collection 体系]]></title>
    <link href="http://www.throne4j.com/16020916538677.html"/>
    <updated>2020-10-08T01:27:33+08:00</updated>
    <id>http://www.throne4j.com/16020916538677.html</id>
    <content type="html"><![CDATA[
<p>Collection 集合体系是 List、 Set 和 Queue 的接口</p>

<p>List 特点： 存取有序，可以根据索引来直接取值<br/>
Set 特点： 存取无序，元素不可以重复<br/>
Queue特点： 存取有序，两端进出元素</p>

<h2 id="toc_0">List 系集合</h2>

<ul>
<li>ArrayList: 底层由数组实现，排列有序，元素可重复，线程不安全，按照 当前容量*1.5 + 1 进行扩容。</li>
<li>Vector ：</li>
<li>LinkedList</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[在 java 中守护线程和用户线程的区别?]]></title>
    <link href="http://www.throne4j.com/16020533928895.html"/>
    <updated>2020-10-07T14:49:52+08:00</updated>
    <id>http://www.throne4j.com/16020533928895.html</id>
    <content type="html"><![CDATA[
<p>java 中的线程分为两种:守护线程(Daemon)和用户线程(User)。</p>

<p>任何线程都可以设置为守护线程和用户线程，通过方法 Thread.setDaemon(bool on); true 则把该线程设置为守护线程，反之则为用户线程 。Thread.setDaemon()必须在 Thread.start()之前调用，否则运行时会抛出异常。<br/>
两者的区别:<br/>
唯一的区别是判断虚拟机(JVM)何时离开，Daemon 是为其他线程提供服务，<br/>
如果全部的 User Thread 已经结束，Daemon 没有可服务的线程，JVM 关闭。 扩展:Thread Dump 打印出来的线程信息，含有 daemon 字样的线程即为守<br/>
护进程</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[伪共享]]></title>
    <link href="http://www.throne4j.com/16020519457966.html"/>
    <updated>2020-10-07T14:25:45+08:00</updated>
    <id>http://www.throne4j.com/16020519457966.html</id>
    <content type="html"><![CDATA[
<h2 id="toc_0">缓存行</h2>

<p>对计算机组成原理相对熟悉的小伙伴都知道，CPU 的速度比内存的速度高了几个数量级，为了 CPU 更快从内存中读取数据，设置了多级缓存机制，如下图所示：<br/>
<figure><img src="media/16020519457966/16020586138762.jpg" alt=""/></figure></p>

<p>当 CPU 运算时，首先会从 L1 缓存查找所需要的数据，如果没有找到，再去 L2 缓存中去找，以此类推，直到从内存中获取数据，这也就意味着，越长的调用链，所耗费的执行时间也越长。</p>

<p>那是不是可以从主内存拿数据的时候，顺便多拿一些呢？这样就可以避免频繁从主内存中获取数据了。聪明的计算机科学家已经想到了这个法子，这就是缓存行的由来。</p>

<p>缓存是由多个缓存行组成的，而每个缓存行大小通常来说，大小为 64 字节，并且每个缓存行有效地引用主内存中的一块儿地址，CPU 每次从主内存中获取数据时，会将相邻的数据也一同拉取到缓存行中，这样当 CPU 执行运算时，就大大减少了与主内存的交互。</p>

<pre><code class="language-java">public class CacheLineDemo {

    //考虑一般缓存行大小是64字节，一个 long 类型占8字节
    static long[][] arr;

    public static void main(String[] args) {

        int size = 1024 * 1024;

        arr = new long[size][];
        for (int i = 0; i &lt; size; i++) {
            arr[i] = new long[8];
            for (int j = 0; j &lt; 8; j++) {
                arr[i][j] = 0L;
            }
        }
        long sum = 0L;
        long marked = System.currentTimeMillis();
        for (int i = 0; i &lt; size; i++) {
            for (int j = 0; j &lt; 8; j++) {
                sum = arr[i][j];
            }
        }
        System.out.println(&quot;[cache line]Loop times:&quot; + (System.currentTimeMillis() - marked) + &quot;ms&quot;);

        marked = System.currentTimeMillis();
        for (int i = 0; i &lt; 8; i += 1) {
            for (int j = 0; j &lt; size; j++) {
                sum = arr[j][i];
            }
        }
        System.out.println(&quot;[no cache line]Loop times:&quot; + (System.currentTimeMillis() - marked) + &quot;ms&quot;);
    }

}
</code></pre>

<h2 id="toc_1">伪共享问题</h2>

<p>当 CPU 执行完后，还需要将数据回写到内存上，以便于别的线程可以从主内存中获取最新的数据。假设两个线程都加载了相同的 Cache line 数据，会产生什么样的影响呢？下面我用一张图解释：</p>

<p><figure><img src="media/16020519457966/16020585354167.jpg" alt=""/></figure></p>

<p>数据 A、B、C 被加载到同一个 Cache line，假设线程 1 在 core1 中修改 A，线程 2 在 core2 中修改 B。</p>

<p>线程 1 首先对 A 进行修改，这时 core1 会告知其它 CPU 核，当前引用同一地址的 Cache line 已经无效，随后 core2 发起修改 B，会导致 core1 将数据回写到主内存中，core2 这时会重新从主内存中读取该 Cache line 数据。</p>

<p>可见，如果同一个 Cache line 的内容被多个线程读取，就会产生相互竞争，频繁回写主内存，降低了性能。</p>

<h2 id="toc_2">如何解决伪共享问题</h2>

<p>要解决伪共享这个问题最简单的做法就是将线程间共享元素分开到不同的 Cache line 中，这种做法叫用空间换取时间，具体做法如下：</p>

<pre><code class="language-java">public final static class ValuePadding {
  // 前置填充对象
  protected long p1, p2, p3, p4, p5, p6, p7;
  // value 值
  protected volatile long value = 0L;
  // 后置填充对象
  protected long p9, p10, p11, p12, p13, p14, p15;
}
</code></pre>

<p>JDK1.8 有专门的注解 @Contended 来避免伪共享，为了更加直观，我使用了对象填充的方法，其中 protected long p1, p2, p3, p4, p5, p6, p7 作为前置填充对象，protected long p9, p10, p11, p12, p13, p14, p15作为后置填充对象，这样任意线程访问 ValuePadding 时，value 都处于不同的 Cache line 中，不会产生伪共享问题。</p>

<p>下面的例子用来演示伪共享与解决伪共享后的性能差异：</p>

<pre><code class="language-java">public class FakeShareDemo {

    public static void main(String[] args) throws InterruptedException {
        for (int i = 1; i &lt; 10; i++) {
            System.gc();
            final long start = System.currentTimeMillis();
            runTest(Type.PADDING, i);
            System.out.println(&quot;[PADDING]Thread num &quot; + i + &quot; duration = &quot; + (System.currentTimeMillis() - start));
        }

        for (int i = 1; i &lt; 10; i++) {
            System.gc();
            final long start = System.currentTimeMillis();
            runTest(Type.NO_PADDING, i);
            System.out.println(&quot;[NO_PADDING] Thread num &quot; + i + &quot; duration = &quot; + (System.currentTimeMillis() - start));
        }
    }

    private static void runTest(Type type, int NUM_THREADS) throws InterruptedException {
        Thread[] threads = new Thread[NUM_THREADS];

        switch (type) {
            case PADDING:
                DataPadding.longs = new ValuePadding[NUM_THREADS];
                for (int i = 0; i &lt; DataPadding.longs.length; i++) {
                    DataPadding.longs[i] = new ValuePadding();
                }
                break;
            case NO_PADDING:
                Data.longs = new ValueNoPadding[NUM_THREADS];
                for (int i = 0; i &lt; Data.longs.length; i++) {
                    Data.longs[i] = new ValueNoPadding();
                }
                break;
        }


        for (int i = 0; i &lt; threads.length; i++) {
            threads[i] = new Thread(new FakeSharing(type, i));
        }
        for (Thread t : threads) {
            t.start();
        }
        for (Thread t : threads) {
            t.join();
        }
    }

    // 线程执行单元
    static class FakeSharing implements Runnable {
        public final static long ITERATIONS = 500L * 1000L * 100L;
        private int arrayIndex;
        private Type type;

        public FakeSharing(Type type, final int arrayIndex) {
            this.arrayIndex = arrayIndex;
            this.type = type;
        }

        public void run() {
            long i = ITERATIONS + 1;
            // 读取共享变量中指定的下标对象，并对其value变量不断修改
            // 由于每次读取数据都会写入缓存行，如果线程间有共享的缓存行数据，就会导致伪共享问题发生
            // 如果对象已填充，那么线程每次读取到缓存行中的对象就不会产生伪共享问题
            switch (type) {
                case NO_PADDING:
                    while (0 != --i) {
                        Data.longs[arrayIndex].value = 0L;
                    }
                    break;
                case PADDING:
                    while (0 != --i) {
                        DataPadding.longs[arrayIndex].value = 0L;
                    }
                    break;
            }
        }
    }

    // 线程间贡献的数据
    public final static class Data {
        public static ValueNoPadding[] longs;
    }

    public final static class DataPadding {
        public static ValuePadding[] longs;
    }

    // 使用填充对象
    public final static class ValuePadding {
        // 前置填充对象
        protected long p1, p2, p3, p4, p5, p6;
        // value 值
        protected volatile long value = 0L;
        // 后置填充对象
        protected long p9, p10, p11, p12, p13, p14, p15;
    }

    // 不填充对象
    //    @sun.misc.Contended
    public final static class ValueNoPadding {
        protected volatile long value = 0L;
    }

    enum Type {
        NO_PADDING,
        PADDING
    }
}
</code></pre>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Disruptor]]></title>
    <link href="http://www.throne4j.com/16020511412975.html"/>
    <updated>2020-10-07T14:12:21+08:00</updated>
    <id>http://www.throne4j.com/16020511412975.html</id>
    <content type="html"><![CDATA[
<p>Disruptor 是英国外汇交易公司 LMAX 开发的一个高性能队列，研发的初衷是 解决内部的内存队列的延迟问题，而不是分布式队列。基于 Disruptor 开发的系 统单线程能支撑每秒 600 万订单，2010 年在 QCon 演讲后，获得了业界关注。</p>

<p>Disruptor 是一个高性能的线程间异步通信的框架，即在同一个 JVM 进程中 的多线程间消息传递。</p>

<h2 id="toc_0">传统队列问题</h2>

<p>在 JDK 中，Java 内部的队列 BlockQueue 的各种实现，仔细分析可以得知， 队列的底层数据结构一般分成三种:数组、链表和堆，堆这里是为了实现带有优先级特性的队列暂且不考虑。</p>

<p>在稳定性和性能要求特别高的系统中，为了防止生产者速度过快，导致内存 溢出，只能选择有界队列;同时，为了减少 Java 的垃圾回收对系统性能的影响， 会尽量选择 Array 格式的数据结构。这样筛选下来，符合条件的队列就只有 ArrayBlockingQueue。但是 ArrayBlockingQueue 是通过加锁的方式保证线程安全， 而且 ArrayBlockingQueue 还存在伪共享问题，这两个问题严重影响了性能。</p>

<p>ArrayBlockingQueue 的这个伪共享问题存在于哪里呢，分析下核心的部分源 码，其中最核心的三个成员变量为：takeIndex、putIndex、count，在 ArrayBlockingQueue 的核心 enqueue 和 dequeue 方法中经常会用到的，这三 个变量很容易放到同一个缓存行中，进而产生伪共享问题。</p>

<h2 id="toc_1">高性能的原理</h2>

<p>引入环形的数组结构:数组元素不会被回收，避免频繁的 GC， </p>

<p>无锁的设计:采用 CAS 无锁方式，保证线程的安全性 </p>

<p>属性填充:通过添加额外的无用信息，避免伪共享问题</p>

<p>环形数组结构是整个 Disruptor 的核心所在。</p>

<p><figure><img src="media/16020511412975/16020523609287.jpg" alt="" style="width:738px;"/></figure></p>

<p>首先，因为是数组，所以要比链表快，而且根据我们对上面缓存行的解释知道， 数组中的一个元素加载，相邻的数组元素也是会被预加载的，因此在这样的结构中，cpu 无需时不时去主存加载数组中的下一个元素。而且，你可以为数组预先分配内存，使得数组对象一直存在(除非程序终止)。</p>

<p>这就意味着不需要花大量的时间用于垃圾回收。此外，不像链表那样，需要为每一个添加到其上面的对象创造节点对象，对应的，当删除节点时，需要执行相应的内存清理操作。环形数组中的元素采用覆盖方式，避免了 jvm 的 GC。</p>

<p>其次，结构作为环形，数组的大小为 2 的 n 次方，这样元素定位可以通过位运算效率会更高，这个跟一致性哈希中的环形策略有点像。在 disruptor 中，这个牛逼的环形结构就是 RingBuffer，既然是数组，那么就有大小，而且这个大小必须是2的n次方。</p>

<p>其实质只是一个普通的数组，只是当放置数据填充满队列(即到达 2<sup>n-1</sup> 位 置)之后，再填充数据，就会从 0 开始，覆盖之前的数据，于是就相当于一个环。</p>

<p>每个生产者首先通过 CAS 竞争获取可以写的空间，然后再进行慢慢往里放数据，如果正好这个时候消费者要消费数据，那么每个消费者都需要获取最大可消费的下标。</p>

<p>同时，Disruptor 不像传统的队列，分为一个队头指针和一个队尾指针，而是只有一个角标(上图的 seq)，它属于一个 volatile 变量，同时也是我们能够不用锁操作就能实现 Disruptor 的原因之一，而且通过缓存行补充，避免伪共享 问题。该指针是通过一直自增的方式来获取下一个可写或者可读数据。</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[jdk8 新增的功能特性]]></title>
    <link href="http://www.throne4j.com/16020424526444.html"/>
    <updated>2020-10-07T11:47:32+08:00</updated>
    <id>http://www.throne4j.com/16020424526444.html</id>
    <content type="html"><![CDATA[
<h2 id="toc_0">原子操作 CAS</h2>

<p>JDK1.8 时，java.util.concurrent.atomic 包中提供了一个新的原子类:LongAdder。</p>

<p>LongAdder 在高并发的场景下会比它的前辈 --- AtomicLong 具有更好的性能，代价是消耗更多的内存空间。</p>

<p>AtomicLong 是利用了底层的 CAS 操作来提供并发性的，调用了 Unsafe 类的 getAndAddLong 方法，该方法是个 native 方法，它的逻辑是采用自旋的方式不断 更新目标值，直到更新成功。</p>

<p>在并发量较低的环境下，线程冲突的概率比较小，自旋的次数不会很多。但 是，高并发环境下，N 个线程同时进行自旋操作，会出现大量失败并不断自旋的 情况，此时 AtomicLong 的自旋会成为瓶颈。</p>

<p>AtomicLong 中有个内部变量 value 保存着实际的 long 值，所有的操作都是 针对该变量进行。也就是说，高并发环境下，value 变量其实是一个热点，也就 是 N 个线程竞争一个热点。</p>

<p>LongAdder 的基本思路就是分散热点，将 value 值分散到一个数组中，不同 线程会命中到数组的不同槽中，各个线程只对自己槽中的那个值进行 CAS 操作，这样热点就被分散了，冲突的概率就小很多。如果要获取真正的 long 值，只有将各个槽中的变量值累加返回，这个值也仅仅是个近似值，这也是他不能完全代替LongAtomic 的原因之一。</p>

<p>除了引入 LongAdder外，还有引入了其它三个类：LongAccumulator、DoubleAdder、DoubleAccumulator。</p>

<p>通过 LongBinaryOperator，可以自定义对入参的任意操作，并返回结果 (LongBinaryOperator 接收 2 个 long 作为参数，并返回 1 个 long)。<br/>
LongAccumulator 内部原理和 LongAdder 几乎完全一样。 DoubleAdder 和 DoubleAccumulator 用于操作 double 原始类型。</p>

<h2 id="toc_1">StampLock</h2>

<p>StampedLock 是 Java8 引入的一种新的锁机制,简单的理解,可以认为它是读写 锁的一个改进版本。</p>

<p>读写锁虽然分离了读和写的功能,使得读与读之间可以完全并发,但是读和写之间依然是冲突的,读锁会完全阻塞写锁,它使用的依然是悲观的锁策略.如果有大量的读线程,他也有可能引起写线程的饥饿。</p>

<p>而 StampedLock 则提供了一种乐观的读策略,这种乐观策略的锁非常类似于 无锁的操作,使得乐观锁完全不会阻塞写线程。它的思想是读写锁中读不仅不阻塞读，同时也不应该阻塞写。</p>

<p>读不阻塞写的实现思路:<br/>
在读的时候如果发生了写，则应当重读而不是在读的时候直接阻塞写!即读 写之间不会阻塞对方，但是写和写之间还是阻塞的! StampedLock 的内部实现是基于 CLH 的。</p>

<h2 id="toc_2">CompleteableFuture</h2>

<p>JDK1.8 才新加入的一个实现类 CompletableFuture，实现了 Future<T>，CompletionStage<T>两个接口。详情见 ： <a href="15882235745444.html">CompletableFuture</a></p>

<h2 id="toc_3">lambada 表达式</h2>

<p>在语法上，Lambda 表达式包含三个部分，参数列表，箭头，主体，比如:</p>

<pre><code class="language-text">(parameters) -&gt; expression 

(parameters) -&gt; {statements;}
</code></pre>

<p>Lambda 表达式用在函数式接口上，所谓函数式接口，是只定义了一个抽象方法的接口(Interface)，接口中是否有默认方法，不影响。</p>

<p>注解@FunctionalInterface 可以帮助我们在设计函数式接口时防止出错。</p>

<p>我们常用的 Runnable,Callable 都是函数式接口，JDK8 中新增了几个函数式接 口:</p>

<ul>
<li><p>Predicate<T> :<br/>
包含 test 方法，接受泛型的 T，返回 boolean，可以视为断言(检查)接口 </p></li>
<li><p>Consumer<T> :<br/>
包含 accept 方法，接受泛型的 T，无返回，可以视为数据消费接口 </p></li>
<li><p>Function<T，R> :<br/>
包含 apply 方法，接受泛型的 T，返回 R，可以视为映射转换接口 Supplier<T><br/>
包含 get 方法，无输入，返回 T，可以视为创建一个新对象接口 </p></li>
<li><p>UnaryOperator<T></p></li>
</ul>

<p>扩展至 Function<T，T>，所以这个本质上也是一个映射转换接口，只不过映 射转换后的类型保持不变</p>

<ul>
<li><p>BiFunction<T, U, R><br/>
包含 apply 方法，接受泛型的 T、U，返回 R，可以视为复合型映射转换接口</p></li>
<li><p>BinaryOperator<T><br/>
扩展至Function BiFunction<T,T,T>，所以这个本质上也是一个复合型映射转 换接口，只不过映射转换后的类型保持不变</p></li>
<li><p>BiPredicate <T, U><br/>
包含 test 方法，接受泛型的 T，U，返回 boolean，可以视为复合型断言(检 查)接口</p></li>
<li><p>BiConsumer<T，U>:<br/>
包含 accept 方法，接受泛型的 T，U，无返回，可以视为复合型数据消费接<br/>
口</p></li>
</ul>

<h3 id="toc_4">函数描述符</h3>

<p>函数式接口的抽象方法的签名基本上就是 Lambda 表达式的签名。我们将这 种抽象方法叫作函数描述符</p>

<p>Runnable 接口可以看作一个什么也不接受什么也不返回(void)的函数的签 名，因为它只有一个叫作 run 的抽象方法，这个方法什么也不接受，什么也不返 回(void)。</p>

<p>我们可以用 () -&gt; void 代表参数列表为空，且返回 void 的函数。这正是 Runnable 接口所代表的。我们于是可以称() -&gt; void 是 Runnable 接口的函数描述符。</p>

<h2 id="toc_5">新增类库的新特性</h2>

<h3 id="toc_6">Optional</h3>

<p>Optional实际上是个容器：它可以保存类型T的值，或者仅仅保存null。Optional提供很多有用的方法，这样我们就不用显式进行空值检测。</p>

<pre><code class="language-java">Optional&lt; String &gt; fullName = Optional.ofNullable( null );
System.out.println( &quot;Full Name is set? &quot; + fullName.isPresent() );        
System.out.println( &quot;Full Name: &quot; + fullName.orElseGet( () -&gt; &quot;[none]&quot; ) ); 
System.out.println( fullName.map( s -&gt; &quot;Hey &quot; + s + &quot;!&quot; ).orElse( &quot;Hey Stranger!&quot; ) );
</code></pre>

<h3 id="toc_7">Stream</h3>

<p>最新添加的Stream API（java.util.stream） 把真正的函数式编程风格引入到Java中,Stream API极大简化了集合框架的处理</p>

<h3 id="toc_8">Date/Time API (JSR 310)</h3>

<p>当前北京时间：  2020-10-07 23:41:45</p>

<ul>
<li>第一个是Clock类<br/>
它通过指定一个时区，然后就可以获取到当前的时刻，日期与时间。Clock可以替换System.currentTimeMillis()与TimeZone.getDefault()。</li>
</ul>

<pre><code class="language-java">// Get the system clock as UTC offset 
final Clock clock = Clock.systemUTC();
System.out.println( clock.instant() );
System.out.println( clock.millis() );

2020-10-07T15:46:53.818Z
1602085613878
</code></pre>

<ul>
<li>LocaleDateTime、LocalDate、LocalTime</li>
</ul>

<p>LocaleDate只持有ISO-8601格式且无时区信息的日期部分。相应的，LocaleTime只持有ISO-8601格式且无时区信息的时间部分。LocaleDate与LocalTime都可以从Clock中得到</p>

<pre><code class="language-java">// Get the local date and local time
final LocalDate date = LocalDate.now();
final LocalDate dateFromClock = LocalDate.now( clock );
         
System.out.println( date );
System.out.println( dateFromClock );
         
// Get the local date and local time
final LocalTime time = LocalTime.now();
final LocalTime timeFromClock = LocalTime.now( clock );
         
System.out.println( time );
System.out.println( timeFromClock );

2020-10-07
2020-10-07
23:47:33.925
15:47:33.925
</code></pre>

<p>LocalDateTime</p>

<pre><code class="language-java">// Get the local date/time
final LocalDateTime datetime = LocalDateTime.now();
final LocalDateTime datetimeFromClock = LocalDateTime.now( clock );
         
System.out.println( datetime );
System.out.println( datetimeFromClock );

2020-10-07T23:50:34.974
2020-10-07T15:50:34.974
</code></pre>

<p>如果你需要特定时区的日期/时间，那么ZonedDateTime是你的选择。它持有ISO-8601格式具具有时区信息的日期与时间</p>

<pre><code class="language-java">// Get the zoned date/time
final ZonedDateTime zonedDatetime = ZonedDateTime.now();
final ZonedDateTime zonedDatetimeFromClock = ZonedDateTime.now( clock );
final ZonedDateTime zonedDatetimeFromZone = ZonedDateTime.now( ZoneId.of( &quot;Asia/Shanghai&quot; ) );
         
System.out.println( zonedDatetime );
System.out.println( zonedDatetimeFromClock );
System.out.println( zonedDatetimeFromZone );

2020-10-07T23:56:49.792+08:00[Asia/Shanghai]
2020-10-07T15:56:49.792Z
2020-10-07T23:56:49.792+08:00[Asia/Shanghai]
</code></pre>

<h2 id="toc_9">JavaScript 引擎 Nashorn</h2>

<p>Nashorn，一个新的JavaScript引擎随着Java 8一起公诸于世，它允许在JVM上开发运行某些JavaScript应用。Nashorn就是javax.script.ScriptEngine的另一种实现，并且它们俩遵循相同的规则，允许Java与JavaScript相互调用。下面看一个例子：</p>

<pre><code class="language-java">ScriptEngineManager manager = new ScriptEngineManager();
ScriptEngine engine = manager.getEngineByName( &quot;JavaScript&quot; );
         
System.out.println( engine.getClass().getName() );
System.out.println( &quot;Result:&quot; + engine.eval( &quot;function f() { return 1; }; f() + 1;&quot; ) );
</code></pre>

<h2 id="toc_10">Base64</h2>

<p>在Java 8中，Base64编码已经成为Java类库的标准。</p>

<pre><code class="language-java">import java.nio.charset.StandardCharsets;
import java.util.Base64;
 
public class Base64s {
    public static void main(String[] args) {
        final String text = &quot;Base64 finally in Java 8!&quot;;
         
        final String encoded = Base64
            .getEncoder()
            .encodeToString( text.getBytes( StandardCharsets.UTF_8 ) );
        System.out.println( encoded );
         
        final String decoded = new String( 
            Base64.getDecoder().decode( encoded ),
            StandardCharsets.UTF_8 );
        System.out.println( decoded );
    }
}
</code></pre>

<p>Base64类同时还提供了对URL、MIME友好的编码器与解码器（Base64.getUrlEncoder() / Base64.getUrlDecoder(), Base64.getMimeEncoder() / Base64.getMimeDecoder()）。</p>

<h2 id="toc_11">并行（parallel）数组</h2>

<p>Java 8增加了大量的新方法来对数组进行并行处理。可以说，最重要的是parallelSort()方法，因为它可以在多核机器上极大提高数组排序的速度。</p>

<pre><code class="language-java">import java.util.Arrays;
import java.util.concurrent.ThreadLocalRandom;
 
public class ParallelArrays {
    public static void main( String[] args ) {
        long[] arrayOfLong = new long [ 20000 ];        
         
        Arrays.parallelSetAll( arrayOfLong, 
            index -&gt; ThreadLocalRandom.current().nextInt( 1000000 ) );
        Arrays.stream( arrayOfLong ).limit( 10 ).forEach( 
            i -&gt; System.out.print( i + &quot; &quot; ) );
        System.out.println();
         
        Arrays.parallelSort( arrayOfLong );     
        Arrays.stream( arrayOfLong ).limit( 10 ).forEach( 
            i -&gt; System.out.print( i + &quot; &quot; ) );
        System.out.println();
    }
}
</code></pre>

<h2 id="toc_12">Java虚拟机（JVM）的新特性</h2>

<p>PermGen空间被移除了，取而代之的是Metaspace（JEP 122）。JVM选项-XX:PermSize与-XX:MaxPermSize分别被-XX:MetaSpaceSize与-XX:MaxMetaspaceSize所代替</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[java 内存模型]]></title>
    <link href="http://www.throne4j.com/16019058386155.html"/>
    <updated>2020-10-05T21:50:38+08:00</updated>
    <id>http://www.throne4j.com/16019058386155.html</id>
    <content type="html"><![CDATA[
<h2 id="toc_0">概念</h2>

<p>在并发编程中，我们需要处理两个关键问题：线程之间如何通信及线程之间如何同步（这里的线程是指并发执行的活动实体）。通信是指线程之间以何种机制来交换信息。</p>

<p>在命令式编程中，线程之间的通信机制有两种</p>

<ul>
<li><p>共享内存<br/>
在共享内存的并发模型里，线程之间共享程序的公共状态，线程之间通过写 - 读内存中的公共状态来隐式进行通信。</p></li>
<li><p>消息传递。<br/>
在消息传递的并发模型里，线程之间没有公共状态，线程之间必须通过明确的发送消息来显式进行通信。</p></li>
</ul>

<p>同步是指程序用于控制不同线程之间操作发生相对顺序的机制。在共享内存并发模型里，同步是显式进行的。程序员必须显式指定某个方法或某段代码需要在线程之间互斥执行。在消息传递的并发模型里，由于消息的发送必须在消息的接收之前，因此同步是隐式进行的。</p>

<p>Java 的并发采用的是共享内存模型，Java 线程之间的通信总是隐式进行，整个通信过程对程序员完全透明。如果编写多线程程序的 Java 程序员不理解隐式进行的线程之间通信的工作机制，很可能会遇到各种奇怪的内存可见性问题。</p>

<p>Java内存模型是围绕着在并发过程中如何处理原子性、可见性和有序性这三个特征来建立的</p>

<h2 id="toc_1">java 内存模型产生的原因</h2>

<p>由于计算机的存储设备与处理器的运算速度有着几个数量级的差距，所以现代计算机系统都不得不加入一层或多 层读写速度尽可能接近处理器运算速度的高速缓存(Cache)来作为内存与处理器之间的缓冲:将运算 需要使用的数据复制到缓存中，让运算能快速进行，当运算结束后再从缓存同步回内存之中，这样处 理器就无须等待缓慢的内存读写了。下图说明了处理器、高速缓存、主内存之间的交互关系：</p>

<p><figure><img src="media/16019058386155/16019095209346.jpg" alt=""/></figure></p>

<p>基于高速缓存的存储交互很好地解决了处理器与内存速度之间的矛盾，但是也为计算机系统带来 更高的复杂度，它引入了一个新的问题:<strong><em>缓存一致性(Cache Coherence)</em></strong>。</p>

<p><strong>在多路处理器系统中，每个处理器都有自己的高速缓存，而它们又共享同一主内存(Main Memory)，这种系统称为共享内存多核系统</strong></p>

<p>Java虚拟机规范中曾试图定义一种 ”java 内存模型“来 屏蔽各种硬件和操作系统的内存访问差异，以实现让Java程序在各种平台下都能达到一致的内存访问效果。</p>

<p>Java内存模型的主要目的是定义程序中各种变量的访问规则，即关注在虚拟机中把变量值存储到 内存和从内存中取出变量值这样的底层细节。</p>

<p>Java内存模型规定了所有的变量都存储在主内存(Main Memory)中，每条线程还有自己的工作内存，线程的工作内存中保存了被该线程使用的变量的主内存副本，线程对变量的所有操作(读取、赋值等)都必须在工作内存中进行，而不能直接读写主内存中的数据。</p>

<p>线程、主内存、工作内存的交互关系如下图所示：<br/>
<figure><img src="media/16019058386155/16019096316120.jpg" alt=""/></figure></p>

<h2 id="toc_2">java内存模型带来的问题</h2>

<h3 id="toc_3">可见性问题</h3>

<p><figure><img src="media/16019058386155/16019103811364.jpg" alt=""/></figure></p>

<p>左边 CPU 中运行的线程从主存中拷贝共享对象 obj 到它的 CPU 缓存，把对 象 obj 的 count 变量改为 2。但这个变更对运行在右边 CPU 中的线程不可见，因 为这个更改还没有 flush 到主存中。</p>

<p>要解决共享对象可见性这个问题，我们可以使用 volatile 关键字或者是加锁</p>

<h3 id="toc_4">竞争问题</h3>

<p><figure><img src="media/16019058386155/16019105072940.jpg" alt=""/></figure></p>

<p>图中两个加 1 操作是并行的，不管 是线程 A 还是线程 B 先 flush 计算结果到主存，最终主存中的 Obj.count 只会增 加 1 次变成 2，尽管一共有两次加 1 操作。 </p>

<p>要解决上面的问题我们可以使用 java synchronized 代码块</p>

<h2 id="toc_5">重排序</h2>

<p>在执行程序时为了提高性能，编译器和处理器常常会对指令做重排序。重排序分三种类型：</p>

<ul>
<li>编译器优化的重排序。编译器在不改变单线程程序语义的前提下，可以重新安排语句的执行顺序。</li>
<li>指令级并行的重排序。现代处理器采用了指令级并行技术（Instruction-Level Parallelism， ILP）来将多条指令重叠执行。如果不存在数据依赖性，处理器可以改变语句对应机器指令的执行顺序。</li>
<li>内存系统的重排序。由于处理器使用缓存和读 / 写缓冲区，这使得加载和存储操作看上去可能是在乱序执行。</li>
</ul>

<p>从 java 源代码到最终实际执行的指令序列，会分别经历下面三种重排序：<br/>
<figure><img src="media/16019058386155/16019073878819.jpg" alt=""/></figure></p>

<p>JMM 属于语言级的内存模型，它确保在不同的编译器和不同的处理器平台之上，通过禁止特定类型的编译器重排序和处理器重排序，为程序员提供一致的内存可见性保证。</p>

<h2 id="toc_6">数据依赖性</h2>

<p>数据依赖性:如果两个操作访问同一个变量，且这两个操作中有一个为写操 作，此时这两个操作之间就存在数据依赖性。数据依赖分为下列 3 种类型，上面 3 种情况，只要重排序两个操作的执行顺序，程序的执行结果就会被改变。<br/>
<figure><img src="media/16019058386155/16019107599119.jpg" alt=""/></figure></p>

<p>依赖关系：<br/>
<figure><img src="media/16019058386155/16019108342482.jpg" alt=""/></figure></p>

<p>很明显，A 和 C 存在数据依赖，B 和 C 也存在数据依赖，而 A 和 B 之间不存 在数据依赖，如果重排序了 A 和 C 或者 B 和 C 的执行顺序，程序的执行结果就 会被改变。</p>

<p>不管如何重排序，都必须保证代码在单线程下的运行正确，连单线 程下都无法正确，更不用讨论多线程并发的情况，所以就提出了一个 as-if-serial 的概念。</p>

<h2 id="toc_7">as-if-serial</h2>

<p>as-if-serial 语义的意思指：不管怎么重排序（编译器和处理器为了提高并行度），（单线程）程序的执行结果不能被改变。编译器、runtime 和处理器都必须遵守 as-if-serial 语义。</p>

<p>为了遵守as-if-serial语义，编译器和处理器不会对存在数据依赖关系的操作做重排序，因为这种重排序会改变执行结果。但是，如果操作之间不存在数据依赖关系，这些操作可能被编译器和处理器重排序。</p>

<h2 id="toc_8">happens-before</h2>

<p>在 Java 规范提案中为让大家理解内存可见性的这个概念，JSR-133 提出了 happens-before 的概念，通过这个概念来阐述操作之间的内存可见性。如果一个操作执行的结果需要对另一个操作可见，那么这两个操作之间必须存在 happens-before 关系。这里提到的两个操作既可以是在一个线程之内，也可以是在不同线程之间。 </p>

<p>happens-before 关系保证正确同步的多线程程序的执行结果不被改变。与程序员密切相关的 happens-before 规则如下：</p>

<ul>
<li>程序顺序规则：一个线程中的每个操作，happens-before 于该线程中的任意后续操作。</li>
<li>监视器锁规则：对一个监视器锁的解锁，happens-before 于随后对这个监视器锁的加锁。</li>
<li>volatile 变量规则：对一个 volatile 域的写，happens-before 于任意后续对这个 volatile 域的读。</li>
<li>传递性：如果 A happens- before B，且 B happens- before C，那么 A happens- before C。</li>
<li>start() 规则：如果线程A执行操作 ThreadB.start()（启动线程B)，那么 A 线程 的ThreadB.start() 操作 happens-before 与线程 B 中的任意操作</li>
<li>join()规则：如果线程 A 执行曹邹 ThreadB.join()并成功返回，那么线程 B 中的任意操作 happens-before 与线程A从 ThreadB.join() 操作成功返回。</li>
<li>线程中断规则： 对线程 interrupt 方法的调用 happens-before 与被中断线程的代码检测到中断事件的发生。</li>
</ul>

<p>注意： 两个操作之间具有 happens-before 关系，并不意味着前一个操作必须要在后一个操作之前执行！happens-before 仅仅要求前一个操作（执行的结果）对后一个操作可见，且前一个操作按顺序排在第二个操作之前。</p>

<h2 id="toc_9">内存屏障</h2>

<p>Java 编译器在生成指令序列的适当位置会插入内存屏障指令来禁止特定类型的处理器重排序，从而让程序按我们预想的流程去执行。</p>

<ul>
<li>保证特定操作的执行顺序。</li>
<li>影响某些数据(或则是某条指令的执行结果)的内存可见性</li>
</ul>

<p>编译器和 CPU 能够重排序指令，保证最终相同的结果，尝试优化性能。插入一条 Memory Barrier 会告诉编译器和 CPU:不管什么指令都不能和这条 Memory Barrier 指令重排序。</p>

<p>Memory Barrier 所做的另外一件事是强制刷出各种 CPU cache，如一个 Write-Barrier(写入屏障)将刷出所有在 Barrier 之前写入 cache 的数据，因此， 任何 CPU 上的线程都能读取到这些数据的最新版本。</p>

<p>JMM 把内存屏障指令分为 4 类</p>

<ul>
<li>LoadLoad barrier<br/>
Load1: LoadLoad: Load2  确保load1 数据的装载，之前与Load2及所有后续装载指令的装载 </li>
<li>StoreStore barrier<br/>
Store1: StoreStore : Store2 确保 Sttore1 数据对其他处理器可见（刷新到内存)之前与Store2以及所有后续存储指令的存储</li>
<li>LoadStore barrier<br/>
Load1：LoadStore：Store2 确保 Load1数据装载之前与Store2以及所有后续的存储指令刷新到内存 </li>
<li>StoreLoad barrier<br/>
Store1: StoreLoad: Load2  确保Store1数据对其他处理器变得可见（指刷新到内存）之前与 Load2 以及所有后续装载指令的装载，StoreLoad barrier 会使该屏障之前的所有内存访问指令（存储和装载指令）完成之后，才执行该屏障之后的内存访问指令</li>
</ul>

<p>StoreLoad Barrier 是一个“全能型”的屏障，它同时具有其他 3 个屏障的 效果。现代的多处理器大多支持该屏障(其他类型的屏障不一定被所有处理器支持)。</p>

<h2 id="toc_10">volatile 详解</h2>

<h3 id="toc_11">volatile特性</h3>

<p>volatile 变量自身具有下列特性：</p>

<ul>
<li>可见性： 对于一个volatile 变量的读 总是能看到对这个volatile变量最后的写入</li>
<li>原子性： 对任意单个 volatile 变量的 读/写 具有原子性，但类似于 volatile++ 这种复合操作不具有原子性</li>
</ul>

<h3 id="toc_12">volatile 的内存语义</h3>

<p>内存语义： 可以简单理解为 volatile， synchronize，atomic，lock 之类的在 JVM 中的内存方面实现原则。</p>

<p>volatile 写的内存语义： 当写一个 volatile 变量时，JMM 会把该线程对应的本地内存中的共享变量值刷新到主内存。</p>

<p>volatile读的内存语义： 当读一个 volatile 变量时，JMM 会把该线程对应的本地内存中的共享变量值刷新到主内存。</p>

<pre><code class="language-java">public class VolatileDemo {
    private volatile static boolean ready;
    private static int number = 1;

    private static class PrintThread extends Thread {
        public PrintThread(String name) {
            super(name);
        }
        @Override
        public void run() {
            while (!ready) {
                System.out.println(&quot;number: &quot; + number++);
            }
        }
    }

    public static void main(String[] args) throws InterruptedException {
        new PrintThread(&quot;volatile apply&quot;).start();
        Thread.sleep(2000);
        ready = true;
    }
}
</code></pre>

<h3 id="toc_13">volatile 的内存屏障</h3>

<p>在java中 对volatile 修饰的变量，编译器在生成字节码的时候，会在指令序列中插入内存屏障来禁止特定类型的处理器重排序问题。</p>

<p><figure><img src="media/16019058386155/16020040859402.jpg" alt=""/></figure></p>

<p><figure><img src="media/16019058386155/16020041276962.jpg" alt=""/></figure></p>

<h3 id="toc_14">volatile 的实现原理</h3>

<p>通过对 OpenJDK 中的 unsafe.cpp 源码的分析，会发现被 volatile 关键字修饰的变量会存在一个“lock:”的前缀。</p>

<p>Lock 前缀，Lock 不是一种内存屏障，但是它能完成类似内存屏障的功能。Lock<br/>
会对 CPU 总线和高速缓存加锁，可以理解为 CPU 指令级的一种锁。</p>

<p>同时该指令会将当前处理器缓存行的数据直接写会到系统内存中，且这个写 回内存的操作会使在其他 CPU 里缓存了该地址的数据无效。</p>

<p>在具体的执行上，它先对总线和缓存加锁，然后执行后面的指令，最后释放 锁后会把高速缓存中的脏数据全部刷新回主内存。在 Lock 锁住总线的时候， 其他 CPU 的读写请求都会被阻塞，直到锁释放。</p>

<h2 id="toc_15">final 关键字</h2>

<p>final 引用不能从构造函数内逃逸</p>

<h3 id="toc_16">final 的两个重排序规则</h3>

<p>对应 final 域，编译器和处理器需要遵守两个重排序规则。</p>

<ul>
<li>写 final 域的重排序规则可以确保在对象引用为任意线程可见之前， 对象的 final 域已经被正常的初始化了，而普通域不具有这样的保证。</li>
<li>读 final 域的重排序规则可以确保在读一个对象的 final 域之前，一定会先读包含这个 final 域的对象的引用。</li>
</ul>

<h3 id="toc_17">final 语义的实现</h3>

<p>会要求编译器在final域的写之后，构造函数return之前插入一个StoreStore<br/>
障屏。<br/>
读 final 域的重排序规则要求编译器在读 final 域的操作前面插入一个<br/>
LoadLoad 屏障</p>

<h2 id="toc_18">synchronized 的实现原理</h2>

<p><a href="16005248987908.html">Synchronized关键字解析</a></p>

]]></content>
  </entry>
  
</feed>
