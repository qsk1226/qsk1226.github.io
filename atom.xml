<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[大爷来玩儿啊~]]></title>
  <link href="http://www.throne4j.com/atom.xml" rel="self"/>
  <link href="http://www.throne4j.com/"/>
  <updated>2021-12-24T23:36:32+08:00</updated>
  <id>http://www.throne4j.com/</id>
  <author>
    <name><![CDATA[]]></name>
    
  </author>
  <generator uri="http://www.mweb.im/">MWeb</generator>
  
  <entry>
    <title type="html"><![CDATA[ElasticSearch 文本分析]]></title>
    <link href="http://www.throne4j.com/16399269127746.html"/>
    <updated>2021-12-19T23:15:12+08:00</updated>
    <id>http://www.throne4j.com/16399269127746.html</id>
    <content type="html"><![CDATA[
<ul>
<li><a href="#1%E3%80%81%E6%96%87%E6%9C%AC%E5%88%86%E6%9E%90">1、文本分析</a>
<ul>
<li><a href="#1-1%E3%80%81%E5%AD%97%E7%AC%A6%E8%BF%87%E6%BB%A4">1.1、字符过滤</a></li>
<li><a href="#1-2%E3%80%81%E5%88%87%E5%88%86%E4%B8%BA%E5%88%86%E8%AF%8D">1.2、切分为分词</a></li>
<li><a href="#1-3%E3%80%81%E5%88%86%E8%AF%8D%E8%BF%87%E6%BB%A4%E5%99%A8">1.3、分词过滤器</a></li>
<li><a href="#1-4%E3%80%81%E5%88%86%E8%AF%8D%E7%B4%A2%E5%BC%95">1.4、分词索引</a></li>
<li><a href="#1-5%E3%80%81%E5%88%86%E6%9E%90%E5%99%A8">1.5、分析器</a></li>
</ul>
</li>
<li><a href="#2%E3%80%81%E9%85%8D%E7%BD%AE%E5%88%86%E6%9E%90%E5%99%A8">2、配置分析器</a>
<ul>
<li><a href="#2-1%E3%80%81%E5%88%9B%E5%BB%BA%E7%B4%A2%E5%BC%95%E6%97%B6">2.1、创建索引时</a>
<ul>
<li><a href="#2-1-1%E3%80%81%E8%AE%BE%E7%BD%AE%E7%B4%A2%E5%BC%95%E9%BB%98%E8%AE%A4%E5%88%86%E8%AF%8D%E5%99%A8">2.1.1、设置索引默认分词器</a></li>
<li><a href="#2-1-2%E3%80%81%E4%B8%BA%E5%AD%97%E6%AE%B5%E6%8C%87%E5%AE%9A%E5%86%85%E7%BD%AE%E5%88%86%E8%AF%8D%E5%99%A8">2.1.2、为字段指定内置分词器</a></li>
</ul>
</li>
<li><a href="#2-2%E3%80%81%E6%96%87%E6%A1%A3%E6%90%9C%E7%B4%A2%E6%97%B6">2.2、文档搜索时</a>
<ul>
<li><a href="#2-2-1%E3%80%81%E6%90%9C%E7%B4%A2%E6%97%B6%E6%8C%87%E5%AE%9A-analyzer%E6%9F%A5%E8%AF%A2%E5%8F%82%E6%95%B0">2.2.1、搜索时指定 analyzer 查询参数</a></li>
<li><a href="#2-2-2%E3%80%81%E5%88%9B%E5%BB%BA%E7%B4%A2%E5%BC%95%E6%97%B6%E6%8C%87%E5%AE%9A%E5%AD%97%E6%AE%B5%E7%9A%84-analyzer%E5%92%8C-seach-analyzer">2.2.2、创建索引时指定字段的 analyzer 和 seach_analyzer</a></li>
<li><a href="#2-2-3%E3%80%81%E5%88%9B%E5%BB%BA%E7%B4%A2%E5%BC%95%E6%97%B6%E6%8C%87%E5%AE%9A%E7%B4%A2%E5%BC%95%E7%9A%84%E9%BB%98%E8%AE%A4%E6%90%9C%E7%B4%A2%E5%88%86%E8%AF%8D%E5%99%A8">2.2.3、创建索引时指定索引的默认搜索分词器</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#3%E3%80%81%E5%86%85%E7%BD%AE%E5%88%86%E6%9E%90%E5%99%A8">3、内置分析器</a>
<ul>
<li><a href="#3-1%E3%80%81%E5%AD%97%E7%AC%A6%E8%BF%87%E6%BB%A4%E5%99%A8-character-filters">3.1、字符过滤器(Character filters)</a>
<ul>
<li><a href="#3-1-1%E3%80%81html%E5%AD%97%E7%AC%A6%E8%BF%87%E6%BB%A4%E5%99%A8-html-strip-char-filter">3.1.1、HTML 字符过滤器(HTML Strip Char Filter)</a></li>
<li><a href="#3-1-2%E3%80%81%E6%98%A0%E5%B0%84%E5%AD%97%E7%AC%A6%E8%BF%87%E6%BB%A4%E5%99%A8-mapping-char-filter">3.1.2、映射字符过滤器(Mapping Char Filter)</a></li>
<li><a href="#3-1-3%E3%80%81%E6%A8%A1%E5%BC%8F%E6%9B%BF%E6%8D%A2%E8%BF%87%E6%BB%A4%E5%99%A8-pattern-replace-char-filter">3.1.3、模式替换过滤器(Pattern Replace Char Filter)</a></li>
</ul>
</li>
<li><a href="#3-2%E3%80%81%E5%88%86%E8%AF%8D%E5%99%A8%EF%BC%88tokenizer%EF%BC%89">3.2、分词器（Tokenizer）</a>
<ul>
<li><a href="#3-2-1%E3%80%81%E6%A0%87%E5%87%86%E5%88%86%E8%AF%8D%E5%99%A8-standard">3.2.1、标准分词器(standard)</a></li>
<li><a href="#3-2-2%E3%80%81%E5%85%B3%E9%94%AE%E8%AF%8D%E5%88%86%E8%AF%8D%E5%99%A8-keyword">3.2.2、关键词分词器(keyword)</a></li>
<li><a href="#3-2-3%E3%80%81%E5%AD%97%E6%AF%8D%E5%88%86%E8%AF%8D%E5%99%A8-letter">3.2.3、字母分词器(letter)</a></li>
<li><a href="#3-2-4%E3%80%81%E5%B0%8F%E5%86%99%E5%88%86%E8%AF%8D%E5%99%A8-lowercase">3.2.4、小写分词器(lowercase)</a></li>
<li><a href="#3-2-5%E3%80%81%E7%A9%BA%E7%99%BD%E5%88%86%E8%AF%8D%E5%99%A8-whitespace">3.2.5、空白分词器(whitespace)</a></li>
<li><a href="#3-2-6%E3%80%81%E6%A8%A1%E5%BC%8F%E5%88%86%E8%AF%8D%E5%99%A8-pattern">3.2.6、模式分词器(pattern)</a></li>
<li><a href="#3-2-7%E3%80%81uax-url%E7%94%B5%E5%AD%90%E9%82%AE%E4%BB%B6%E5%88%86%E8%AF%8D%E5%99%A8-uax-url-email">3.2.7、UAX URL 电子邮件分词器(uax_url_email)</a></li>
<li><a href="#3-2-8%E3%80%81%E8%B7%AF%E5%BE%84%E5%B1%82%E6%AC%A1%E5%88%86%E8%AF%8D%E5%99%A8-path-hierarchy">3.2.8、路径层次分词器(path_hierarchy)</a></li>
</ul>
</li>
<li><a href="#3-3%E3%80%81%E5%88%86%E8%AF%8D%E8%BF%87%E6%BB%A4%E5%99%A8-token-filters">3.3、分词过滤器(Token filters)</a>
<ul>
<li><a href="#3-3-1%E3%80%81%E6%A0%87%E5%87%86%E5%88%86%E8%AF%8D%E8%BF%87%E6%BB%A4%E5%99%A8-standard">3.3.1、标准分词过滤器(standard)</a></li>
<li><a href="#3-3-2%E3%80%81%E5%B0%8F%E5%86%99%E5%88%86%E8%AF%8D%E8%BF%87%E6%BB%A4%E5%99%A8-lowercase">3.3.2、小写分词过滤器(lowercase)</a></li>
<li><a href="#3-3-3%E3%80%81%E9%95%BF%E5%BA%A6%E5%88%86%E8%AF%8D%E8%BF%87%E6%BB%A4%E5%99%A8-length">3.3.3、长度分词过滤器(length)</a></li>
<li><a href="#3-3-4%E3%80%81%E5%81%9C%E7%94%A8%E8%AF%8D%E5%88%86%E8%AF%8D%E8%BF%87%E6%BB%A4%E5%99%A8-stop">3.3.4、停用词分词过滤器(stop)</a></li>
<li><a href="#3-3-5%E3%80%81%E6%88%AA%E6%96%AD%E5%88%86%E8%AF%8D%E8%BF%87%E6%BB%A4%E5%99%A8%E3%80%81%E4%BF%AE%E5%89%AA%E5%88%86%E8%AF%8D%E8%BF%87%E6%BB%A4%E5%99%A8%E5%92%8C%E9%99%90%E5%88%B6%E5%88%86%E8%AF%8D%E6%95%B0%E9%87%8F%E8%BF%87%E6%BB%A4%E5%99%A8">3.3.5、截断分词过滤器、修剪分词过滤器和限制分词数量过滤器</a></li>
</ul>
</li>
<li><a href="#3-4%E3%80%81%E5%B8%B8%E7%94%A8%E5%86%85%E7%BD%AE%E5%88%86%E6%9E%90%E5%99%A8">3.4、常用内置分析器</a>
<ul>
<li><a href="#3-4-1%E3%80%81%E6%A0%87%E5%87%86%E5%88%86%E6%9E%90%E5%99%A8">3.4.1、标准分析器</a></li>
<li><a href="#3-4-2%E3%80%81%E7%AE%80%E5%8D%95%E5%88%86%E6%9E%90%E5%99%A8">3.4.2、简单分析器</a></li>
<li><a href="#3-4-3%E3%80%81%E7%A9%BA%E7%99%BD%E5%88%86%E6%9E%90%E5%99%A8">3.4.3、空白分析器</a></li>
<li><a href="#3-4-4%E3%80%81%E5%81%9C%E7%94%A8%E8%AF%8D%E5%88%86%E6%9E%90%E5%99%A8">3.4.4、停用词分析器</a></li>
<li><a href="#3-4-5%E3%80%81%E5%85%B3%E9%94%AE%E8%AF%8D%E5%88%86%E6%9E%90%E5%99%A8">3.4.5、关键词分析器</a></li>
<li><a href="#3-4-6%E3%80%81%E6%A8%A1%E5%BC%8F%E5%88%86%E6%9E%90%E5%99%A8">3.4.6、模式分析器</a></li>
<li><a href="#3-4-7%E3%80%81%E9%9B%AA%E7%90%83%E5%88%86%E6%9E%90%E5%99%A8">3.4.7、雪球分析器</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#4%E3%80%81%E8%87%AA%E5%AE%9A%E4%B9%89%E5%88%86%E6%9E%90%E5%99%A8">4、自定义分析器</a></li>
<li><a href="#5%E3%80%81%E4%B8%AD%E6%96%87%E5%88%86%E6%9E%90%E5%99%A8">5、中文分析器</a></li>
</ul>

<p>词条(term)查询和全文(fulltext)查询最大的不同之处是:全文查询首先 分析(Analyze)查询字符串，使用默认的分析器分解成一系列的分词，term1， term2，termN，然后从索引中搜索是否有文档包含这些分词中的一个或多个。</p>
<p>所以，在基于全文的检索里，ElasticSearch 引擎会先分析(analyze)查询字 符串，将其拆分成小写的分词，只要已分析的字段中包含词条的任意一个，或全 部包含，就匹配查询条件，返回该文档;如果不包含任意一个分词，表示没有任何文档匹配查询条件。</p>
<p>这里就牵涉到了 ES 里很重要的概念，文本分析，当然对应非 text 类型字段来说，本身不存在文本数据词项提取的问题，所以没有文本分析的问题。</p>
<hr />
<h2><a id="1%E3%80%81%E6%96%87%E6%9C%AC%E5%88%86%E6%9E%90" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>1、文本分析</h2>
<p>分析( analysis )是在文档被发送并加入倒排索引之前，Elasticsearch 在其主体 上进行的操作。在文档被加入索引之前，Elasticsearch 让每个被分析字段经过一 系列的处理步骤。</p>
<ul>
<li>字符过滤: 使用字符过滤器转变字符。</li>
<li>文本切分为分词: 将文本切分为单个或多个分词。</li>
<li>分词过滤: 使用分词过滤器转变每个分词。</li>
<li>分词索引: 将这些分词存储到索引中。</li>
</ul>
<p>例如下面一段话：&quot;I want to play games,it include WOW&amp;Diablo&amp;Starcraft&quot;，经过分析后的分词为： i want to play games it include WOW Diablo Starcraft</p>
<h3><a id="1-1%E3%80%81%E5%AD%97%E7%AC%A6%E8%BF%87%E6%BB%A4" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>1.1、字符过滤</h3>
<p>Elasticsearch 首先运行字符过滤器(char_filter)，这些过滤器将特定的字符 序列转变为其他的字符序列。这个可以用于将 HTML 从文本中剥离，或者是将任 意数量的字符转化为其他字符，例如可将上面的 &quot;I want to.....&quot; 中的 &amp; 转换为 AND</p>
<h3><a id="1-2%E3%80%81%E5%88%87%E5%88%86%E4%B8%BA%E5%88%86%E8%AF%8D" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>1.2、切分为分词</h3>
<p>在应用了字符过滤器之后，文本需要被分割为可以操作的片段。底层的 Lucene 是不会对大块的字符串数据进行操作。相反，它处理的是被称为分词 (token) 的数据。</p>
<p>分词是从文本片段生成的，可能会产生任意数量(甚至是 0)的分词。例如， 在英文中一个通用的分词是标准分词器，它根据空格、换行和破折号等其他字符, 将文本分割为分词。</p>
<h3><a id="1-3%E3%80%81%E5%88%86%E8%AF%8D%E8%BF%87%E6%BB%A4%E5%99%A8" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>1.3、分词过滤器</h3>
<p>一旦文本块被转换为分词，ES 将会对每个分词运用分词过滤器<br />
(token_filter)，这些分词过滤器可以将一个分词作为输入， 然后根据需要进行 修改，添加或者是删除。最为有用的和常用的分词过滤器是小写分词过滤器,它 将输人的分词变为小写，确保在搜索词条 &quot;mysql&quot; 的时候，可以发现关于 &quot;MySql&quot; 的数据。</p>
<p>分词可以经过多于 1 个的分词过滤器，每个过滤器对分词进行不同的操 作，将数据塑造为最佳的形式，便于之后的索引。</p>
<h3><a id="1-4%E3%80%81%E5%88%86%E8%AF%8D%E7%B4%A2%E5%BC%95" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>1.4、分词索引</h3>
<p>当分词经历了零个或者多个分词过滤器，它们将被发送到 Lucene 进行文档 的索引。这些分词存入倒排索引。</p>
<h3><a id="1-5%E3%80%81%E5%88%86%E6%9E%90%E5%99%A8" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>1.5、分析器</h3>
<p>所有这些不同的部分，组成了一个分析器( analyzer ), 它可以定义为零个或多 个字符过滤器、1 个分词器、零个或多个分词过滤器。</p>
<p>Elasticsearch 中提供了很多预定义的分析器。我们可以直接使用它们而无须构建自己的分析器。</p>
<hr />
<h2><a id="2%E3%80%81%E9%85%8D%E7%BD%AE%E5%88%86%E6%9E%90%E5%99%A8" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>2、配置分析器</h2>
<p>文本分词会发生在两个地方:</p>
<ul>
<li>创建索引<br />
当索引文档字符类型为 text 时，在建立索引时将会对该字段进行分词;</li>
<li>搜索<br />
当对一个 text 类型的字段进行全文检索时，会对用户输入的文本进行分词。</li>
</ul>
<p>这两个地方都可以对分词进行配置。</p>
<h3><a id="2-1%E3%80%81%E5%88%9B%E5%BB%BA%E7%B4%A2%E5%BC%95%E6%97%B6" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>2.1、创建索引时</h3>
<p>ES 将按照下面顺序来确定使用哪个分词器:</p>
<ul>
<li>先判断字段是否有设置分词器，如果有，则使用字段属性上的分词器设置;</li>
<li>如果设置了 analysis.analyzer.default，则使用该设置的分词器;</li>
<li>如果上面两个都未设置，则使用默认的 standard 分词器</li>
</ul>
<h4><a id="2-1-1%E3%80%81%E8%AE%BE%E7%BD%AE%E7%B4%A2%E5%BC%95%E9%BB%98%E8%AE%A4%E5%88%86%E8%AF%8D%E5%99%A8" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>2.1.1、设置索引默认分词器</h4>
<pre><code class="language-bash">PUT /my_index
{
  &quot;settings&quot;: {
    &quot;analysis&quot;: {
      &quot;analyzer&quot;: {
        &quot;default&quot;: {
          &quot;type&quot;: &quot;simple&quot;
        }
      }
    }
  }
}
</code></pre>
<p>response :</p>
<pre><code class="language-json">{
  &quot;acknowledged&quot; : true,
  &quot;shards_acknowledged&quot; : true,
  &quot;index&quot; : &quot;test&quot;
}
</code></pre>
<p>还可以为索引配置内置分词器，并修改内置的部分选项修改它的行为</p>
<pre><code class="language-bash">DELETE /my_index
PUT /my_index
{
  &quot;settings&quot;: {
    &quot;analysis&quot;: {
      &quot;analyzer&quot;: {
        &quot;my_analyzer&quot;: {
          &quot;type&quot;: &quot;standard&quot;,
          &quot;stopwords&quot;: [&quot;the&quot;, &quot;a&quot;, &quot;an&quot;, &quot;this&quot;, &quot;is&quot;]
        }
      }
    }
  }
}
</code></pre>
<h4><a id="2-1-2%E3%80%81%E4%B8%BA%E5%AD%97%E6%AE%B5%E6%8C%87%E5%AE%9A%E5%86%85%E7%BD%AE%E5%88%86%E8%AF%8D%E5%99%A8" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>2.1.2、为字段指定内置分词器</h4>
<pre><code class="language-bash">DELETE /my_index
PUT /my_index
{
  &quot;mappings&quot;: {
    &quot;properties&quot;: {
      &quot;title&quot;: {
        &quot;type&quot;: &quot;text&quot;,
        &quot;analyzer&quot;: &quot;standard&quot;,
        &quot;search_analyzer&quot;: &quot;simple&quot;
      }
    }
  }
}
</code></pre>
<p>还可以自定义分词器</p>
<pre><code class="language-bash">DELETE /my_index
PUT /my_index
{
  &quot;settings&quot;: {
    &quot;analysis&quot;: {
      &quot;analyzer&quot;: {
        &quot;std_english&quot;: {
          &quot;type&quot;: &quot;standard&quot;,
          &quot;stopwords&quot;: &quot;_english_&quot;
        }
      }
    }
  },
  &quot;mappings&quot;: {
    &quot;properties&quot;: {
      &quot;my_text&quot;: {
        &quot;type&quot;: &quot;text&quot;,
        &quot;analyzer&quot;: &quot;standard&quot;,
        &quot;fields&quot;: {
          &quot;english&quot;: {
            &quot;type&quot;: &quot;text&quot;,
            &quot;analyzer&quot;: &quot;std_english&quot;
          }
        }
      }
    }
  }
}
</code></pre>
<p>我们首先，在索引 my_index 中配置了一个分析器 std_english，std_english 中使用了内置分析器 standard，并将 standard 的停止词模式改为英语模式 <code>_english_</code> (缺省是没有的)，对字段 my_text 配置为多数据类型，分别使用了两 种分析器，standard 和 std_english。</p>
<pre><code class="language-bash">POST /my_index/_analyze
{
  &quot;field&quot;: &quot;my_text&quot;,
  &quot;text&quot;: &quot;There is an old man&quot;
}
</code></pre>
<p>response:</p>
<pre><code class="language-json">{
  &quot;tokens&quot; : [
    {
      &quot;token&quot; : &quot;there&quot;,
      &quot;start_offset&quot; : 0,
      &quot;end_offset&quot; : 5,
      &quot;type&quot; : &quot;&lt;ALPHANUM&gt;&quot;,
      &quot;position&quot; : 0
    },
    {
      &quot;token&quot; : &quot;is&quot;,
      &quot;start_offset&quot; : 6,
      &quot;end_offset&quot; : 8,
      &quot;type&quot; : &quot;&lt;ALPHANUM&gt;&quot;,
      &quot;position&quot; : 1
    },
    {
      &quot;token&quot; : &quot;an&quot;,
      &quot;start_offset&quot; : 9,
      &quot;end_offset&quot; : 11,
      &quot;type&quot; : &quot;&lt;ALPHANUM&gt;&quot;,
      &quot;position&quot; : 2
    },
    {
      &quot;token&quot; : &quot;old&quot;,
      &quot;start_offset&quot; : 12,
      &quot;end_offset&quot; : 15,
      &quot;type&quot; : &quot;&lt;ALPHANUM&gt;&quot;,
      &quot;position&quot; : 3
    },
    {
      &quot;token&quot; : &quot;man&quot;,
      &quot;start_offset&quot; : 16,
      &quot;end_offset&quot; : 19,
      &quot;type&quot; : &quot;&lt;ALPHANUM&gt;&quot;,
      &quot;position&quot; : 4
    }
  ]
}
</code></pre>
<pre><code class="language-bash">POST /my_index/_analyze
{
  &quot;field&quot;: &quot;my_text.english&quot;,
  &quot;text&quot;: &quot;There is an old man&quot;
}
</code></pre>
<p>response:</p>
<pre><code class="language-json">{
  &quot;tokens&quot; : [
    {
      &quot;token&quot; : &quot;old&quot;,
      &quot;start_offset&quot; : 12,
      &quot;end_offset&quot; : 15,
      &quot;type&quot; : &quot;&lt;ALPHANUM&gt;&quot;,
      &quot;position&quot; : 3
    },
    {
      &quot;token&quot; : &quot;man&quot;,
      &quot;start_offset&quot; : 16,
      &quot;end_offset&quot; : 19,
      &quot;type&quot; : &quot;&lt;ALPHANUM&gt;&quot;,
      &quot;position&quot; : 4
    }
  ]
}
</code></pre>
<p>通过上面两个例子看出来 There is an 经过分析之后，被删除了，而standard没有被删除，这是因为 my_text.english 单独配置单独的停止词。</p>
<h3><a id="2-2%E3%80%81%E6%96%87%E6%A1%A3%E6%90%9C%E7%B4%A2%E6%97%B6" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>2.2、文档搜索时</h3>
<p>文档搜索时使用的分析器有一点复杂，它依次从如下参数中如果查找文档分析器，如果都没有设置则使用 standard 分析器:</p>
<ul>
<li>1、搜索时指定 analyzer 参数</li>
<li>2、创建索引时字段指定的 search_analyzer 属性</li>
<li>3、创建索引时字段指定的 analyzer 属性</li>
<li>4、创建索引时 setting 里指定的 analysis.analyzer.default_search</li>
<li>5、如果都没有设置则使用 standard 分析器</li>
</ul>
<h4><a id="2-2-1%E3%80%81%E6%90%9C%E7%B4%A2%E6%97%B6%E6%8C%87%E5%AE%9A-analyzer%E6%9F%A5%E8%AF%A2%E5%8F%82%E6%95%B0" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>2.2.1、搜索时指定 analyzer 查询参数</h4>
<pre><code class="language-bash">GET /my_index/_search
{
  &quot;query&quot;: {
    &quot;match&quot;: {
      &quot;message&quot;: {
        &quot;query&quot;: &quot;Quick foxes&quot;,
        &quot;analyzer&quot;: &quot;stop&quot;
      }
    }
  }
}
</code></pre>
<h4><a id="2-2-2%E3%80%81%E5%88%9B%E5%BB%BA%E7%B4%A2%E5%BC%95%E6%97%B6%E6%8C%87%E5%AE%9A%E5%AD%97%E6%AE%B5%E7%9A%84-analyzer%E5%92%8C-seach-analyzer" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>2.2.2、创建索引时指定字段的 analyzer 和 seach_analyzer</h4>
<pre><code class="language-bash">PUT /my_index
{
  &quot;mappings&quot;: {
    &quot;properties&quot;: {
      &quot;title&quot;: {
        &quot;type&quot;: &quot;text&quot;,
        &quot;analyzer&quot;: &quot;whitespace&quot;,
        &quot;search_analyzer&quot;: &quot;simple&quot;
      }
    }
  }
}
</code></pre>
<h4><a id="2-2-3%E3%80%81%E5%88%9B%E5%BB%BA%E7%B4%A2%E5%BC%95%E6%97%B6%E6%8C%87%E5%AE%9A%E7%B4%A2%E5%BC%95%E7%9A%84%E9%BB%98%E8%AE%A4%E6%90%9C%E7%B4%A2%E5%88%86%E8%AF%8D%E5%99%A8" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>2.2.3、创建索引时指定索引的默认搜索分词器</h4>
<p>analysis.analyzer.default_search</p>
<pre><code class="language-bash">PUT /my_index
{
  &quot;settings&quot;: {
    &quot;analysis&quot;: {
      &quot;analyzer&quot;: {
        &quot;default&quot;: {
          &quot;type&quot;: &quot;simple&quot;
        },
        &quot;default_seach&quot;: {
          &quot;type&quot;: &quot;whitespace&quot;
        }
      }
    }
  }
}
</code></pre>
<hr />
<h2><a id="3%E3%80%81%E5%86%85%E7%BD%AE%E5%88%86%E6%9E%90%E5%99%A8" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>3、内置分析器</h2>
<p>前面说过，每个被分析字段经过一系列的处理步骤:<br />
1、字符过滤---使用字符过滤器转变字符。<br />
2、文本切分为分词---将文本切分为单个或多个分词。<br />
3、分词过滤---使用分词过滤器转变每个分词。</p>
<p>每个分析器基本上都要包含上面三个步骤至少一个。其中字符过滤器可以为 0 个，也可以为多个，分词器则必须，但是也只能有一个，分词过滤器可以为 0 个，也可以为多个。</p>
<p>Elasticsearch 已经为我们内置了很多的字符过滤器、分词器和分词过滤器， 以及分析器。不过常用的就是那么几个。</p>
<h3><a id="3-1%E3%80%81%E5%AD%97%E7%AC%A6%E8%BF%87%E6%BB%A4%E5%99%A8-character-filters" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>3.1、字符过滤器(Character filters)</h3>
<p>字符过滤器种类不多, elasticearch 只提供了三种字符过滤器</p>
<h4><a id="3-1-1%E3%80%81html%E5%AD%97%E7%AC%A6%E8%BF%87%E6%BB%A4%E5%99%A8-html-strip-char-filter" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>3.1.1、HTML 字符过滤器(HTML Strip Char Filter)</h4>
<p>从文本中去除 HTML 元素。<br />
html_strip</p>
<pre><code class="language-bash">POST _analyze
{
  &quot;tokenizer&quot;: &quot;keyword&quot;,
  &quot;char_filter&quot;: [&quot;html_strip&quot;],
  &quot;text&quot;: &quot;&lt;p&gt;I&amp;apos;m your &lt;b&gt;brother&lt;/b&gt;!&lt;/p&gt;&quot;
}
</code></pre>
<p>response:</p>
<pre><code class="language-json">{
  &quot;tokens&quot; : [
    {
      &quot;token&quot; : &quot;&quot;&quot;
I'm your brother!
&quot;&quot;&quot;,
      &quot;start_offset&quot; : 0,
      &quot;end_offset&quot; : 36,
      &quot;type&quot; : &quot;word&quot;,
      &quot;position&quot; : 0
    }
  ]
}
</code></pre>
<h4><a id="3-1-2%E3%80%81%E6%98%A0%E5%B0%84%E5%AD%97%E7%AC%A6%E8%BF%87%E6%BB%A4%E5%99%A8-mapping-char-filter" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>3.1.2、映射字符过滤器(Mapping Char Filter)</h4>
<p>接收键值的映射，每当遇到与键相同的字符串时，它就用该键关联的值替换它们。</p>
<p>我们自定义一个分析器，其内的分词器使用关键字分词器，字符 过滤器则是自定制的，将字符中的 &quot;土豆哪里去挖&quot; 替换为 666，&quot;一挖一麻袋&quot; 替换为 888。</p>
<pre><code class="language-bash">PUT /my_index
{
  &quot;settings&quot;: {
    &quot;analysis&quot;: {
      &quot;analyzer&quot;: {
        &quot;my_analyzer&quot;: {
          &quot;tokenizer&quot;: &quot;keyword&quot;,
          &quot;char_filter&quot;: [
            &quot;my_char_filter&quot;
          ]
        }
      },
      &quot;char_filter&quot;: {
        &quot;my_char_filter&quot;: {
          &quot;type&quot;: &quot;mapping&quot;,
          &quot;mappings&quot;: [
            &quot;土豆哪里去挖 =&gt; 666&quot;,
            &quot;一挖一麻袋 =&gt; 888&quot;
          ]
        }
      }
    }
  }
}
</code></pre>
<pre><code class="language-bash">POST /my_index/_analyze
{
  &quot;analyzer&quot;: &quot;my_analyzer&quot;,
  &quot;text&quot;: &quot; 土豆哪里去挖，土豆山里去挖，一挖一麻袋&quot;
}
</code></pre>
<p>response:</p>
<pre><code class="language-json">{
  &quot;tokens&quot; : [
    {
      &quot;token&quot; : &quot; 666，土豆山里去挖，888&quot;,
      &quot;start_offset&quot; : 0,
      &quot;end_offset&quot; : 20,
      &quot;type&quot; : &quot;word&quot;,
      &quot;position&quot; : 0
    }
  ]
}

</code></pre>
<h4><a id="3-1-3%E3%80%81%E6%A8%A1%E5%BC%8F%E6%9B%BF%E6%8D%A2%E8%BF%87%E6%BB%A4%E5%99%A8-pattern-replace-char-filter" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>3.1.3、模式替换过滤器(Pattern Replace Char Filter)</h4>
<pre><code class="language-bash">POST _analyze
{
  &quot;analyzer&quot;: &quot;standard&quot;,
  &quot;text&quot;: &quot;My phone number is 123-456-789&quot;
}
</code></pre>
<p>这样分词，会导致 123-456-789 被分为 123 456 789，但是我们希望 123-456-789 是一个整体，可以使用模式替换过滤器，替换掉“-”</p>
<pre><code class="language-plain_text">PUT /my_index
{
  &quot;settings&quot;: {
    &quot;analysis&quot;: {
      &quot;analyzer&quot;: {
        &quot;my_analyzer&quot;: {
          &quot;tokenizer&quot;: &quot;standard&quot;,
          &quot;char_filter&quot;: [
            &quot;my_char_filter&quot;
          ]
        }
      },
      &quot;char_filter&quot;: {
        &quot;my_char_filter&quot;: {
          &quot;type&quot;: &quot;pattern_replace&quot;,
          &quot;pattern&quot;: &quot;&quot;&quot;(\d+)-(?=\d)&quot;&quot;&quot;,
          &quot;replacement&quot;: &quot;$1_&quot;
        }
      }
    }
  }
}
</code></pre>
<pre><code class="language-bash">POST /my_index/_analyze
{
  &quot;analyzer&quot;: &quot;my_analyzer&quot;,
  &quot;text&quot;: &quot;My phone number is 123-456-789&quot;
}
</code></pre>
<p>response</p>
<pre><code class="language-json">{
  &quot;tokens&quot; : [
    {
      &quot;token&quot; : &quot;My&quot;,
      &quot;start_offset&quot; : 0,
      &quot;end_offset&quot; : 2,
      &quot;type&quot; : &quot;&lt;ALPHANUM&gt;&quot;,
      &quot;position&quot; : 0
    },
    {
      &quot;token&quot; : &quot;credit&quot;,
      &quot;start_offset&quot; : 3,
      &quot;end_offset&quot; : 9,
      &quot;type&quot; : &quot;&lt;ALPHANUM&gt;&quot;,
      &quot;position&quot; : 1
    },
    {
      &quot;token&quot; : &quot;card&quot;,
      &quot;start_offset&quot; : 10,
      &quot;end_offset&quot; : 14,
      &quot;type&quot; : &quot;&lt;ALPHANUM&gt;&quot;,
      &quot;position&quot; : 2
    },
    {
      &quot;token&quot; : &quot;is&quot;,
      &quot;start_offset&quot; : 15,
      &quot;end_offset&quot; : 17,
      &quot;type&quot; : &quot;&lt;ALPHANUM&gt;&quot;,
      &quot;position&quot; : 3
    },
    {
      &quot;token&quot; : &quot;123_456_789&quot;,
      &quot;start_offset&quot; : 18,
      &quot;end_offset&quot; : 29,
      &quot;type&quot; : &quot;&lt;NUM&gt;&quot;,
      &quot;position&quot; : 4
    }
  ]
}
</code></pre>
<hr />
<h3><a id="3-2%E3%80%81%E5%88%86%E8%AF%8D%E5%99%A8%EF%BC%88tokenizer%EF%BC%89" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>3.2、分词器（Tokenizer）</h3>
<h4><a id="3-2-1%E3%80%81%E6%A0%87%E5%87%86%E5%88%86%E8%AF%8D%E5%99%A8-standard" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>3.2.1、标准分词器(standard)</h4>
<p>标准分词器( standard tokenizer) 是一个基于语法的分词器，对于大多数欧洲<br />
语言来说是不错的。它还处理了 Unicode 文本的切分。它也移除了逗号和句号这样的标点符号</p>
<h4><a id="3-2-2%E3%80%81%E5%85%B3%E9%94%AE%E8%AF%8D%E5%88%86%E8%AF%8D%E5%99%A8-keyword" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>3.2.2、关键词分词器(keyword)</h4>
<p>关键词分词器( keyword tokenizer )是-种简单的分词器，将整个文本作为单个的分词，提供给分词过滤器。只想应用分词过滤器，而不做任何分词操作时，它可能非常有用。</p>
<pre><code class="language-bash">POST _analyze
{
  &quot;analyzer&quot;: &quot;keyword&quot;,
  &quot;text&quot;: &quot;My credit-- card is 123-456-789&quot;
}
</code></pre>
<p>response</p>
<pre><code class="language-json">{
  &quot;tokens&quot; : [
    {
      &quot;token&quot; : &quot;My credit-- card is 123-456-789&quot;,
      &quot;start_offset&quot; : 0,
      &quot;end_offset&quot; : 31,
      &quot;type&quot; : &quot;word&quot;,
      &quot;position&quot; : 0
    }
  ]
}

</code></pre>
<h4><a id="3-2-3%E3%80%81%E5%AD%97%E6%AF%8D%E5%88%86%E8%AF%8D%E5%99%A8-letter" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>3.2.3、字母分词器(letter)</h4>
<p>字母分词器根据非字母的符号,将文本切分成分词。例如，对于句子<br />
“Hi,there.&quot;分词是 Hi 和 there,</p>
<p>因为逗号、空格和句号都不是字母: 'Hi, there. '分词是 Hi 和 there。</p>
<h4><a id="3-2-4%E3%80%81%E5%B0%8F%E5%86%99%E5%88%86%E8%AF%8D%E5%99%A8-lowercase" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>3.2.4、小写分词器(lowercase)</h4>
<p>小写分词器( lowercase tokenizer)结合了常规的字母分词器和小写分词过滤<br />
器(如你所想，它将整个分词转化为小写)的行为。通过 1 个单独的分词器来实现 的主要原因是，2 次进行两项操作会获得更好的性能。<br />
'Hi, there.'分词是 hi 和 there。</p>
<h4><a id="3-2-5%E3%80%81%E7%A9%BA%E7%99%BD%E5%88%86%E8%AF%8D%E5%99%A8-whitespace" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>3.2.5、空白分词器(whitespace)</h4>
<p>空白分词器( whitespace tokenizer )通过空白来分隔不同的分词，空白包括空<br />
格、制表符、换行等。请注意，这种分词器不会删除任何标点符号。<br />
'Hi， there. '分词是 Hi,和 there.</p>
<h4><a id="3-2-6%E3%80%81%E6%A8%A1%E5%BC%8F%E5%88%86%E8%AF%8D%E5%99%A8-pattern" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>3.2.6、模式分词器(pattern)</h4>
<p>模式分词器( patterm tokenizer)允许指定一个任 意的模式，将文本切分为分<br />
词。被指定的模式应该匹配间隔符号。例如，可以创建一个定制分析器，它在出 现文本“. -.”的地方将分词断开。</p>
<h4><a id="3-2-7%E3%80%81uax-url%E7%94%B5%E5%AD%90%E9%82%AE%E4%BB%B6%E5%88%86%E8%AF%8D%E5%99%A8-uax-url-email" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>3.2.7、UAX URL 电子邮件分词器(uax_url_email)</h4>
<h4><a id="3-2-8%E3%80%81%E8%B7%AF%E5%BE%84%E5%B1%82%E6%AC%A1%E5%88%86%E8%AF%8D%E5%99%A8-path-hierarchy" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>3.2.8、路径层次分词器(path_hierarchy)</h4>
<p>路径层次分词器( path hierarchy tokenizer )允许以特定的方式索引文件系统<br />
的路径，这样在搜索时，共享同样路径的文件将被作为结果返回。</p>
<hr />
<h3><a id="3-3%E3%80%81%E5%88%86%E8%AF%8D%E8%BF%87%E6%BB%A4%E5%99%A8-token-filters" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>3.3、分词过滤器(Token filters)</h3>
<h4><a id="3-3-1%E3%80%81%E6%A0%87%E5%87%86%E5%88%86%E8%AF%8D%E8%BF%87%E6%BB%A4%E5%99%A8-standard" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>3.3.1、标准分词过滤器(standard)</h4>
<p>不要认为标准分词过滤器(standard token filter)进行了什么复杂的计算，实际上它什么事情也没做。</p>
<h4><a id="3-3-2%E3%80%81%E5%B0%8F%E5%86%99%E5%88%86%E8%AF%8D%E8%BF%87%E6%BB%A4%E5%99%A8-lowercase" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>3.3.2、小写分词过滤器(lowercase)</h4>
<p>小写分词过滤器( lowercase token filter)只是做了这件事:将任何经过的分词<br />
转换为小写。这应该非常简单也易于理解。</p>
<h4><a id="3-3-3%E3%80%81%E9%95%BF%E5%BA%A6%E5%88%86%E8%AF%8D%E8%BF%87%E6%BB%A4%E5%99%A8-length" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>3.3.3、长度分词过滤器(length)</h4>
<p>长度分词过滤器(length token filter)将长度超出最短和最长限制范围的单词<br />
过滤掉。举个例子，如果将 min 设置为 2，并将 max 设置为 8，任何小于 2 个字符和任何大于 8 个字符的分词将会被移除。</p>
<h4><a id="3-3-4%E3%80%81%E5%81%9C%E7%94%A8%E8%AF%8D%E5%88%86%E8%AF%8D%E8%BF%87%E6%BB%A4%E5%99%A8-stop" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>3.3.4、停用词分词过滤器(stop)</h4>
<p>停用词分词过滤器(stop token fite)将停用词从分词流中移除。对于英文而言，<br />
这意味着停用词列表中的所有分词都将会被完全移除。用户也可以为这个过滤器指定一个待移除单词的列表。</p>
<p>什么是停用词?<br />
停用词是指在信息检索中，为节省存储空间和提高搜索效率，在处理自然语 言数据(或文本)之前或之后会自动过滤掉某些字或词，这些字或词即被称为 Stop Words(停用词)</p>
<p>停用词(Stop Words)大致可分为如下两类：</p>
<ul>
<li>使用十分广泛，甚至是过于频繁的一些单词。比如英文的“i”、“is”、 “what”，中文的“我”、“就”之类词几乎在每个文档上均会出现</li>
<li>文本中出现频率很高，但实际意义又不大的词。这一类主要包括了语气 助词、副词、介词、连词等，通常自身并无明确意义，只有将其放入一个完整的 句子中才有一定作用的词语。如常见的“的”、“在”、“和”、“接着”之类</li>
</ul>
<h4><a id="3-3-5%E3%80%81%E6%88%AA%E6%96%AD%E5%88%86%E8%AF%8D%E8%BF%87%E6%BB%A4%E5%99%A8%E3%80%81%E4%BF%AE%E5%89%AA%E5%88%86%E8%AF%8D%E8%BF%87%E6%BB%A4%E5%99%A8%E5%92%8C%E9%99%90%E5%88%B6%E5%88%86%E8%AF%8D%E6%95%B0%E9%87%8F%E8%BF%87%E6%BB%A4%E5%99%A8" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>3.3.5、截断分词过滤器、修剪分词过滤器和限制分词数量过滤器</h4>
<p>下面 3 个分词过滤器，通过某种方式限制分词流</p>
<ul>
<li>截断分词过滤器(truncate token filter)允许你通过定制配置中的 length 参数，截断超过一定长度的分词。默认截断多于 10 个字符的部分。</li>
<li>修剪分词过滤器(trim token filter) 删除 1 个分词中的所有空白部分。例如， 分词&quot; foo &quot;将被转变为分词 foo。</li>
<li>限制分词数量分词过滤器(limit token count token filter)限制了某个字段可包含分词的最大数量。例如，如果创建了一个定制的分词数量过滤器，限制是 8, 那么分词流中只有前 8 个分词会被索引。这个设置使用 max_token_count 参数， 默认是 1 (只有 1 个分词会被索引)。</li>
</ul>
<hr />
<h3><a id="3-4%E3%80%81%E5%B8%B8%E7%94%A8%E5%86%85%E7%BD%AE%E5%88%86%E6%9E%90%E5%99%A8" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>3.4、常用内置分析器</h3>
<h4><a id="3-4-1%E3%80%81%E6%A0%87%E5%87%86%E5%88%86%E6%9E%90%E5%99%A8" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>3.4.1、标准分析器</h4>
<p>当没有指定分析器的时候，标准分析器( standardanalyzer)是文本的默认分析<br />
器。它综合了对大多欧洲语言来说合理的默认模块，它没有字符过滤器，包括标 准分词器、小写转换分词过滤器和停用词分词过滤器(默认为_none_，也就是不去除停止词)。这里只需要记住，如果不为某个字段指定分析器，那么该字段就 会使用标准分析器。可配置的参数如下:</p>
<ul>
<li>max_token_length，默认值 255，表示词项最大长度，超过这个长度将按该 长度分为多个词项</li>
<li>stopwords，默认值_none_，表示分析器使用的停止词数组，可使用内置停 止词列表，比如_english_等</li>
<li>stopwords_path 停止词文件路径</li>
</ul>
<h4><a id="3-4-2%E3%80%81%E7%AE%80%E5%8D%95%E5%88%86%E6%9E%90%E5%99%A8" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>3.4.2、简单分析器</h4>
<p>简单分析器(simple analyzer)就是那么简单!它只使用了小写转换分词器，这<br />
意味着在非字母处进行分词，并将分词自动转变为小写。这个分析器对于亚洲语言来说效果不佳，因为亚洲语言不是根据空白来分词，所以请仅仅针对欧洲语言使用它。</p>
<h4><a id="3-4-3%E3%80%81%E7%A9%BA%E7%99%BD%E5%88%86%E6%9E%90%E5%99%A8" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>3.4.3、空白分析器</h4>
<p>空白分析器(whitespace analyzer)什么事情都不做,只是根据空白将文本切分<br />
为若干分词。</p>
<h4><a id="3-4-4%E3%80%81%E5%81%9C%E7%94%A8%E8%AF%8D%E5%88%86%E6%9E%90%E5%99%A8" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>3.4.4、停用词分析器</h4>
<p>停用词分析器(stop analyzer) 和建单分析器的行为很像，只是在分词流中额外的过滤了停用词</p>
<h4><a id="3-4-5%E3%80%81%E5%85%B3%E9%94%AE%E8%AF%8D%E5%88%86%E6%9E%90%E5%99%A8" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>3.4.5、关键词分析器</h4>
<p>关键词分析器（keyword analyzer）将整个字段当做一个单独的分词</p>
<h4><a id="3-4-6%E3%80%81%E6%A8%A1%E5%BC%8F%E5%88%86%E6%9E%90%E5%99%A8" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>3.4.6、模式分析器</h4>
<p>模板分析器(pattern analyzer)允许你指定一个分词切分的模式。 但是，由<br />
于可能无论如何都要指定模式，通常更有意义的做法是使用定制分析器，组合现 有的模式分词器和所需的分词过滤器</p>
<h4><a id="3-4-7%E3%80%81%E9%9B%AA%E7%90%83%E5%88%86%E6%9E%90%E5%99%A8" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>3.4.7、雪球分析器</h4>
<p>雪球分析器(snowball analyzer)除了使用标准的分词器和分词过滤器(和标<br />
准分析器一样),也使用了小写分词过滤器和停用词过滤器。它还使用了雪球词干 器对文本进行词干提取。</p>
<hr />
<h2><a id="4%E3%80%81%E8%87%AA%E5%AE%9A%E4%B9%89%E5%88%86%E6%9E%90%E5%99%A8" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>4、自定义分析器</h2>
<p>业务需求如下:<br />
去除所有的 HTML 标签<br />
将 &amp; 替换成 and ，使用一个自定义的 mapping 字符过滤器</p>
<p>1）自定义 mapping 类型的字符过滤器<br />
2）自定义一个停止词 分词过滤器<br />
3）组装分析器</p>
<pre><code class="language-bash">DELETE /my_index
PUT /my_index
{
  &quot;settings&quot;: {
    &quot;analysis&quot;: {
      &quot;analyzer&quot;: {
        &quot;my_analyzer&quot;:{
          &quot;type&quot;:&quot;custom&quot;,
          &quot;char_filter&quot;:[&quot;&amp;_to_and&quot;,&quot;html_strip&quot;],
          &quot;filter&quot;:[&quot;my_stopwords&quot;,&quot;lowercase&quot;],
          &quot;tokenizer&quot;:&quot;standard&quot;
          
        }
      },
      &quot;char_filter&quot;: {
        &quot;&amp;_to_and&quot;:{
          &quot;type&quot;:&quot;mapping&quot;,
          &quot;mappings&quot;:[
            &quot;&amp;=&gt;and&quot;
          ]
        }
      },
      &quot;filter&quot;: {
        &quot;my_stopwords&quot;:{
          &quot;type&quot;:&quot;stop&quot;,
          &quot;stopwords&quot;:[
            &quot;play&quot;,&quot;giao&quot;
          ]
        }
      }
    }
  }
}
</code></pre>
<pre><code class="language-bash">POST /my_index/_analyze
{
  &quot;analyzer&quot;: &quot;my_analyzer&quot;,
  &quot;text&quot;: [&quot;I giao do not want to giao work,play&quot;]
}
</code></pre>
<p>response:</p>
<pre><code class="language-json">{
  &quot;tokens&quot; : [
    {
      &quot;token&quot; : &quot;i&quot;,
      &quot;start_offset&quot; : 0,
      &quot;end_offset&quot; : 1,
      &quot;type&quot; : &quot;&lt;ALPHANUM&gt;&quot;,
      &quot;position&quot; : 0
    },
    {
      &quot;token&quot; : &quot;do&quot;,
      &quot;start_offset&quot; : 7,
      &quot;end_offset&quot; : 9,
      &quot;type&quot; : &quot;&lt;ALPHANUM&gt;&quot;,
      &quot;position&quot; : 2
    },
    {
      &quot;token&quot; : &quot;not&quot;,
      &quot;start_offset&quot; : 10,
      &quot;end_offset&quot; : 13,
      &quot;type&quot; : &quot;&lt;ALPHANUM&gt;&quot;,
      &quot;position&quot; : 3
    },
    {
      &quot;token&quot; : &quot;want&quot;,
      &quot;start_offset&quot; : 14,
      &quot;end_offset&quot; : 18,
      &quot;type&quot; : &quot;&lt;ALPHANUM&gt;&quot;,
      &quot;position&quot; : 4
    },
    {
      &quot;token&quot; : &quot;to&quot;,
      &quot;start_offset&quot; : 19,
      &quot;end_offset&quot; : 21,
      &quot;type&quot; : &quot;&lt;ALPHANUM&gt;&quot;,
      &quot;position&quot; : 5
    },
    {
      &quot;token&quot; : &quot;work&quot;,
      &quot;start_offset&quot; : 27,
      &quot;end_offset&quot; : 31,
      &quot;type&quot; : &quot;&lt;ALPHANUM&gt;&quot;,
      &quot;position&quot; : 7
    }
  ]
}
</code></pre>
<h2><a id="5%E3%80%81%E4%B8%AD%E6%96%87%E5%88%86%E6%9E%90%E5%99%A8" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>5、中文分析器</h2>
<p>上面的分析器对中文的支持不行，这里使用比较有名的 ik 作为这次的中文分析器。</p>
<p>IK 分词器有两种分词效果，一种是 ik_max_word(最大分词)和 ik_smart(最 小分词)</p>
<ul>
<li>ik_max_word: 会将文本做最细粒度的拆分，比如会将“马克思主义思想”拆分为“马克思主义,马克思,马克,思想,主义等等，会穷尽各种可能的组合;</li>
<li>ik_smart: 会做最粗粒度的拆分，比如会将“马克思主义思想”拆分为 “马克思主义,思想”。</li>
</ul>
<p>使用方式和一般的分析器没有什么差别</p>
<pre><code class="language-bash">POST _analyze
{
  &quot;analyzer&quot;: &quot;ik_max_word&quot;,
  &quot;text&quot;: [&quot;马克思主义思想&quot;]
}

</code></pre>
<p>response:</p>
<pre><code class="language-json">{
  &quot;tokens&quot; : [
    {
      &quot;token&quot; : &quot;马克思主义&quot;,
      &quot;start_offset&quot; : 0,
      &quot;end_offset&quot; : 5,
      &quot;type&quot; : &quot;CN_WORD&quot;,
      &quot;position&quot; : 0
    },
    {
      &quot;token&quot; : &quot;马克思&quot;,
      &quot;start_offset&quot; : 0,
      &quot;end_offset&quot; : 3,
      &quot;type&quot; : &quot;CN_WORD&quot;,
      &quot;position&quot; : 1
    },
    {
      &quot;token&quot; : &quot;马克&quot;,
      &quot;start_offset&quot; : 0,
      &quot;end_offset&quot; : 2,
      &quot;type&quot; : &quot;CN_WORD&quot;,
      &quot;position&quot; : 2
    },
    {
      &quot;token&quot; : &quot;思&quot;,
      &quot;start_offset&quot; : 2,
      &quot;end_offset&quot; : 3,
      &quot;type&quot; : &quot;CN_CHAR&quot;,
      &quot;position&quot; : 3
    },
    {
      &quot;token&quot; : &quot;主义&quot;,
      &quot;start_offset&quot; : 3,
      &quot;end_offset&quot; : 5,
      &quot;type&quot; : &quot;CN_WORD&quot;,
      &quot;position&quot; : 4
    },
    {
      &quot;token&quot; : &quot;思想&quot;,
      &quot;start_offset&quot; : 5,
      &quot;end_offset&quot; : 7,
      &quot;type&quot; : &quot;CN_WORD&quot;,
      &quot;position&quot; : 5
    }
  ]
}

</code></pre>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[ElasticSearch 检索]]></title>
    <link href="http://www.throne4j.com/16398164590937.html"/>
    <updated>2021-12-18T16:34:19+08:00</updated>
    <id>http://www.throne4j.com/16398164590937.html</id>
    <content type="html"><![CDATA[
<ul>
<li><a href="#1%E3%80%81%E6%95%B0%E6%8D%AE%E6%A3%80%E7%B4%A2%E5%92%8C%E5%88%86%E6%9E%90">1、数据检索和分析</a>
<ul>
<li><a href="#1-1%E3%80%81-search%E6%8E%A5%E5%8F%A3">1.1、_search 接口</a></li>
</ul>
</li>
<li><a href="#2%E3%80%81%E5%9F%BA%E4%BA%8E%E8%AF%8D%E9%A1%B9%E7%9A%84%E6%90%9C%E7%B4%A2">2、基于词项的搜索</a>
<ul>
<li><a href="#2-1%E3%80%81term%E6%9F%A5%E8%AF%A2">2.1、term 查询</a></li>
<li><a href="#2-2%E3%80%81terms%E6%9F%A5%E8%AF%A2">2.2、terms 查询</a></li>
<li><a href="#2-3%E3%80%81range%E6%9F%A5%E8%AF%A2%E5%92%8C-exists%E6%9F%A5%E8%AF%A2">2.3、range 查询和 exists 查询</a></li>
<li><a href="#2-4%E3%80%81prefix%E6%9F%A5%E8%AF%A2">2.4、prefix 查询</a></li>
<li><a href="#2-5%E3%80%81wildcard%E6%9F%A5%E8%AF%A2%E5%92%8C-regexp%E6%9F%A5%E8%AF%A2">2.5、wildcard 查询和 regexp 查询</a></li>
</ul>
</li>
<li><a href="#3%E3%80%81%E5%9F%BA%E4%BA%8E%E5%85%A8%E6%96%87%E7%B4%A2%E5%BC%95%E7%9A%84%E6%A3%80%E7%B4%A2">3、基于全文索引的检索</a>
<ul>
<li><a href="#3-1%E3%80%81match%E6%9F%A5%E8%AF%A2">3.1、match 查询</a></li>
<li><a href="#3-2%E3%80%81match-phrase%E6%9F%A5%E8%AF%A2">3.2、match_phrase 查询</a></li>
<li><a href="#3-3%E3%80%81multi-match%E6%9F%A5%E8%AF%A2">3.3、multi_match 查询</a></li>
<li><a href="#3-4%E3%80%81match-phrase-prefix%E6%9F%A5%E8%AF%A2">3.4、match_phrase_prefix 查询</a></li>
</ul>
</li>
<li><a href="#4%E3%80%81%E6%A8%A1%E7%B3%8A%E6%9F%A5%E8%AF%A2%E3%80%81%E7%BA%A0%E9%94%99%E4%B8%8E%E6%8F%90%E7%A4%BA%E5%99%A8">4、模糊查询、纠错与提示器</a>
<ul>
<li><a href="#4-1%E7%BC%96%E8%BE%91%E8%B7%9D%E7%A6%BB%E7%AE%97%E6%B3%95">4.1编辑距离算法</a>
<ul>
<li><a href="#4-1-1%E3%80%81levenshtein%E7%AE%97%E6%B3%95">4.1.1、Levenshtein算法</a></li>
<li><a href="#4-1-2%E3%80%81ngram%E7%AE%97%E6%B3%95">4.1.2、NGram 算法</a></li>
</ul>
</li>
<li><a href="#4-1-2%E3%80%81%E6%A8%A1%E7%B3%8A%E6%9F%A5%E8%AF%A2">4.1.2、模糊查询</a></li>
<li><a href="#%E7%BA%A0%E9%94%99%E4%B8%8E%E6%8F%90%E7%A4%BA%E5%99%A8">纠错与提示器</a></li>
</ul>
</li>
<li><a href="#%E7%BB%84%E5%90%88%E6%9F%A5%E8%AF%A2">组合查询</a>
<ul>
<li><a href="#bool%E7%BB%84%E5%90%88%E6%9F%A5%E8%AF%A2">bool 组合查询</a></li>
<li><a href="#dis-max%E7%BB%84%E5%90%88%E6%9F%A5%E8%AF%A2">dis_max 组合查询</a></li>
<li><a href="#constant-score%E6%9F%A5%E8%AF%A2">constant_score 查询</a></li>
<li><a href="#boosting%E6%9F%A5%E8%AF%A2">boosting 查询</a></li>
</ul>
</li>
</ul>

<h2><a id="1%E3%80%81%E6%95%B0%E6%8D%AE%E6%A3%80%E7%B4%A2%E5%92%8C%E5%88%86%E6%9E%90" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>1、数据检索和分析</h2>
<p>为了方便我们学习，我们导入 kibana 为我们提供的范例数据。<br />
<figure><img src="media/16398107958136/16399184131685.jpg" alt="" /></figure></p>
<hr />
<h3><a id="1-1%E3%80%81-search%E6%8E%A5%E5%8F%A3" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>1.1、_search 接口</h3>
<p>_search 接口有两种请求方法，一种是基于 URI 的请求方式，另一种是基于请求体的方式，无论哪种，他们执行的语法都是基于 DSL(ES 为我们定义的查询语言，基于 JSON 的查询语言)，只是形式上不同。</p>
<pre><code class="language-bash">POST /kibana_sample_data_flights/_search
{
  &quot;query&quot;: {
    &quot;match_all&quot;: {
      
    }
  },
  &quot;from&quot;:0,
  &quot;size&quot;:10,
  &quot;sort&quot;: [{&quot;AvgTicketPrice&quot;: {&quot;order&quot;: &quot;desc&quot;}}]
}

&quot;_source&quot;: [&quot;AvgTicketPrice&quot;,&quot;DistanceMiles&quot;,&quot;Origin*&quot;,&quot;Dest*&quot;]
&quot;_source&quot;: {
    &quot;includes&quot;: [&quot;AvgTicketPrice&quot;,&quot;DistanceMiles&quot;,&quot;Dest*&quot;,&quot;origin*&quot;],
    &quot;excludes&quot;: [&quot;DestRegion&quot;]
}
</code></pre>
<ul>
<li>
<p>query-这是搜索请求中最重要的组成部分，它配置了基于评分返回的最佳文档，也包括了你不希望返回哪些文档。</p>
</li>
<li>
<p>size-代表了返回文档的数量</p>
</li>
<li>
<p>from 和 size， 用于指定结果的开始点，以及每“页&quot;结果的数量。<br />
from 与 size 的和不能超过 index. max_result_window 这个索引配 置项设置的值。默认情况下这个配置项的值为 10000,所以如果要查询 10000 条以 后的文档，就必须要增加这个配置值。</p>
</li>
<li>
<p>_source 指定 _source 字段如何返回。默认是返回完整的 _source 字段。 通过配置 _source, 将过滤返回的字段。如果索引的文档很大，而且无须结果中的 全部内容，就使用这个功能。请注意，如果想使用它，就不能在索引映射中关闭 _ source 字段。</p>
</li>
<li>
<p>sort 默认的排序是基于文档的得分 _score 的。如果并不关心得分，或者期望许多文档的得分相同，添加额外的 sort 将帮助你控制哪些文档被返回</p>
</li>
</ul>
<p>response:</p>
<pre><code class="language-json">{
  &quot;took&quot; : 6,
  &quot;timed_out&quot; : false,
  &quot;_shards&quot; : {
    &quot;total&quot; : 1,
    &quot;successful&quot; : 1,
    &quot;skipped&quot; : 0,
    &quot;failed&quot; : 0
  },
  &quot;hits&quot; : {
    &quot;total&quot; : {
      &quot;value&quot; : 10000,
      &quot;relation&quot; : &quot;gte&quot;
    },
    &quot;max_score&quot; : 1.0,
    &quot;hits&quot; : [
      {
        &quot;_index&quot; : &quot;kibana_sample_data_flights&quot;,
        &quot;_type&quot; : &quot;_doc&quot;,
        &quot;_id&quot; : &quot;kMrI0n0BBQs7jchubFRe&quot;,
        &quot;_score&quot; : 1.0,
        &quot;_source&quot; : {
          &quot;FlightNum&quot; : &quot;9HY9SWR&quot;,
          &quot;DestCountry&quot; : &quot;AU&quot;,
          &quot;OriginWeather&quot; : &quot;Sunny&quot;,
          &quot;OriginCityName&quot; : &quot;Frankfurt am Main&quot;,
          &quot;AvgTicketPrice&quot; : 841.2656419677076,
          &quot;DistanceMiles&quot; : 10247.856675613455,
          &quot;FlightDelay&quot; : false,
          &quot;DestWeather&quot; : &quot;Rain&quot;,
          &quot;Dest&quot; : &quot;Sydney Kingsford Smith International Airport&quot;,
          &quot;FlightDelayType&quot; : &quot;No Delay&quot;,
          &quot;OriginCountry&quot; : &quot;DE&quot;,
          &quot;dayOfWeek&quot; : 0,
          &quot;DistanceKilometers&quot; : 16492.32665375846,
          &quot;timestamp&quot; : &quot;2021-12-13T00:00:00&quot;,
          &quot;DestLocation&quot; : {
            &quot;lat&quot; : &quot;-33.94609833&quot;,
            &quot;lon&quot; : &quot;151.177002&quot;
          },
          &quot;DestAirportID&quot; : &quot;SYD&quot;,
          &quot;Carrier&quot; : &quot;Kibana Airlines&quot;,
          &quot;Cancelled&quot; : false,
          &quot;FlightTimeMin&quot; : 1030.7704158599038,
          &quot;Origin&quot; : &quot;Frankfurt am Main Airport&quot;,
          &quot;OriginLocation&quot; : {
            &quot;lat&quot; : &quot;50.033333&quot;,
            &quot;lon&quot; : &quot;8.570556&quot;
          },
          &quot;DestRegion&quot; : &quot;SE-BD&quot;,
          &quot;OriginAirportID&quot; : &quot;FRA&quot;,
          &quot;OriginRegion&quot; : &quot;DE-HE&quot;,
          &quot;DestCityName&quot; : &quot;Sydney&quot;,
          &quot;FlightTimeHour&quot; : 17.179506930998397,
          &quot;FlightDelayMin&quot; : 0
        }
      }
    ]
  }
}

</code></pre>
<ul>
<li>took - Elasticsearch 执行搜索的时间（毫秒）</li>
<li>time_out - 告诉我们搜索是否超时</li>
<li>_shards - 告诉我们多少个分片被搜索了，以及统计了成功/失败的搜索分片</li>
<li>hits - 搜索结果</li>
<li>hits.total - 搜索结果</li>
<li>hits.hits - 实际的搜索结果数组（默认为前 10 的文档）</li>
<li>sort - 结果的排序 key（键）（没有则按 score 排序）</li>
<li>score 和max_score -现在暂时忽略这些字段</li>
</ul>
<hr />
<h2><a id="2%E3%80%81%E5%9F%BA%E4%BA%8E%E8%AF%8D%E9%A1%B9%E7%9A%84%E6%90%9C%E7%B4%A2" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>2、基于词项的搜索</h2>
<h3><a id="2-1%E3%80%81term%E6%9F%A5%E8%AF%A2" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>2.1、term 查询</h3>
<p>对词项做精确匹配,对于字符串而言，字符串的精确匹配是指字符的大小写，字符的数量和位置都是相同的,词条(term)查询使用字符的完全匹配方式进行文本搜索，词条查询不会分析(analyze)查询字符串，给定的字段必须完全匹配词条查询中指定的 字符串。<br />
可以及将 term 理解为 关系型数据库 SQL语句中 where 条件的等于号</p>
<pre><code class="language-bash">GET kibana_sample_data_flights/_search
{
  &quot;query&quot;: {
    &quot;term&quot;: {
      &quot;OriginCityName&quot;: &quot;Manchester&quot;
    }
  }
}
</code></pre>
<h3><a id="2-2%E3%80%81terms%E6%9F%A5%E8%AF%A2" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>2.2、terms 查询</h3>
<p>可以把 terms 查询理解为 关系型数据库 SQL 语句中 where 条件的 in 操作符</p>
<pre><code class="language-bash">GET kibana_sample_data_flights/_search
{
  &quot;query&quot;: {
    &quot;terms&quot;: {
      &quot;OriginCityName&quot;: [&quot;Manchester&quot;,&quot;Olenegorsk&quot;]
    }
  }
}
</code></pre>
<p>Elasticsearch 在 terms 查询中还支持跨索引查询，这类似于关系型数据库中 的一对多或多对多关系。<a href="https://www.infoq.cn/article/ekygoihkqifj4kdgso6a">跨索引查询</a></p>
<pre><code class="language-bash">POST /articles/_search
{
  &quot;query&quot;: {
    &quot;terms&quot;: {
      &quot;_id&quot;: {
        &quot;index&quot;: &quot;users&quot;,
        &quot;id&quot;: 1,
        &quot;path&quot;: &quot;articles&quot;
      }
    }
  }
}
</code></pre>
<h3><a id="2-3%E3%80%81range%E6%9F%A5%E8%AF%A2%E5%92%8C-exists%E6%9F%A5%E8%AF%A2" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>2.3、range 查询和 exists 查询</h3>
<p>查询介于一定范围之内的值，适用于数字、日期甚至是字符串。</p>
<p>可以查询出延误时间在 100~200 之间的航班:</p>
<pre><code class="language-bash">GET kibana_sample_data_flights/_search
{
  &quot;query&quot;: {
    &quot;range&quot;: {
      &quot;FlightDelayMin&quot;: {
        &quot;gte&quot;: 100,
        &quot;lte&quot;: 200
      }
    }
  }
}
</code></pre>
<p>gte:大于等于 (greater than and equal)<br />
gt:大于 (greater than)<br />
lte:小于等于 (less than and equal)<br />
lt:大于 (less than )<br />
boost:相关性评分(后面的章节会讲到相关性评分)<br />
exists 查询检索字段值不为空的的文档，无论其值是多少，在查询中通过 field 字段设置检查非空的字段名称，只能有一个。</p>
<h3><a id="2-4%E3%80%81prefix%E6%9F%A5%E8%AF%A2" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>2.4、prefix 查询</h3>
<p>prefix 查询允许你根据给定的前缀来搜索词条，这里前缀在同样搜索之前是<br />
没有经过分析的。</p>
<p>找到航班目的国家中所有以 C 开头的文档:</p>
<pre><code class="language-bash">GET kibana_sample_data_flights/_search
{
  &quot;query&quot;: {
    &quot;prefix&quot;: {
      &quot;DestCountry&quot;: &quot;C&quot;
    }
  }
}
</code></pre>
<h3><a id="2-5%E3%80%81wildcard%E6%9F%A5%E8%AF%A2%E5%92%8C-regexp%E6%9F%A5%E8%AF%A2" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>2.5、wildcard 查询和 regexp 查询</h3>
<p>--</p>
<h2><a id="3%E3%80%81%E5%9F%BA%E4%BA%8E%E5%85%A8%E6%96%87%E7%B4%A2%E5%BC%95%E7%9A%84%E6%A3%80%E7%B4%A2" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>3、基于全文索引的检索</h2>
<p>开始基于全文索引的检索学习之前先了解一下 ES 的文本分析：<br />
<a href="16399269127746.html">ElasticSearch 文本分析</a></p>
<h3><a id="3-1%E3%80%81match%E6%9F%A5%E8%AF%A2" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>3.1、match 查询</h3>
<pre><code class="language-bash">PUT /crazy
{
  &quot;settings&quot;: {
    &quot;number_of_shards&quot;: 3,
    &quot;number_of_replicas&quot;: 2,
    &quot;analysis&quot;: {
      &quot;analyzer&quot;: {
        &quot;default&quot;: {
          &quot;type&quot;: &quot;ik_smart&quot;
        },
        &quot;default_seach&quot;: {
          &quot;type&quot;: &quot;whitespace&quot;
        }
      }
    }
  },
  &quot;mappings&quot;: {
    &quot;properties&quot;: {
      &quot;id&quot;: {
        &quot;type&quot;: &quot;integer&quot;
      },
      &quot;name&quot;: {
        &quot;type&quot;: &quot;keyword&quot;
      },
      &quot;age&quot;: {
        &quot;type&quot;: &quot;long&quot;
      },
      &quot;birth&quot;: {
        &quot;type&quot;: &quot;date&quot;
      },
      &quot;desc&quot;: {
        &quot;type&quot;: &quot;text&quot;
      },
      &quot;tag&quot;: {
        &quot;type&quot;: &quot;text&quot;
      }
    }
  }
}

POST /crazy/_doc
{
  &quot;name&quot;: &quot;疯子&quot;,
  &quot;age&quot;: 23,
  &quot;birth&quot;: &quot;1997-06-06&quot;,
  &quot;desc&quot;: &quot;疯子学elk来了&quot;,
  &quot;tag&quot;: [
    &quot;JAVA&quot;,
    &quot;帅哥&quot;,
    &quot;HTML&quot;,
    &quot;暖男&quot;,
    &quot;看书&quot;
  ]
}
POST /crazy/_doc
{
  &quot;name&quot;: &quot;小傻子&quot;,
  &quot;age&quot;: 20,
  &quot;birth&quot;: &quot;2000-12-20&quot;,
  &quot;desc&quot;: &quot;傻子不爱吃苹果&quot;,
  &quot;tag&quot;: [
    &quot;游戏&quot;,
    &quot;直播&quot;,
    &quot;直男&quot;,
    &quot;渣男&quot;,
    &quot;旅游&quot;
  ]
}
POST /crazy/_doc
{
  &quot;name&quot;: &quot;张张三&quot;,
  &quot;age&quot;: 5,
  &quot;birth&quot;: &quot;2015-02-20&quot;,
  &quot;desc&quot;: &quot;张三5岁了，他也不爱吃苹果&quot;,
  &quot;tag&quot;: [
    &quot;萌宝&quot;,
    &quot;游戏&quot;,
    &quot;小暖男&quot;,
    &quot;睡觉&quot;,
    &quot;玩具&quot;
  ]
}
POST /crazy/_doc
{
  &quot;name&quot;: &quot;李四&quot;,
  &quot;age&quot;: 50,
  &quot;birth&quot;: &quot;1970-04-25&quot;,
  &quot;desc&quot;: &quot;李四50岁了，她爱吃香蕉，是个老太太&quot;,
  &quot;tag&quot;: [
    &quot;老人&quot;,
    &quot;听戏&quot;,
    &quot;散步&quot;,
    &quot;睡觉&quot;,
    &quot;老太婆&quot;
  ]
}
POST /crazy/_doc
{
  &quot;name&quot;: &quot;王五五&quot;,
  &quot;age&quot;: 30,
  &quot;birth&quot;: &quot;1990-09-25&quot;,
  &quot;desc&quot;: &quot;王五爱吃苹果，还学java，也爱吃香蕉&quot;,
  &quot;tag&quot;: [
    &quot;直男&quot;,
    &quot;技术宅&quot;,
    &quot;睡觉&quot;,
    &quot;听音乐&quot;,
    &quot;大佬&quot;
  ]
}
</code></pre>
<pre><code class="language-bash">GET /crazy/_search
{
  &quot;query&quot;: {
    &quot;match&quot;: {
      &quot;desc&quot;: {
        &quot;query&quot;: &quot;苹果 香蕉&quot;
      }
    }
  }
}
</code></pre>
<p>response:</p>
<pre><code class="language-json">{
  &quot;took&quot; : 496,
  &quot;timed_out&quot; : false,
  &quot;_shards&quot; : {
    &quot;total&quot; : 3,
    &quot;successful&quot; : 3,
    &quot;skipped&quot; : 0,
    &quot;failed&quot; : 0
  },
  &quot;hits&quot; : {
    &quot;total&quot; : {
      &quot;value&quot; : 4,
      &quot;relation&quot; : &quot;eq&quot;
    },
    &quot;max_score&quot; : 1.6285594,
    &quot;hits&quot; : [
      {
        &quot;_index&quot; : &quot;crazy&quot;,
        &quot;_type&quot; : &quot;_doc&quot;,
        &quot;_id&quot; : &quot;k8oS3n0BBQs7jchulL4s&quot;,
        &quot;_score&quot; : 1.6285594,
        &quot;_source&quot; : {
          &quot;name&quot; : &quot;小傻子&quot;,
          &quot;age&quot; : 20,
          &quot;birth&quot; : &quot;2000-12-20&quot;,
          &quot;desc&quot; : &quot;傻子不爱吃苹果&quot;,
          &quot;tag&quot; : [
            &quot;游戏&quot;,
            &quot;直播&quot;,
            &quot;直男&quot;,
            &quot;渣男&quot;,
            &quot;旅游&quot;
          ]
        }
      },
      {
        &quot;_index&quot; : &quot;crazy&quot;,
        &quot;_type&quot; : &quot;_doc&quot;,
        &quot;_id&quot; : &quot;lMoS3n0BBQs7jchunb7W&quot;,
        &quot;_score&quot; : 1.2199391,
        &quot;_source&quot; : {
          &quot;name&quot; : &quot;张张三&quot;,
          &quot;age&quot; : 5,
          &quot;birth&quot; : &quot;2015-02-20&quot;,
          &quot;desc&quot; : &quot;张三5岁了，他也不爱吃苹果&quot;,
          &quot;tag&quot; : [
            &quot;萌宝&quot;,
            &quot;游戏&quot;,
            &quot;小暖男&quot;,
            &quot;睡觉&quot;,
            &quot;玩具&quot;
          ]
        }
      },
      {
        &quot;_index&quot; : &quot;crazy&quot;,
        &quot;_type&quot; : &quot;_doc&quot;,
        &quot;_id&quot; : &quot;lcoS3n0BBQs7jchupb7G&quot;,
        &quot;_score&quot; : 1.2067741,
        &quot;_source&quot; : {
          &quot;name&quot; : &quot;李四&quot;,
          &quot;age&quot; : 50,
          &quot;birth&quot; : &quot;1970-04-25&quot;,
          &quot;desc&quot; : &quot;李四50岁了，她爱吃香蕉，是个老太太&quot;,
          &quot;tag&quot; : [
            &quot;老人&quot;,
            &quot;听戏&quot;,
            &quot;散步&quot;,
            &quot;睡觉&quot;,
            &quot;老太婆&quot;
          ]
        }
      },
      {
        &quot;_index&quot; : &quot;crazy&quot;,
        &quot;_type&quot; : &quot;_doc&quot;,
        &quot;_id&quot; : &quot;lsoS3n0BBQs7jchurb4k&quot;,
        &quot;_score&quot; : 1.1507283,
        &quot;_source&quot; : {
          &quot;name&quot; : &quot;王五五&quot;,
          &quot;age&quot; : 30,
          &quot;birth&quot; : &quot;1990-09-25&quot;,
          &quot;desc&quot; : &quot;王五爱吃苹果，还学java，也爱吃香蕉&quot;,
          &quot;tag&quot; : [
            &quot;直男&quot;,
            &quot;技术宅&quot;,
            &quot;睡觉&quot;,
            &quot;听音乐&quot;,
            &quot;大佬&quot;
          ]
        }
      }
    ]
  }
}

</code></pre>
<p>看上面的query查询，查询字符串是“苹果 香蕉”，被分析器分词之后，产生过两个词 苹果、香蕉，然后根据分析的结果构造一个布尔查询，只要desc里面包含任意一个关键字 苹果或香蕉，文档就会被返回</p>
<p>匹配查询的行为受到两个参数的控制:</p>
<ul>
<li>operator:表示单个字段如何匹配查询条件的分词,默认值是 or，可设置为 and</li>
<li>minimum_should_match:表示字段匹配的条件数量,默认是1，等于 2 的时候，需要同时满足苹果、香蕉的文档才会返回。</li>
</ul>
<h3><a id="3-2%E3%80%81match-phrase%E6%9F%A5%E8%AF%A2" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>3.2、match_phrase 查询</h3>
<p>当希望寻找邻近的单词时，match_phrase 查询可以帮你达到目的。<br />
例如下面这个，desc中 经过 _analyzer接口分析后王五和香蕉之间相差7各个分词，其中 slop 参数缺省为 0，它告诉 match_phrase 查询词项能够最远相隔多远时仍然将文档视为匹配。</p>
<pre><code class="language-bash">GET /crazy/_search
{
  &quot;query&quot;: {
    &quot;match_phrase&quot;: {
      &quot;desc&quot;: {
        &quot;query&quot;: &quot;王五 香蕉&quot;,
        &quot;slop&quot;: 7
      }
    }
  }
}
</code></pre>
<p>response :</p>
<pre><code class="language-json">{
  &quot;took&quot; : 0,
  &quot;timed_out&quot; : false,
  &quot;_shards&quot; : {
    &quot;total&quot; : 3,
    &quot;successful&quot; : 3,
    &quot;skipped&quot; : 0,
    &quot;failed&quot; : 0
  },
  &quot;hits&quot; : {
    &quot;total&quot; : {
      &quot;value&quot; : 1,
      &quot;relation&quot; : &quot;eq&quot;
    },
    &quot;max_score&quot; : 0.33427334,
    &quot;hits&quot; : [
      {
        &quot;_index&quot; : &quot;crazy&quot;,
        &quot;_type&quot; : &quot;_doc&quot;,
        &quot;_id&quot; : &quot;oMom3n0BBQs7jchu8b7d&quot;,
        &quot;_score&quot; : 0.33427334,
        &quot;_source&quot; : {
          &quot;name&quot; : &quot;王五五&quot;,
          &quot;age&quot; : 30,
          &quot;birth&quot; : &quot;1990-09-25&quot;,
          &quot;desc&quot; : &quot;王五爱吃苹果，还学java，也爱吃香蕉&quot;,
          &quot;tag&quot; : [
            &quot;直男&quot;,
            &quot;技术宅&quot;,
            &quot;睡觉&quot;,
            &quot;听音乐&quot;,
            &quot;大佬&quot;
          ]
        }
      }
    ]
  }
}
</code></pre>
<h3><a id="3-3%E3%80%81multi-match%E6%9F%A5%E8%AF%A2" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>3.3、multi_match 查询</h3>
<p>多个字段上执行匹配相同的查询，叫做&quot;multi_match&quot;查询</p>
<pre><code class="language-bash">POST /crazy/_search
{
  &quot;query&quot;: {
    &quot;multi_match&quot;: {
      &quot;query&quot;: &quot;疯子&quot;,
      &quot;fields&quot;: [
        &quot;desc&quot;,
        &quot;name&quot;
      ]
    }
  }
}
</code></pre>
<p>response :</p>
<pre><code class="language-json">{
  &quot;took&quot; : 2,
  &quot;timed_out&quot; : false,
  &quot;_shards&quot; : {
    &quot;total&quot; : 3,
    &quot;successful&quot; : 3,
    &quot;skipped&quot; : 0,
    &quot;failed&quot; : 0
  },
  &quot;hits&quot; : {
    &quot;total&quot; : {
      &quot;value&quot; : 2,
      &quot;relation&quot; : &quot;eq&quot;
    },
    &quot;max_score&quot; : 0.8514803,
    &quot;hits&quot; : [
      {
        &quot;_index&quot; : &quot;crazy&quot;,
        &quot;_type&quot; : &quot;_doc&quot;,
        &quot;_id&quot; : &quot;o8q64n0BBQs7jchuxL6Z&quot;,
        &quot;_score&quot; : 0.8514803,
        &quot;_source&quot; : {
          &quot;name&quot; : &quot;疯子&quot;,
          &quot;age&quot; : 23,
          &quot;birth&quot; : &quot;1997-06-06&quot;,
          &quot;desc&quot; : &quot;疯子不疯魔&quot;,
          &quot;tag&quot; : [
            &quot;JAVA&quot;,
            &quot;帅哥&quot;,
            &quot;HTML&quot;,
            &quot;暖男&quot;,
            &quot;看书&quot;
          ]
        }
      },
      {
        &quot;_index&quot; : &quot;crazy&quot;,
        &quot;_type&quot; : &quot;_doc&quot;,
        &quot;_id&quot; : &quot;nMom3n0BBQs7jchu0b6J&quot;,
        &quot;_score&quot; : 0.8405091,
        &quot;_source&quot; : {
          &quot;name&quot; : &quot;疯子&quot;,
          &quot;age&quot; : 23,
          &quot;birth&quot; : &quot;1997-06-06&quot;,
          &quot;desc&quot; : &quot;疯子学elk来了&quot;,
          &quot;tag&quot; : [
            &quot;JAVA&quot;,
            &quot;帅哥&quot;,
            &quot;HTML&quot;,
            &quot;暖男&quot;,
            &quot;看书&quot;
          ]
        }
      }
    ]
  }
}
</code></pre>
<p>提升字段权重，在查询字段后使用 ^ 符号可以提高字段的权重，增加字段的分数 _score, 提升权限之后，另一个的 _score 会变成提升过权重的分数值。</p>
<pre><code class="language-bash">POST /crazy/_search
{
  &quot;query&quot;: {
    &quot;multi_match&quot;: {
      &quot;query&quot;: &quot;疯子&quot;,
      &quot;fields&quot;: [
        &quot;desc&quot;,
        &quot;name^3&quot;
      ]
    }
  }
}

</code></pre>
<p>response：</p>
<pre><code class="language-json">{
  &quot;took&quot; : 0,
  &quot;timed_out&quot; : false,
  &quot;_shards&quot; : {
    &quot;total&quot; : 3,
    &quot;successful&quot; : 3,
    &quot;skipped&quot; : 0,
    &quot;failed&quot; : 0
  },
  &quot;hits&quot; : {
    &quot;total&quot; : {
      &quot;value&quot; : 2,
      &quot;relation&quot; : &quot;eq&quot;
    },
    &quot;max_score&quot; : 2.0794415,
    &quot;hits&quot; : [
      {
        &quot;_index&quot; : &quot;crazy&quot;,
        &quot;_type&quot; : &quot;_doc&quot;,
        &quot;_id&quot; : &quot;nMom3n0BBQs7jchu0b6J&quot;,
        &quot;_score&quot; : 2.0794415,
        &quot;_source&quot; : {
          &quot;name&quot; : &quot;疯子&quot;,
          &quot;age&quot; : 23,
          &quot;birth&quot; : &quot;1997-06-06&quot;,
          &quot;desc&quot; : &quot;疯子学elk来了&quot;,
          &quot;tag&quot; : [
            &quot;JAVA&quot;,
            &quot;帅哥&quot;,
            &quot;HTML&quot;,
            &quot;暖男&quot;,
            &quot;看书&quot;
          ]
        }
      },
      {
        &quot;_index&quot; : &quot;crazy&quot;,
        &quot;_type&quot; : &quot;_doc&quot;,
        &quot;_id&quot; : &quot;o8q64n0BBQs7jchuxL6Z&quot;,
        &quot;_score&quot; : 2.0794415,
        &quot;_source&quot; : {
          &quot;name&quot; : &quot;疯子&quot;,
          &quot;age&quot; : 23,
          &quot;birth&quot; : &quot;1997-06-06&quot;,
          &quot;desc&quot; : &quot;疯子不疯魔&quot;,
          &quot;tag&quot; : [
            &quot;JAVA&quot;,
            &quot;帅哥&quot;,
            &quot;HTML&quot;,
            &quot;暖男&quot;,
            &quot;看书&quot;
          ]
        }
      }
    ]
  }
}

</code></pre>
<p>如果在 multimatch 查询中不指定 fields 参数，默认会将文档中的所有字段都匹配一遍。但不建议这么做，可能会出现性能问题，也没有什么意义。</p>
<h3><a id="3-4%E3%80%81match-phrase-prefix%E6%9F%A5%E8%AF%A2" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>3.4、match_phrase_prefix 查询</h3>
<p>被称为基于前缀的短语匹配</p>
<hr />
<h2><a id="4%E3%80%81%E6%A8%A1%E7%B3%8A%E6%9F%A5%E8%AF%A2%E3%80%81%E7%BA%A0%E9%94%99%E4%B8%8E%E6%8F%90%E7%A4%BA%E5%99%A8" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>4、模糊查询、纠错与提示器</h2>
<h3><a id="4-1%E7%BC%96%E8%BE%91%E8%B7%9D%E7%A6%BB%E7%AE%97%E6%B3%95" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>4.1编辑距离算法</h3>
<p>在ES基于全文的查询中，除了与短语相关的查询之外，其余查询都包含一个名为fuzziness 的参数用于支持模糊查询。ES 支持的模糊查询与 SQL语言中的模糊查询不一样，SQL 的模糊查询使用 “%keyword%” 的形式，效果是查询字段值中包含 keyword的记录。</p>
<p>ES支持的模糊查询比这个要强，他可以根据一个拼写错误的词项匹配正确的结果，例如根据 firefix 查询到 firefox。在自然语言处理领域，两个词项之间的差异通常称为距离或编辑距离。距离的大小用于说明两个词项之间差异的大小。</p>
<p>编辑距离的算法有多种，在 ES中主要使用 Levenshtein 和 NGram 两种。其他的与此相关的算法也都是在这两种算法的基础上进行的改造，基本思想都是一致的。所以掌握这两种算法的核心思想是很有必要的。</p>
<h4><a id="4-1-1%E3%80%81levenshtein%E7%AE%97%E6%B3%95" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>4.1.1、Levenshtein算法</h4>
<p>Levenshtein 算法是前苏联数学家 Vladimir Levenshein 在 1965 年开发的一套算法，这个算法可以对两个字符间的差异成都做量化。量化结果是一个正整数，反映的是一个字符变成另一个字符最少要多少次处理。由于 Levenshtein 算法是最为普遍接收的编辑距离算法，所以在很多文献中如果没有特殊说明编辑距离算法就是指 Levenshtein 算法。</p>
<p>在 Levenshtein 算法中定义了三种字符操作（替换、插入、删除），后来又补充了一个换位操作。在转换过程中，每执行一次操作编辑距离就 +1，编辑距离越大说明两个字符串之间的差距越大。</p>
<p>firefix 到 firefox 需要将 i 替换成 o，所以编辑距离为 1，</p>
<p>fax 到 fair需要将x替换成i并在结尾处插入r，编辑距离为 2</p>
<p>编辑距离同样为 2 的情况下，fax 到 fair 与 从 elastesearsh 到 elasticsearch ,后面elastesearsh 是由拼写错误引起的可能性就更大。所以编辑距离相同的情况下，单词越长错误与正确就越接近。</p>
<h4><a id="4-1-2%E3%80%81ngram%E7%AE%97%E6%B3%95" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>4.1.2、NGram 算法</h4>
<p>NGram 一般是指 N 个连续的字符，具体的字符个数被定义为 NGram 的 size。</p>
<p>size 为 1 的 NGram 称为 Unigram, size 为 2 时称为 Bigram,而 size 为 3 时则称为 Trigram。如果NGram 处理的单元不是字符而是单词，一般被称为 Shingle。使用NGram计算编辑距离的基本思想是让字符串分解为 NGram,然后比较分解后共有的 NGram 数量。</p>
<p>有两个字符串 a、b，则 NGram 距离的具体运算公式为：</p>
<blockquote>
<p>ngram(a) +ngram(b) - 2* (ngram(a)\(∩\)ngram(b))</p>
</blockquote>
<p>公式中 ngram(a)和ngram(b)代表 a、b两个字符串NGram的数量，ngram(a)\(∩\)ngram(b) 则是两者共有的 NGram 的数量。</p>
<p>例如按Bigram处理 firefix 和firefox两个单词，分别为 “fi,ir,re,ef,fi,ix” 和 “fi,ir,re,ef,fo,ox”。那么两个字符串的 Bigram 个数为 6，共有的Bigram 为4，则最终 Ngram 距离为 6+6-2*4=4</p>
<p>在应用上，Levenshtein 算法更多的应用于单个词项的模糊查询上，而NGram则应用于多个词项匹配中。ES同时应用了两种算法。</p>
<h3><a id="4-1-2%E3%80%81%E6%A8%A1%E7%B3%8A%E6%9F%A5%E8%AF%A2" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>4.1.2、模糊查询</h3>
<p>返回包含与搜索字词相似的字词文档;为了找到相似的术语，fuzzy 查询将在指定的编辑距离内创建一组搜索词的所有可能的变体或扩展。查询然后返回每个扩展的完全匹配。</p>
<pre><code class="language-bash">POST /kibana_sample_data_logs/_search
{
  &quot;query&quot;: {
    &quot;fuzzy&quot;: {
      &quot;agent&quot;: {
        &quot;value&quot;: &quot;mozilla&quot;,
        &quot;fuzziness&quot;: 1
      }
    }
  }
}
</code></pre>
<p>response:</p>
<pre><code class="language-json">{
    &quot;took&quot;:0,
    &quot;timed_out&quot;:false,
    &quot;_shards&quot;:{
        &quot;total&quot;:1,
        &quot;successful&quot;:1,
        &quot;skipped&quot;:0,
        &quot;failed&quot;:0
    },
    &quot;hits&quot;:{
        &quot;total&quot;:{
            &quot;value&quot;:10000,
            &quot;relation&quot;:&quot;gte&quot;
        },
        &quot;max_score&quot;:0.000037115216,
        &quot;hits&quot;:[
            {
                &quot;_index&quot;:&quot;kibana_sample_data_logs&quot;,
                &quot;_type&quot;:&quot;_doc&quot;,
                &quot;_id&quot;:&quot;k8rI0n0BBQs7jchue4fn&quot;,
                &quot;_score&quot;:0.000037115216,
                &quot;_source&quot;:{
                    &quot;agent&quot;:&quot;Mozilla/5.0 (X11; Linux x86_64; rv:6.0a1) Gecko/20110421 Firefox/6.0a1&quot;,
                    &quot;bytes&quot;:6219,
                    &quot;clientip&quot;:&quot;223.87.60.27&quot;,
                    &quot;extension&quot;:&quot;deb&quot;,
                    &quot;geo&quot;:{
                        &quot;srcdest&quot;:&quot;IN:US&quot;,
                        &quot;src&quot;:&quot;IN&quot;,
                        &quot;dest&quot;:&quot;US&quot;,
                        &quot;coordinates&quot;:{
                            &quot;lat&quot;:39.41042861,
                            &quot;lon&quot;:-88.8454325
                        }
                    },
                    &quot;host&quot;:&quot;artifacts.elastic.co&quot;,
                    &quot;index&quot;:&quot;kibana_sample_data_logs&quot;,
                    &quot;ip&quot;:&quot;223.87.60.27&quot;,
                    &quot;machine&quot;:{
                        &quot;ram&quot;:8589934592,
                        &quot;os&quot;:&quot;win 8&quot;
                    },
                    &quot;memory&quot;:null,
                    &quot;message&quot;:&quot;223.87.60.27 - - [2018-07-22T00:39:02.912Z] \&quot;GET /elasticsearch/elasticsearch-6.3.2.deb_1 HTTP/1.1\&quot; 200 6219 \&quot;-\&quot; \&quot;Mozilla/5.0 (X11; Linux x86_64; rv:6.0a1) Gecko/20110421 Firefox/6.0a1\&quot;&quot;,
                    &quot;phpmemory&quot;:null,
                    &quot;referer&quot;:&quot;http://twitter.com/success/wendy-lawrence&quot;,
                    &quot;request&quot;:&quot;/elasticsearch/elasticsearch-6.3.2.deb&quot;,
                    &quot;response&quot;:200,
                    &quot;tags&quot;:[
                        &quot;success&quot;,
                        &quot;info&quot;
                    ],
                    &quot;timestamp&quot;:&quot;2021-12-12T00:39:02.912Z&quot;,
                    &quot;url&quot;:&quot;https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-6.3.2.deb_1&quot;,
                    &quot;utc_time&quot;:&quot;2021-12-12T00:39:02.912Z&quot;,
                    &quot;event&quot;:{
                        &quot;dataset&quot;:&quot;sample_web_logs&quot;
                    }
                }
            },
            .......
        ]
    }
}
</code></pre>
<ul>
<li>value，必填项，希望在 field 中找到的术语</li>
<li>fuzziness，选填项，匹配允许的最大编辑距离;可以被设置为“0”， “1”， “2”或“auto”。“auto”是推荐的选项，它会根据查询词的长度定义距离。</li>
<li>max_expansions，选填项，创建的最大变体项，默认为 50。应该避免使用较 大的值，尤其是当 prefix_length 参数值为 0 时，如果过多，会影响查找性能。</li>
<li>prefix_length，选填项，创建扩展时保留不变的开始字符数。默认为 0</li>
<li>transpositions，选填项，指示编辑是否包括两个相邻字符串的转置(ab→ba)。<br />
默认为 true。</li>
</ul>
<h3><a id="%E7%BA%A0%E9%94%99%E4%B8%8E%E6%8F%90%E7%A4%BA%E5%99%A8" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>纠错与提示器</h3>
<hr />
<h2><a id="%E7%BB%84%E5%90%88%E6%9F%A5%E8%AF%A2" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>组合查询</h2>
<h3><a id="bool%E7%BB%84%E5%90%88%E6%9F%A5%E8%AF%A2" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>bool 组合查询</h3>
<p>bool 组合查询将一组布尔类型子句组合起来， 形成个大的布尔条件。在它的子句中，一些子句的确会决定文档是否会作为结果返回，而另一些子句则不决定文档是否可以作为结果，但会影响到结果的相关度。</p>
<p>bool组合查询可用的布尔类型子句包括 must、filter、should 和 must_not 四 种，它们接收参数值的类型为数组。</p>
<ul>
<li>must 查询结果中必须要包含的内容，影响相关度</li>
<li>filter 查询结果中必须要包含的内容，不会影响相关度</li>
<li>should 查询结果非必须包含项，包含了会提高分数，影响相关度</li>
<li>must_not 查询结果中不能包含的内容，不会影响相关度</li>
</ul>
<p>filter 和 must _not 单纯只用于过滤文档，它们对文档相关度没有任何影响，也就是这两种子句对查询结果的 _score 没有影响。</p>
<p>should 对执行结果影响相关度，但在是否过滤结果上则取决于上下文。当 should 子句与 must 子句或 filter 子句同时出现在子句中时，should 子句将不会过滤结果。也就是说，在这种情况下，即使 should 子句不满足，结果也会返回。</p>
<pre><code class="language-bash">POST /kibana_sample_data_logs/_search
{
  &quot;query&quot;: {
    &quot;bool&quot;: {
      &quot;must&quot;: [
        {
          &quot;match&quot;: {
            &quot;message&quot;: {
              &quot;query&quot;: &quot;firefox&quot;
            }
          }
        }
      ],
      &quot;should&quot;: [
        {
          &quot;term&quot;: {
            &quot;geo.src&quot;: {
              &quot;value&quot;: &quot;CN&quot;
            }
          }
        },
        {
          &quot;term&quot;: {
            &quot;geo.dest&quot;: {
              &quot;value&quot;: &quot;CN&quot;
            }
          }
        }
      ]
    }
  },
  &quot;sort&quot;: [
    {
      &quot;_score&quot;: {
        &quot;order&quot;: &quot;asc&quot;
      }
    }
  ]
}

</code></pre>
<p>response:</p>
<pre><code class="language-json">{
  &quot;took&quot; : 13,
  &quot;timed_out&quot; : false,
  &quot;_shards&quot; : {
    &quot;total&quot; : 1,
    &quot;successful&quot; : 1,
    &quot;skipped&quot; : 0,
    &quot;failed&quot; : 0
  },
  &quot;hits&quot; : {
    &quot;total&quot; : {
      &quot;value&quot; : 5362,
      &quot;relation&quot; : &quot;eq&quot;
    },
    &quot;max_score&quot; : null,
    &quot;hits&quot; : [
      {
        &quot;_index&quot; : &quot;kibana_sample_data_logs&quot;,
        &quot;_type&quot; : &quot;_doc&quot;,
        &quot;_id&quot; : &quot;EMrI0n0BBQs7jchue4jn&quot;,
        &quot;_score&quot; : 0.923208,
        &quot;_source&quot; : {
          &quot;agent&quot; : &quot;Mozilla/5.0 (X11; Linux x86_64; rv:6.0a1) Gecko/20110421 Firefox/6.0a1&quot;,
          &quot;bytes&quot; : 7309,
          &quot;clientip&quot; : &quot;116.183.201.95&quot;,
          &quot;extension&quot; : &quot;zip&quot;,
          &quot;geo&quot; : {
            &quot;srcdest&quot; : &quot;ID:ID&quot;,
            &quot;src&quot; : &quot;ID&quot;,
            &quot;dest&quot; : &quot;ID&quot;,
            &quot;coordinates&quot; : {
              &quot;lat&quot; : 34.91572222,
              &quot;lon&quot; : -81.9565
            }
          },
          &quot;host&quot; : &quot;artifacts.elastic.co&quot;,
          &quot;index&quot; : &quot;kibana_sample_data_logs&quot;,
          &quot;ip&quot; : &quot;116.183.201.95&quot;,
          &quot;machine&quot; : {
            &quot;ram&quot; : 13958643712,
            &quot;os&quot; : &quot;win xp&quot;
          },
          &quot;memory&quot; : null,
          &quot;message&quot; : &quot;116.183.201.95 - - [2018-07-22T12:21:13.414Z] \&quot;GET /apm-server/apm-server-6.3.2-windows-x86.zip HTTP/1.1\&quot; 200 7309 \&quot;-\&quot; \&quot;Mozilla/5.0 (X11; Linux x86_64; rv:6.0a1) Gecko/20110421 Firefox/6.0a1\&quot;&quot;,
          &quot;phpmemory&quot; : null,
          &quot;referer&quot; : &quot;http://www.elastic-elastic-elastic.com/success/edward-white&quot;,
          &quot;request&quot; : &quot;/apm-server/apm-server-6.3.2-windows-x86.zip&quot;,
          &quot;response&quot; : 200,
          &quot;tags&quot; : [
            &quot;success&quot;,
            &quot;security&quot;
          ],
          &quot;timestamp&quot; : &quot;2021-12-12T12:21:13.414Z&quot;,
          &quot;url&quot; : &quot;https://artifacts.elastic.co/downloads/apm-server/apm-server-6.3.2-windows-x86.zip&quot;,
          &quot;utc_time&quot; : &quot;2021-12-12T12:21:13.414Z&quot;,
          &quot;event&quot; : {
            &quot;dataset&quot; : &quot;sample_web_logs&quot;
          }
        },
        &quot;sort&quot; : [
          0.923208
        ]
      }
    ]
  }
}

</code></pre>
<p>通过按照_score 升序排序可以看到 不符合 should 条件的文档也被搜索出来了。</p>
<p>但是如果在查询条件中将 must 子句删除，那么 should 子句就至少要满足有一条。should 子句需要满足的个数由 query 的 minimum_should_match 参数决定， 默认情况下它的值为 1。</p>
<p>布尔查询在计算相关性得分时，采取了匹配越多分值越高的策略。由于 filter 和 must_not 不参与分值运算，所以文档的最后得分是 must 和 should 子句的相 关性分值相加后返回给用户。</p>
<h3><a id="dis-max%E7%BB%84%E5%90%88%E6%9F%A5%E8%AF%A2" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>dis_max 组合查询</h3>
<p>dis_max 查询也是种组合查询， 只是它在计算相关性度时与 bool 查询不同。 dis_max 查询在计算相关性分值时，会在子查询中取最大相关性分值为最终相关性分值结果，而忽略其他子查询的相关性得分。dis_max 查询通过 queries 参数 接收对象的数组。</p>
<pre><code class="language-bash">POST /kibana_sample_data_logs/_search
{
  &quot;query&quot;: {
    &quot;dis_max&quot;: {
      &quot;queries&quot;: [
        
        {
          &quot;term&quot;: {
            &quot;geo.src&quot;: &quot;CN&quot;
          }
        },
        {
          &quot;term&quot;: {
            &quot;geo.dest&quot;: &quot;CN&quot;
          }
        },
        {
          &quot;match&quot;: {
            &quot;message&quot;: &quot;firefox&quot;
          }
        }
      ]
    }
  },
  &quot;_source&quot;: [&quot;geo&quot;, &quot;agent&quot;,&quot;request&quot;]
}

&quot;tie_breaker&quot;: 0.7
</code></pre>
<p>response:</p>
<pre><code class="language-json">{
  &quot;took&quot; : 2,
  &quot;timed_out&quot; : false,
  &quot;_shards&quot; : {
    &quot;total&quot; : 1,
    &quot;successful&quot; : 1,
    &quot;skipped&quot; : 0,
    &quot;failed&quot; : 0
  },
  &quot;hits&quot; : {
    &quot;total&quot; : {
      &quot;value&quot; : 8325,
      &quot;relation&quot; : &quot;eq&quot;
    },
    &quot;max_score&quot; : 1.6806533,
    &quot;hits&quot; : [
      {
        &quot;_index&quot; : &quot;kibana_sample_data_logs&quot;,
        &quot;_type&quot; : &quot;_doc&quot;,
        &quot;_id&quot; : &quot;mMrI0n0BBQs7jchue4fn&quot;,
        &quot;_score&quot; : 1.6806533,
        &quot;_source&quot; : {
          &quot;geo&quot; : {
            &quot;srcdest&quot; : &quot;EG:CN&quot;,
            &quot;src&quot; : &quot;EG&quot;,
            &quot;coordinates&quot; : {
              &quot;lon&quot; : -85.80931806,
              &quot;lat&quot; : 35.98531194
            },
            &quot;dest&quot; : &quot;CN&quot;
          },
          &quot;request&quot; : &quot;/apm&quot;,
          &quot;agent&quot; : &quot;Mozilla/5.0 (X11; Linux x86_64; rv:6.0a1) Gecko/20110421 Firefox/6.0a1&quot;
        }
      },
      {
        &quot;_index&quot; : &quot;kibana_sample_data_logs&quot;,
        &quot;_type&quot; : &quot;_doc&quot;,
        &quot;_id&quot; : &quot;o8rI0n0BBQs7jchue4fn&quot;,
        &quot;_score&quot; : 1.6806533,
        &quot;_source&quot; : {
          &quot;geo&quot; : {
            &quot;srcdest&quot; : &quot;VN:CN&quot;,
            &quot;src&quot; : &quot;VN&quot;,
            &quot;coordinates&quot; : {
              &quot;lon&quot; : -86.52211111,
              &quot;lat&quot; : 30.77883333
            },
            &quot;dest&quot; : &quot;CN&quot;
          },
          &quot;request&quot; : &quot;/beats/metricbeat/metricbeat-6.3.2-amd64.deb&quot;,
          &quot;agent&quot; : &quot;Mozilla/5.0 (X11; Linux i686) AppleWebKit/534.24 (KHTML, like Gecko) Chrome/11.0.696.50 Safari/534.24&quot;
        }
      }
    ]
  }
}

</code></pre>
<p>在示例的返回结果中，即使 文档 message 和 geo 字段都满足查询条件它也不定会排在最前面。可添加 tie_breaker 参数并设置为 0.7。</p>
<p>在添加了 tie _breaker 参数后，相关度非最高值字段在参与最終相关度结果时的权重就降低为 0.7。但它们对结果排序会产生影响，完全满足条件的文档将 排在结果最前面。</p>
<p>tie_breaker 参数设置其他字段参与相关度运算的系数。这个系数会在运算最终相关度时乘以其他字段的相关度，再加上最大得分就得到最终的相关度。所以一般来说，tie_breaker 应该小于 1, 默认值为 0。</p>
<h3><a id="constant-score%E6%9F%A5%E8%AF%A2" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>constant_score 查询</h3>
<p>constant_score 查询返回结果中文档的相关度为固定值，这个固定值由 boost 参数设置，默认值为 1.0 。constant_score 查询只有两个参数 filter 和 boost，filter 与bool 组合查询中的 filter 完全相同，仅用于过滤结果，不影响分值。</p>
<pre><code class="language-bash">POST /kibana_sample_data_logs/_search
{
  &quot;query&quot;: {
    &quot;constant_score&quot;: {
      &quot;filter&quot;: {
        &quot;match&quot;:{
          &quot;geo.src&quot;: &quot;CN&quot;
        }
      },
      &quot;boost&quot;: 1.2
    }
  }
}
</code></pre>
<p>通过 boost 参数设置了相关度，所以满足查询条件文档的 score 值将都是1.2，match_all查询也可以当成是一种特殊类型的constant_score 查询， 它会返回索引中所有文档，面每个文档的相关度都是 1.0。它的作用和 boost 参 数类似，组合查询时调整子查询在相关度计算中的权重。</p>
<h3><a id="boosting%E6%9F%A5%E8%AF%A2" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>boosting 查询</h3>
<p>boosting 查询通过 positive 子句设置满足条件的文档，这类似于 bool 查询中 的 must 子句，只有满足 positive 条件的文档才会被返回。boosting 查询通过 negative 子句设置需要排除文档的条件， 这类似于 bool 查询中的 must _not 子 旬。但与 bool 查询不同的是，boosting 查询不会将满足 negative 条件的文档从 返回结果中排除，而只是会拉低它们的相关性.</p>
<pre><code class="language-bash">POST /kibana_sample_data_logs/_search
{
  &quot;query&quot;: {
    &quot;boosting&quot;: {
      &quot;positive&quot;: {
        &quot;match&quot;: {
          &quot;geo.src&quot;: &quot;CN&quot;
        }
      },
      &quot;negative&quot;: {
        &quot;match&quot;: {
          &quot;geo.dest&quot;: &quot;CN&quot;
        }
      },
      &quot;negative_boost&quot;: 0.2
    }
  },
  &quot;sort&quot;: [{&quot;_score&quot;: &quot;asc&quot;}]
}
</code></pre>
<p>参数 negative_ boost 设置了一个系数，当满足 negative 条件时 相关度会乘以这个系数作为最终分值，所以这个值应该小于 1 而大于等于 0。， 如果 geo_src 为 CN 的文档相关度为 1.6，那么 geo.dest 字段也是 CN 的文档相关 度就需要再乘以 0.2, 所以最终相关度为 0.32。</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[管理 ElasticSearch 索引和文档]]></title>
    <link href="http://www.throne4j.com/16398107958136.html"/>
    <updated>2021-12-18T14:59:55+08:00</updated>
    <id>http://www.throne4j.com/16398107958136.html</id>
    <content type="html"><![CDATA[
<ul>
<li><a href="#%E7%B4%A2%E5%BC%95%E7%9A%84%E7%AE%A1%E7%90%86">索引的管理</a></li>
<li><a href="#%E9%85%8D%E7%BD%AE%E7%B4%A2%E5%BC%95">配置索引</a>
<ul>
<li><a href="#%E9%9D%99%E6%80%81%E8%AE%BE%E7%BD%AE">静态设置</a></li>
<li><a href="#%E5%8A%A8%E6%80%81%E8%AE%BE%E7%BD%AE">动态设置</a></li>
<li><a href="#%E9%85%8D%E7%BD%AE%E6%98%A0%E5%B0%84">配置映射</a></li>
</ul>
</li>
<li><a href="#%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C">基本操作</a>
<ul>
<li><a href="#%E5%88%97%E5%87%BA%E6%89%80%E6%9C%89%E7%B4%A2%E5%BC%95">列出所有索引</a></li>
<li><a href="#%E5%88%9B%E5%BB%BA%E7%B4%A2%E5%BC%95">创建索引</a></li>
<li><a href="#%E5%85%B3%E9%97%AD%E5%92%8C%E6%89%93%E5%BC%80%E7%B4%A2%E5%BC%95">关闭和打开索引</a></li>
<li><a href="#%E5%A2%9E%E5%8A%A0%E6%96%87%E6%A1%A3">增加文档</a></li>
<li><a href="#%E6%9F%A5%E8%AF%A2%E6%96%87%E6%A1%A3">查询文档</a></li>
<li><a href="#%E6%9B%B4%E6%96%B0%E6%96%87%E6%A1%A3">更新文档</a></li>
<li><a href="#%E5%88%A0%E9%99%A4%E6%96%87%E6%A1%A3">删除文档</a></li>
</ul>
</li>
</ul>

<p>在 es 中，索引和文档是 REST 接口操作的最基本资源，所以对索引和文档的 管理也是我们必须要知道的。</p>
<p>索引一般是以索引名称出现在 REST 请求操作的资 源路径上，而文档是以文档 ID 为标识出现在资源路径上。</p>
<h2><a id="%E7%B4%A2%E5%BC%95%E7%9A%84%E7%AE%A1%E7%90%86" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>索引的管理</h2>
<p>在REST 接口中，通常GET 用来获取资源，POST 用来更新资源， DELETE 用来删除资源。所以对索引，GET 用来查看索引，PUT 用来创建索引，DELETE 用来删除索引，还有一个 HEAD 请求，用来检验索引是否存在</p>
<hr />
<h2><a id="%E9%85%8D%E7%BD%AE%E7%B4%A2%E5%BC%95" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>配置索引</h2>
<p>通过 settings 参数配置索引，索引的所有配置项都以“index”开头。索引的 管理分为静态设置和动态设置两种。</p>
<h3><a id="%E9%9D%99%E6%80%81%E8%AE%BE%E7%BD%AE" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>静态设置</h3>
<p>只能在索引创建时或在状态为 closed index(闭合索引)上设置，主要配置 索引主分片、压缩编码、路由等相关信息。</p>
<ul>
<li>
<p>index.number_of_shards主分片数，默认为 5.只能在创建索引时设置，不能修改</p>
</li>
<li>
<p>index.shard.check_on_startup 是否应在索引打开前检查分片是否损坏，当 检查到分片损坏将禁止分片被打开。</p>
<ul>
<li>false:默认值;</li>
<li>checksum:检查物理损坏;</li>
<li>true:检查物理和逻辑损坏，这将消耗大量内存和 CPU;</li>
<li>fix:检查物理和逻辑损坏。有损坏的分片将被集群自动删除，这可能导致数据丢失</li>
</ul>
</li>
<li>
<p>index.routing_partition_size 自定义路由值可以转发的目的分片数。默认为 1， 只能在索引创建时设置。此值必须小于 index.number_of_shards</p>
</li>
<li>
<p>index.codec 默认使用LZ4压缩方式存储数据，也可以设置为best_compression，它使用 DEFLATE 方式以牺牲字段存储性能为代价来获得更高 的压缩比例。</p>
</li>
</ul>
<pre><code class="language-bash">PUT articles
{
    &quot;settings&quot;: {
        &quot;index.number_of_shards&quot;:3,
        &quot;index.codec&quot;:&quot;best_compression&quot; 
    }
}
</code></pre>
<h3><a id="%E5%8A%A8%E6%80%81%E8%AE%BE%E7%BD%AE" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>动态设置</h3>
<p>通过接口 &quot;_settings&quot; 进行，同时查询配置也通过这个接口进行</p>
<pre><code class="language-bash">get /articles/_settings
</code></pre>
<p>response:</p>
<pre><code class="language-json">{
  &quot;articles&quot; : {
    &quot;settings&quot; : {
      &quot;index&quot; : {
        &quot;codec&quot; : &quot;best_compression&quot;,
        &quot;routing&quot; : {
          &quot;allocation&quot; : {
            &quot;include&quot; : {
              &quot;_tier_preference&quot; : &quot;data_content&quot;
            }
          }
        },
        &quot;number_of_shards&quot; : &quot;3&quot;,
        &quot;provided_name&quot; : &quot;articles&quot;,
        &quot;creation_date&quot; : &quot;1639902918245&quot;,
        &quot;number_of_replicas&quot; : &quot;1&quot;,
        &quot;uuid&quot; : &quot;C4sWtSqaQX2MnTInk4VDig&quot;,
        &quot;version&quot; : {
          &quot;created&quot; : &quot;7150199&quot;
        }
      }
    }
  }
}
</code></pre>
<p>配置索引则通过:</p>
<pre><code class="language-bash">PUT articles/_settings 
{
    &quot;refresh_interval&quot;:&quot;2s&quot;
}
</code></pre>
<p>常用的配置参数如下:</p>
<ul>
<li>index.number_of_replicas 每个主分片的副本数。默认为 1</li>
<li>index.auto_expand_replicas 基于可用节点的数量自动分配副本数量,默认为 false(即禁用此功能)。</li>
<li>index.refresh_interval 执行刷新操作的频率。默认为 1s。可以设置为 -1 以禁用刷新。</li>
<li>index.max_result_window 用于索引搜索的 from+size 的最大值。默认为10000。</li>
<li>index.blocks.read_only 设置为 true 使索引和索引元数据为只读，false 为允许写入和元数据更改。</li>
<li>index.blocks.read 设置为 true 可禁用对索引的读取操作</li>
<li>index.blocks.write 设置为 true 可禁用对索引的写入操作</li>
<li>index.blocks.metadata 设置为 true 可禁用索引元数据的读取和写入</li>
<li>index.max_refresh_listeners 索引的每个分片上可用的最大刷新侦听器数</li>
<li>index.max_docvalue_fields_search 一次查询最多包含开启 doc_values 字段的个数，默认为 100</li>
<li>index.max_script_fields 查询中允许的最大script_fields数量。默认为32。 index.max_terms_count 可以在 terms 查询中使用的术语的最大数量。默认为 65536。</li>
<li>index.routing.allocation.enable 控制索引分片分配。
<ul>
<li>All(所有分片)</li>
<li>primaries(主分片)</li>
<li>new_primaries(新创建分片)</li>
<li>none(不分片)</li>
</ul>
</li>
<li>index.routing.rebalance.enable 索引的分片重新平衡机制。all、primaries、replicas、none</li>
<li>index.gc_deletes 文档删除后(删除后版本号)还可以存活的周期，默认为 60s</li>
<li>index.max_regex_length 用于正在表达式查询(regex query)正在表达式长度，默认为 1000</li>
</ul>
<hr />
<h3><a id="%E9%85%8D%E7%BD%AE%E6%98%A0%E5%B0%84" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>配置映射</h3>
<p>通过_mapping 接口进行</p>
<pre><code class="language-bash">get /my_index/_mapping
# 或者只看某个字段的属性:
get /my_index/_mapping/field/xxx
</code></pre>
<pre><code class="language-JSON">PUT articles/_mapping
{
  &quot;properties&quot;: {
    &quot;author&quot;: {
      &quot;type&quot;: &quot;keyword&quot;
    },
    &quot;content&quot;: {
      &quot;type&quot;: &quot;text&quot;
    },
    &quot;createdTime&quot;: {
      &quot;type&quot;: &quot;date&quot;
    },
    &quot;title&quot;: {
      &quot;type&quot;: &quot;text&quot;
    },
    &quot;pages&quot;: {
      &quot;type&quot;: &quot;integer&quot;
    }
  }
}
</code></pre>
<p>修改映射，当然就是通过 PUT 或者 POST 方法了。但是要注意，已经存在的映射只能添加字段或者字段的多类型。但是字段创建后就不能删除，大多数参数也不能修改，可以改的是 ignore_above。所以设计索引时要做好规划，至少初始 时的必要字段要规划好</p>
<hr />
<h2><a id="%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>基本操作</h2>
<hr />
<h3><a id="%E5%88%97%E5%87%BA%E6%89%80%E6%9C%89%E7%B4%A2%E5%BC%95" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>列出所有索引</h3>
<pre><code class="language-bash">GET /_cat/indices?v&amp;pretty
</code></pre>
<p>response :</p>
<pre><code class="language-plain_text">health status index                           uuid                   pri rep docs.count docs.deleted store.size pri.store.size
green  open   kibana_sample_data_flights      _OyHWDtqRDuPzEG3EGFjZg   1   0      13059            0      5.2mb          5.2mb
green  open   kibana_sample_data_ecommerce    Z-rv4oSEQbObq_Ird1iw6Q   1   0       4675            0      3.7mb          3.7mb
green  open   kibana_sample_data_logs         Ih3CTx1bTCaAl2gLSuTqog   1   0      14074            0      7.8mb          7.8mb
yellow open   saleorder                       oY6Ay0GXQdWT-lUL4K3f9w   5   1         20            0       72kb           72kb
</code></pre>
<hr />
<h3><a id="%E5%88%9B%E5%BB%BA%E7%B4%A2%E5%BC%95" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>创建索引</h3>
<pre><code class="language-bash">PUT articles
</code></pre>
<p>response:</p>
<pre><code class="language-json">{
  &quot;acknowledged&quot; : true,
  &quot;shards_acknowledged&quot; : true,
  &quot;index&quot; : &quot;articles&quot;
}
</code></pre>
<hr />
<h3><a id="%E5%85%B3%E9%97%AD%E5%92%8C%E6%89%93%E5%BC%80%E7%B4%A2%E5%BC%95" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>关闭和打开索引</h3>
<p>关闭索引，如果关闭了一个索引,就无法通过Elasticsearch 来读取和写人其中的数据，直到再次打开它。</p>
<pre><code class="language-bash">POST /articles/_close

# 打开索引
POST /articles/_open
</code></pre>
<p>response :</p>
<pre><code class="language-bash">{
  # 关闭索引
  &quot;acknowledged&quot; : true,
  &quot;shards_acknowledged&quot; : true,
  &quot;indices&quot; : {
    &quot;articles&quot; : {
      &quot;closed&quot; : true
    }
  }
}

{
  # 打开索引
  &quot;acknowledged&quot; : true,
  &quot;shards_acknowledged&quot; : true
}
</code></pre>
<hr />
<h3><a id="%E5%A2%9E%E5%8A%A0%E6%96%87%E6%A1%A3" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>增加文档</h3>
<pre><code class="language-json">POST articles/_doc/1
{
  &quot;author&quot;: &quot;Leo Tolstoy&quot;,
  &quot;content&quot;: &quot;War and Peace is a literary work mixed with chapters on history and philosophy by the Russian author Leo Tolstoy, first published serially, then published in its entirety in 1869. It is regarded as one of Tolstoy's finest literary achievements and remains an internationally praised classic of world literature.&quot;,
  &quot;createdTime&quot;:&quot;2021-12-19 15:14:25&quot;,
  &quot;title&quot;:&quot;War and Peace&quot;,
  &quot;page&quot;:1225
  
}
</code></pre>
<p>response:</p>
<pre><code class="language-json">{
  &quot;_index&quot; : &quot;articles&quot;,
  &quot;_type&quot; : &quot;_doc&quot;,
  &quot;_id&quot; : &quot;1&quot;,
  &quot;_version&quot; : 1,
  &quot;_seq_no&quot; : 0,
  &quot;_primary_term&quot; : 1,
  &quot;found&quot; : true,
  &quot;_source&quot; : {
    &quot;author&quot; : &quot;Leo Tolstoy&quot;,
    &quot;content&quot; : &quot;War and Peace is a literary work mixed with chapters on history and philosophy by the Russian author Leo Tolstoy, first published serially, then published in its entirety in 1869. It is regarded as one of Tolstoy's finest literary achievements and remains an internationally praised classic of world literature.&quot;,
    &quot;createdTime&quot; : &quot;2021-12-19 15:14:25&quot;,
    &quot;title&quot; : &quot;War and Peace&quot;,
    &quot;page&quot; : 1225
  }
}
</code></pre>
<p>当创建文档的时候，如果不指定 ID，系统会自动创建 ID。自动生成的 ID 是一个不会重复的随机数。使用 GUID 算法，可以保证在分布式环境下，不同节点同一时间创建的_id 一定是不冲突的。</p>
<hr />
<h3><a id="%E6%9F%A5%E8%AF%A2%E6%96%87%E6%A1%A3" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>查询文档</h3>
<pre><code class="language-bash">GET /articles/_doc/1
</code></pre>
<p>response:</p>
<pre><code class="language-json">{
  &quot;_index&quot; : &quot;articles&quot;,
  &quot;_type&quot; : &quot;_doc&quot;,
  &quot;_id&quot; : &quot;1&quot;,
  &quot;_version&quot; : 3,
  &quot;_seq_no&quot; : 2,
  &quot;_primary_term&quot; : 1,
  &quot;found&quot; : true,
  &quot;_source&quot; : {
    &quot;author&quot; : &quot;Leo Tolstoy&quot;,
    &quot;content&quot; : &quot;War and Peace is a literary work mixed with chapters on history and philosophy by the Russian author Leo Tolstoy, first published serially, then published in its entirety in 1869. It is regarded as one of Tolstoy's finest literary achievements and remains an internationally praised classic of world literature.&quot;,
    &quot;createdTime&quot; : &quot;2021-12-19 15:14:25&quot;,
    &quot;title&quot; : &quot;《War and Peace》&quot;,
    &quot;page&quot; : 1225
  }
}

</code></pre>
<hr />
<h3><a id="%E6%9B%B4%E6%96%B0%E6%96%87%E6%A1%A3" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>更新文档</h3>
<p>前面我们用put方法更新了已经存在的文档，但是可以看见他是整体更新文 档，如果我们要更新文档中的某个字段怎么办?需要使用_update 接口。</p>
<pre><code class="language-bash">POST articles/_update/1
{
  &quot;doc&quot;: {
    &quot;createdTime&quot;:&quot;2008-12-19 15:14:25&quot;
  }
}
</code></pre>
<p>response:</p>
<pre><code class="language-json">{
  &quot;_index&quot; : &quot;articles&quot;,
  &quot;_type&quot; : &quot;_doc&quot;,
  &quot;_id&quot; : &quot;1&quot;,
  &quot;_version&quot; : 4,
  &quot;_seq_no&quot; : 3,
  &quot;_primary_term&quot; : 1,
  &quot;found&quot; : true,
  &quot;_source&quot; : {
    &quot;author&quot; : &quot;Leo Tolstoy&quot;,
    &quot;content&quot; : &quot;War and Peace is a literary work mixed with chapters on history and philosophy by the Russian author Leo Tolstoy, first published serially, then published in its entirety in 1869. It is regarded as one of Tolstoy's finest literary achievements and remains an internationally praised classic of world literature.&quot;,
    &quot;createdTime&quot; : &quot;2008-12-19 15:14:25&quot;,
    &quot;title&quot; : &quot;《War and Peace》&quot;,
    &quot;page&quot; : 1225
  }
}
</code></pre>
<p>如果文档中存在 createdTime 字段，更新 createdTime 字段的值，如果不存在 createdTime 字段，则 会新增 createdTime 字段，并将值设为 2008-12-19 15:14:25</p>
<p>update 接口在文档不存在时提示错误，如果希望在文档不存在时创建文档， 则可以在请求中添加 upsert 参数或 doc_as_upsert 参数，</p>
<p>upsert 参数定义了创建新文档使用的文档内容，而 doc_as_upsert 参数的含义是直接使用 doc 参数中的内容作为创建文档时使用的文档内容。</p>
<pre><code class="language-bash">POST articles/_update/2
{
  &quot;doc&quot;: {
    &quot;author&quot;: &quot;Liu Cixin&quot;,
  &quot;content&quot;: &quot;The Three-Body Problem is a science fiction novel written by the Chinese writer Liu Cixin. The title refers to the three-body problem in orbital mechanics. It is the first novel of the Remembrance of Earth's Past trilogy, but the whole series is normally referred to as The Three-Body Problem.&quot;,
  &quot;createdTime&quot;:&quot;2008-12-19 15:14:25&quot;,
  &quot;title&quot;:&quot;The Three-Body Problem&quot;,
  &quot;page&quot;:302
  },
  &quot;doc_as_upsert&quot;:true
}
</code></pre>
<p>response:</p>
<pre><code class="language-json">{
  &quot;_index&quot; : &quot;articles&quot;,
  &quot;_type&quot; : &quot;_doc&quot;,
  &quot;_id&quot; : &quot;2&quot;,
  &quot;_version&quot; : 1,
  &quot;result&quot; : &quot;created&quot;,
  &quot;_shards&quot; : {
    &quot;total&quot; : 2,
    &quot;successful&quot; : 1,
    &quot;failed&quot; : 0
  },
  &quot;_seq_no&quot; : 4,
  &quot;_primary_term&quot; : 1
}
</code></pre>
<hr />
<h3><a id="%E5%88%A0%E9%99%A4%E6%96%87%E6%A1%A3" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>删除文档</h3>
<pre><code class="language-bash">delete /articles/_doc/1
</code></pre>
<p>response:</p>
<pre><code class="language-json">{
  &quot;_index&quot; : &quot;articles&quot;,
  &quot;_type&quot; : &quot;_doc&quot;,
  &quot;_id&quot; : &quot;1&quot;,
  &quot;_version&quot; : 5,
  &quot;result&quot; : &quot;deleted&quot;,
  &quot;_shards&quot; : {
    &quot;total&quot; : 2,
    &quot;successful&quot; : 1,
    &quot;failed&quot; : 0
  },
  &quot;_seq_no&quot; : 5,
  &quot;_primary_term&quot; : 1
}

</code></pre>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Git 回滚到指定分支并推送到远程仓库]]></title>
    <link href="http://www.throne4j.com/16352408415801.html"/>
    <updated>2021-10-26T17:34:01+08:00</updated>
    <id>http://www.throne4j.com/16352408415801.html</id>
    <content type="html"><![CDATA[
<h2><a id="%E4%B8%80%E3%80%81%E6%9F%A5%E7%9C%8B%E6%8F%90%E4%BA%A4%E5%8E%86%E5%8F%B2%E5%B9%B6%E8%8E%B7%E5%8F%96commitid" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>一、查看提交历史并获取commitId:</h2>
<pre><code class="language-plain_text">git log --graph 推荐使用的命令
git log --pretty =oneline
git reflog   查看每一次命令的记录
</code></pre>
<h2><a id="%E4%BA%8C%E3%80%81%E8%BF%9B%E8%A1%8C%E5%9B%9E%E9%80%80%E6%93%8D%E4%BD%9C" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>二、进行回退操作</h2>
<p>git reset --soft:回退到某个版本，只回退了commit的信息。</p>
<p>git reset --mixed:为默认方式，不带任何参数的git reset，即时这种方式，它回退到某个版本，只保留源码，回退commit和index信息。</p>
<p>git reset --hard:彻底回退到某个版本，本地的源码也会变为上一个版本的内容，撤销的commit中所包含的更改被冲掉。</p>
<p>注意git reset --hard与git reset  --soft的区别：<br />
git reset –-soft：回退到某个版本，只回退了commit的信息，不会恢复到index file一级。通常使用在当你git commit -m &quot;注释&quot;提交了你修改的内容，但内容有点问题想撤销，又还要提交，就使用soft，相当于软着路；<br />
git reset -–hard：彻底回退到某个版本，本地的源码也会变为上一个版本的内容，撤销的commit中所包含的更改被冲掉，相当于硬着路，回滚最彻底。</p>
<h2><a id="%E4%B8%89%E3%80%81%E5%BC%BA%E5%88%B6%E6%8E%A8%E9%80%81%E5%88%B0%E8%BF%9C%E7%A8%8B%E4%BB%93%E5%BA%93" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>三、强制推送到远程仓库</h2>
<p>git push -f origin <branch name></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Git常用操作命令收集：]]></title>
    <link href="http://www.throne4j.com/16352407965944.html"/>
    <updated>2021-10-26T17:33:16+08:00</updated>
    <id>http://www.throne4j.com/16352407965944.html</id>
    <content type="html"><![CDATA[
<h2><a id="git%E9%85%8D%E7%BD%AE" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>git 配置</h2>
<p>ssh -T <a href="mailto:git@github.com">git@github.com</a>  <br />
配置你的用户名和密码<br />
git config --global  --list<br />
git config --global user.name &quot;qinshengke&quot;  <br />
git config --global user.email <a href="mailto:qinshengke@meituan.com">qinshengke@meituan.com</a>  </p>
<h2><a id="%E7%BB%91%E5%AE%9A%E6%9C%AC%E5%9C%B0%E5%88%86%E6%94%AF%E5%88%B0github" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>绑定本地分支到github</h2>
<p>git init<br />
git add .<br />
git commit -m “提交说明”<br />
git remote add origin {远程仓库地址}<br />
git remote -v<br />
git push -u origin master</p>
<h2><a id="%E6%9B%BF%E6%8D%A2%E5%88%86%E6%94%AF" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>替换分支</h2>
<p>git push origin transfer:master -f<br />
把本地的 transfer 分支强制(-f)推送到远程 master</p>
<p>把github上没有拉下来的代码或文件拉下来<br />
git pull --rebase origin master</p>
<h3><a id="%E8%BF%9C%E7%A8%8B%E4%BB%93%E5%BA%93%E7%9B%B8%E5%85%B3%E5%91%BD%E4%BB%A4" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>远程仓库相关命令</h3>
<ul>
<li>检出仓库：</li>
</ul>
<blockquote>
<p>$ git clone git://github.com/jquery/jquery.git</p>
</blockquote>
<ul>
<li>查看远程仓库：
<blockquote>
<p>$ git remote -v</p>
</blockquote>
</li>
<li>添加远程仓库：</li>
</ul>
<blockquote>
<p>$ git remote add [name] [url]</p>
</blockquote>
<ul>
<li>删除远程仓库：</li>
</ul>
<blockquote>
<p>$ git remote rm [name]</p>
</blockquote>
<ul>
<li>修改远程仓库</li>
</ul>
<blockquote>
<p>$ git remote set-url --push[name][newUrl]</p>
</blockquote>
<ul>
<li>拉取指定分支代码</li>
</ul>
<blockquote>
<p>$ git clone -b fixbug_20170821 <a href="https://github.com/kkche/service-billing.git">https://github.com/kkche/service-billing.git</a></p>
</blockquote>
<p>拉取远程仓库：$ git pull [remoteName] [localBranchName]</p>
<p>推送远程仓库：$ git push [remoteName] [localBranchName]<br />
git push -u origin master</p>
<h3><a id="%E5%88%86%E6%94%AFbranch%E6%93%8D%E4%BD%9C%E7%9B%B8%E5%85%B3%E5%91%BD%E4%BB%A4" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>分支(branch)操作相关命令</h3>
<ul>
<li>查看本地分支：$ git branch</li>
<li>修改branch名字</li>
</ul>
<blockquote>
<p>$git branch -m sourceName  targetName</p>
</blockquote>
<ul>
<li>
<p>查看远程分支：$ git branch -r</p>
</li>
<li>
<p>创建本地分支：$ git branch [name] ----注意新分支创建后不会自动切换为当前分支</p>
</li>
<li>
<p>切换分支：$ git checkout [name]</p>
</li>
<li>
<p>创建新分支并立即切换到新分支：$ git checkout -b [name]</p>
</li>
<li>
<p>删除分支：$ git branch -d [name]<br />
【-d】选项只能删除已经参与了合并的分支，对于未有合并的分支是无法删除的。如果想强制删除一个分支，可以使用-D选项</p>
</li>
<li>
<p>合并分支：$ git merge [name]<br />
将名称为[name]的分支与当前分支合并(首先进入到想要合并到的分支)</p>
</li>
<li>
<p>创建远程分支(本地分支push到远程)：$ git push origin [name]</p>
</li>
<li>
<p>删除远程分支：$git  push origin --delete [分支名称]</p>
</li>
<li>
<p>推送到远程分支</p>
</li>
</ul>
<blockquote>
<p>$git push origin :branch  (origin 后面有空格)</p>
</blockquote>
<blockquote>
<p>$ git pull &lt;远程主机名&gt; &lt;远程分支名&gt;:&lt;本地分支名&gt; 拉取指定远程分支到本地指定分支中，省略本地分支 则标示本地当前分支</p>
</blockquote>
<blockquote>
<p>$ git branch --set-upstream master origin/next   命令指定master分支追踪origin/next分支。<br />
git push -u origin develop，这个操作在push的同时会指定当前分支的upstream。</p>
</blockquote>
<h3><a id="3%EF%BC%89%E7%89%88%E6%9C%AC-tag%E6%93%8D%E4%BD%9C%E7%9B%B8%E5%85%B3%E5%91%BD%E4%BB%A4" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>3）版本(tag)操作相关命令</h3>
<ul>
<li>查看并筛选本地tag</li>
</ul>
<blockquote>
<p>git tag -l | grep &quot;tag_name&quot;</p>
</blockquote>
<ul>
<li>创建本地tag</li>
</ul>
<blockquote>
<p>git tag -a tag_name -m &quot;tag信息&quot;</p>
</blockquote>
<ul>
<li>推送tag到远程</li>
</ul>
<blockquote>
<p>git push origin :tag_name</p>
</blockquote>
<ul>
<li>删除本地tag分支</li>
</ul>
<blockquote>
<p>git tag -d tag_name</p>
</blockquote>
<ul>
<li>删除远程tag分支 : git push origin :refs/tags/[name]</li>
</ul>
<blockquote>
<p>git push origin :refs/tags/tag_name</p>
</blockquote>
<ul>
<li>删除远程分支</li>
</ul>
<blockquote>
<p>git branch -r -d origin/branch-name</p>
</blockquote>
<h3><a id="%E5%AD%90%E6%A8%A1%E5%9D%97submodule%E7%9B%B8%E5%85%B3%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>子模块(submodule)相关操作命令</h3>
<p>添加子模块：$ git submodule add [url] [path]<br />
如：$ git submodule add git://github.com/soberh/ui-libs.git src/main/webapp/ui-libs<br />
初始化子模块：$ git submodule init ----只在首次检出仓库时运行一次就行<br />
更新子模块：$ git submodule update ----每次更新或切换分支后都需要运行一下<br />
删除子模块：（分4步走哦）</p>
<ul>
<li>1)$ git rm --cached [path]</li>
<li>
<ol start="2">
<li>编辑“.gitmodules”文件，将子模块的相关配置节点删除掉</li>
</ol>
</li>
<li>
<ol start="3">
<li>编辑“.git/config”文件，将子模块的相关配置节点删除掉</li>
</ol>
</li>
<li>
<ol start="4">
<li>手动删除子模块残留的目录<br />
 </li>
</ol>
</li>
</ul>
<h3><a id="%E5%BF%BD%E7%95%A5%E4%B8%80%E4%BA%9B%E6%96%87%E4%BB%B6%E3%80%81%E6%96%87%E4%BB%B6%E5%A4%B9%E4%B8%8D%E6%8F%90%E4%BA%A4" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>忽略一些文件、文件夹不提交</h3>
<p>在仓库根目录下创建名称为“.gitignore”的文件，写入不需要的文件夹名或文件，每个元素占一行即可，如<br />
target<br />
bin<br />
*.db<br />
 <br />
 </p>
<h3><a id="git%E6%93%8D%E4%BD%9C%E5%88%A0%E9%99%A4%E6%96%87%E4%BB%B6" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>git操作-删除文件</h3>
<p>git rm add2.txt<br />
git commit -m &quot;rm test&quot;<br />
git push web<br />
 <br />
git remote set-url origin <a href="mailto:git@gitlab.kanche.com">git@gitlab.kanche.com</a>:kkche/code-check.git</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[git tag操作]]></title>
    <link href="http://www.throne4j.com/16345471631510.html"/>
    <updated>2021-10-18T16:52:43+08:00</updated>
    <id>http://www.throne4j.com/16345471631510.html</id>
    <content type="html"><![CDATA[
<h2><a id="%E6%9F%A5%E7%9C%8B%E5%B9%B6%E7%AD%9B%E9%80%89%E6%9C%AC%E5%9C%B0tag" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>查看并筛选本地tag</h2>
<p>git tag -l | grep &quot;tag_name&quot;</p>
<h2><a id="%E5%88%9B%E5%BB%BA%E6%9C%AC%E5%9C%B0tag" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>创建本地tag</h2>
<p>git tag -a tag_name -m &quot;tag信息&quot;</p>
<h2><a id="%E6%8E%A8%E9%80%81tag%E5%88%B0%E8%BF%9C%E7%A8%8B" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>推送tag到远程</h2>
<p>git push origin :tag_name</p>
<h2><a id="%E5%88%A0%E9%99%A4%E6%9C%AC%E5%9C%B0tag%E5%88%86%E6%94%AF" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>删除本地tag分支</h2>
<p>git tag -d tag_name</p>
<h2><a id="%E5%88%A0%E9%99%A4%E8%BF%9C%E7%A8%8Btag%E5%88%86%E6%94%AF" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>删除远程tag分支</h2>
<p>git push origin :refs/tags/tag_name</p>
<h2><a id="%E5%88%A0%E9%99%A4%E8%BF%9C%E7%A8%8B%E5%88%86%E6%94%AF" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>删除远程分支</h2>
<p>git branch -r -d origin/branch-name</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[真正能支撑高并发以及高可用的复杂系统中的缓存架构有哪些东西？]]></title>
    <link href="http://www.throne4j.com/16317276545331.html"/>
    <updated>2021-09-16T01:40:54+08:00</updated>
    <id>http://www.throne4j.com/16317276545331.html</id>
    <content type="html"><![CDATA[
<p>（1）如何让redis集群支撑几十万QPS高并发+99.99%高可用+TB级海量数据+企业级数据备份与恢复？：redis企业级集群架构</p>
<p>（2）如何支撑高性能以及高并发到极致？同时给缓存架构最后的安全保护层？：(nginx+lua)+redis+ehcache的三级缓存架构</p>
<p>（3）高并发场景下，如何解决数据库与缓存双写的时候数据不一致的情况？：企业级的完美的数据库+缓存双写一致性解决方案</p>
<p>（4）如何解决大value缓存的全量更新效率低下问题？：缓存维度化拆分解决方案</p>
<p>（5）如何将缓存命中率提升到极致？：双层nginx部署架构，以及lua脚本实现的一致性hash流量分发策略</p>
<p>（6）如何解决高并发场景下，缓存重建时的分布式并发重建的冲突问题？：基于zookeeper分布式锁的缓存并发重建解决方案</p>
<p>（7）如何解决高并发场景下，缓存冷启动MySQL瞬间被打死的问题？：基于storm实时统计热数据的分布式快速缓存预热解决方案</p>
<p>（8）如何解决热点缓存导致单机器负载瞬间超高？：基于storm的实时热点发现，以及毫秒级的实时热点缓存负载均衡降级</p>
<p>（9）如何解决分布式系统中的服务高可用问题？避免多层服务依赖因为少量故障导致系统崩溃？：基于hystrix的高可用缓存服务，资源隔离+限流+降级+熔断+超时控制</p>
<p>（10）如何应用分布式系统中的高可用服务的高阶技术？：基于hystrix的容错+多级降级+手动降级+生产环境参数优化经验+可视化运维与监控</p>
<p>（11）如何解决恐怖的缓存雪崩问题？避免给公司带来巨大的经济损失？：独家的事前+事中+事后三层次完美解决方案</p>
<p>（12）如何解决高并发场景下的缓存穿透问题？避免给MySQL带来过大的压力？：缓存穿透解决方案</p>
<p>（13）如何解决高并发场景下的缓存失效问题？避免给redis集群带来过大的压力？：缓存失效解决方案</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[]]></title>
    <link href="http://www.throne4j.com/16293451744173.html"/>
    <updated>2021-08-19T11:52:54+08:00</updated>
    <id>http://www.throne4j.com/16293451744173.html</id>
    <content type="html"><![CDATA[

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[mysql容灾]]></title>
    <link href="http://www.throne4j.com/16281342832757.html"/>
    <updated>2021-08-05T11:31:23+08:00</updated>
    <id>http://www.throne4j.com/16281342832757.html</id>
    <content type="html"><![CDATA[
<h2><a id="%E5%A4%87%E4%BB%BD%E4%B8%8E%E5%AE%B9%E7%81%BE%E7%9A%84%E5%8C%BA%E5%88%AB" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>备份与容灾的区别</h2>
<p>我们都知道不要在一棵树上吊死这样一个道理，所以进行备份就显示出来有多重要了<br />
备份是数据的冷备份，而容灾呢则是为了服务的可持续性，保证系统在分布式环境的连续可用性和数据一致性</p>
<h2><a id="" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a></h2>
<p>对象存储<br />
一次写多次读的特点<br />
静态资源、镜像、备份、日志、模型等</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Mysql serverTimezone 引起的存储的时间与程序插入时间不一致的问题]]></title>
    <link href="http://www.throne4j.com/16212420089177.html"/>
    <updated>2021-05-17T17:00:08+08:00</updated>
    <id>http://www.throne4j.com/16212420089177.html</id>
    <content type="html"><![CDATA[
<p>答：mysql-connector-java.jar 8.0+版本在jdbcUrl中必须配置时区参数</p>
<pre><code class="language-yml">url: jdbc:mysql://localhost:3306/local_db?useUnicode=true&amp;characterEncoding=utf-8&amp;allowMultiQueries=true&amp;zeroDateTimeBehavior=convertToNull&amp;useSSL=false&amp;serverTimezone=Asia/Shanghai
</code></pre>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[ElasticSearch]]></title>
    <link href="http://www.throne4j.com/16208847861002.html"/>
    <updated>2021-05-13T13:46:26+08:00</updated>
    <id>http://www.throne4j.com/16208847861002.html</id>
    <content type="html"><![CDATA[
<ul>
<li><a href="#%E4%BB%80%E4%B9%88%E6%98%AFelasticsearch">什么是ElasticSearch</a></li>
<li><a href="#near-realtime%EF%BC%88nrt%E8%BF%91%E5%AE%9E%E6%97%B6%EF%BC%89">Near Realtime（NRT 近实时）</a></li>
<li><a href="#es%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5">es基础概念</a>
<ul>
<li><a href="#index%EF%BC%88%E7%B4%A2%E5%BC%95%EF%BC%89">Index（索引）</a></li>
<li><a href="#document%E6%96%87%E6%A1%A3">Document(文档)</a></li>
<li><a href="#%E6%98%A0%E5%B0%84">映射</a></li>
<li><a href="#field%E6%96%87%E6%A1%A3%E5%AD%97%E6%AE%B5">Field(文档字段)</a>
<ul>
<li><a href="#%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B">数据类型</a></li>
<li><a href="#%E5%AD%97%E6%AE%B5%E5%8F%82%E6%95%B0">字段参数</a></li>
</ul>
</li>
<li><a href="#%E5%85%83%E5%AD%97%E6%AE%B5meta-fields">元字段 meta-fields</a>
<ul>
<li><a href="#%E8%BA%AB%E4%BB%BD%E6%A0%87%E8%AF%86%E5%85%83%E6%95%B0%E6%8D%AE">身份标识元数据</a></li>
<li><a href="#%E7%B4%A2%E5%BC%95%E5%85%83%E6%95%B0%E6%8D%AE">索引元数据</a></li>
<li><a href="#%E6%96%87%E6%A1%A3%E5%85%83%E6%95%B0%E6%8D%AE">文档元数据</a></li>
<li><a href="#%E8%B7%AF%E7%94%B1%E5%85%83%E6%95%B0%E6%8D%AE">路由元数据</a></li>
<li><a href="#%E5%85%B6%E4%BB%96">其他</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#elasticsearch%E9%9B%86%E7%BE%A4%E6%A6%82%E5%BF%B5">ElasticSearch 集群概念</a>
<ul>
<li><a href="#cluster%EF%BC%88%E9%9B%86%E7%BE%A4%EF%BC%89">Cluster（集群）</a></li>
<li><a href="#node%EF%BC%88%E8%8A%82%E7%82%B9%EF%BC%89">Node（节点）</a></li>
<li><a href="#shards-replicas%EF%BC%88%E5%88%86%E7%89%87%E5%89%AF%E6%9C%AC%EF%BC%89">Shards &amp; Replicas（分片 &amp; 副本）</a></li>
</ul>
</li>
</ul>

<h2><a id="%E4%BB%80%E4%B9%88%E6%98%AFelasticsearch" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>什么是ElasticSearch</h2>
<p>Elasticsearch 是一个高度可扩展且开源的分布式搜索引擎。它可以让您快速且近实时地存储，检索以及分析海量数据。它通常用作那些具有复杂搜索功能和需求的应用的底层引擎或者技术，提供搜集、分析、存储数据三大功能。它的特点有: 分布式，零配置，自动发现，索引自动分片，索引副本机制， restful 风格接口，多数据源，自动搜索负载等等。</p>
<p>同时 Logstash 和 Beats 有助于收集，聚合和丰富您的数据并将其存储在Elasticsearch中。使用Kibana，您可以交互式地探索，可视化和共享对数据的见解，并管理和监视堆栈。Elasticsearch是建立索引，搜索和分析魔术的地方。</p>
<h2><a id="near-realtime%EF%BC%88nrt%E8%BF%91%E5%AE%9E%E6%97%B6%EF%BC%89" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>Near Realtime（NRT 近实时）</h2>
<p>Elasticsearch 是一个近实时的搜索平台。这意味着从您索引一个文档开始直到它可以被查询时会有轻微的延迟时间（默认为一秒），</p>
<h2><a id="es%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>es基础概念</h2>
<table>
<thead>
<tr>
<th>关系型数据库</th>
<th>ElasticSearch</th>
</tr>
</thead>
<tbody>
<tr>
<td>库</td>
<td>index</td>
</tr>
<tr>
<td>表</td>
<td>mapping type</td>
</tr>
<tr>
<td>数据行</td>
<td>document</td>
</tr>
<tr>
<td>字段</td>
<td>field</td>
</tr>
<tr>
<td>表结构</td>
<td>mapping</td>
</tr>
</tbody>
</table>
<h3><a id="index%EF%BC%88%E7%B4%A2%E5%BC%95%EF%BC%89" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>Index（索引）</h3>
<p>Elasticsearch 索引是映射类型的容器。一个 Elasticsearch 索引非常像关系型中的数据库，是独立的大量文档集合。</p>
<h3><a id="document%E6%96%87%E6%A1%A3" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>Document(文档)</h3>
<p>文档是 es 中所有可搜索数据的最小单位，比如日志文件中的日志项、一个订单等具体信息。</p>
<p>文档会被序列化 JSON 格式保存到 ElasticSearch 中，JSON 对象由字段组成， 每个字段都有对象的字段类型(字符串，数值，布尔，日期，二进制，范围类型)。 同时每个文档都有一个 Unique ID，可以自己指定 ID，或者通过 ElasticSearch 自动生成。</p>
<h3><a id="%E6%98%A0%E5%B0%84" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>映射</h3>
<p>映射(mapping)定义了每个字段的类型、字段所使用的分词器等。</p>
<p>映射可以显式映射，由我们在索引映射中进行预先定义;也可以动态映射，在添 加文档的时候，由 es 自动添加到索引，这个过程不需要事先在索引进行字段数 据类型匹配等等，es 会自己推断数据类型。</p>
<h3><a id="field%E6%96%87%E6%A1%A3%E5%AD%97%E6%AE%B5" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>Field(文档字段)</h3>
<p>文档中的一个字段 field 就相当于关系型数据库中的一列 column，它也具有数据类型。</p>
<h4><a id="%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>数据类型</h4>
<ul>
<li>字符串类型<br />
字符串类型可以被分为 text和keyword类型，如果我们让es自动映射数据，es会把字符串定义为 text，并且加一个 keyword 类型字段。
<ul>
<li>text 文本数据类型，用于索引全文值的字段。使用 text 的字段，它会被分词，并加入到倒排索引，一般不具备唯一性的字符串都可以设置为 text</li>
<li>keyword 关键字数据类型，用于索引唯一内容的字段(身份证、订单号、手机号等)，keyword 只能用按照其确切值进行搜索，无法进行模糊查询</li>
</ul>
</li>
<li>数字类型 long、integer、short、byte、double、float</li>
<li>日期类型 date</li>
<li>布尔类型 boolean</li>
<li>复杂数据类型
<ul>
<li>数组：无需专门的数据类型</li>
<li>对象数据类型： 单独的劲松</li>
<li>嵌套数据类型： nested，json对象的数组</li>
</ul>
</li>
<li>地理数据类型
<ul>
<li>地理点数据类型</li>
<li>地理形状数据类型</li>
</ul>
</li>
<li>ipv4 数据类型</li>
<li>单词计数数据类型： token_count</li>
</ul>
<h4><a id="%E5%AD%97%E6%AE%B5%E5%8F%82%E6%95%B0" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>字段参数</h4>
<ul>
<li>analyzer<br />
指定分词器，对于 text 类型的字段，首先会使用分词器进行分词，然后将分词后的词根一个一个存储在 倒排索引中，后续查询主要是针对词根的搜索。<br />
analyzer可以在每个查询、字段、索引中使用，其优先级如下(越靠前越优先)：
<ul>
<li>字段上定义的分词器</li>
<li>索引配置中定义的分词器</li>
<li>默认分词器(stadard)</li>
</ul>
</li>
<li>normalizer<br />
规范化，主要针对 keyword 类型，在索引该字段或查询字段之前，可以先对原始数据进行一些建单的处理，然后再将处理后的结果当成一个词根存入倒排索引中，默认为 null。</li>
</ul>
<pre><code class="language-bash">PUT /my_index
{
  &quot;settings&quot;: {
    &quot;analysis&quot;: {
      &quot;normalizer&quot;: {
        &quot;my_normalizer&quot;: {
          &quot;type&quot;: &quot;custom&quot;,
          &quot;filter&quot;: [&quot;lowercase&quot;,&quot;asciifolding&quot;]
        }
      }
    }
  }
}
</code></pre>
<ul>
<li>
<p>coerce<br />
默认true，保证数值不合法的部分被去除掉，例如将字符串强制转为正数、浮点被强制转为整数</p>
</li>
<li>
<p>copy_to<br />
将字段 copy 到 目标字段</p>
</li>
<li>
<p>doc_values<br />
es中倒排索引的优势是查找包含某个词项的文档，而对于从另外一个方向的相反操作并不高效，即确定单个文档里面所有的唯一此项，默认为true，并且只适用于非 text 类型的字段。</p>
</li>
<li>
<p>dynamic<br />
是否允许动态的隐式增加字段，在执行 index api 或 更新文档 API 时，对于 _source 字段中包含一些原先未定义的字段采取的措施，根据 dynamic 的取值， 会进行不同的操作:</p>
<ul>
<li>true，默认值，表示新的字段会加入到类型映射中。</li>
<li>false,新的字段会被忽略，即不会存入_souce 字段中，即不会存储新字段，也无法通过新字段进行查询。</li>
<li>strict，会显示抛出异常，需要新使用 put mapping api 先显示增加字段映射。</li>
</ul>
</li>
<li>
<p>enabled<br />
是否建立索引，默认情况下为 true，但有时候某些类型的字段，无需建立索引，只是用来存储数据即可，enable 设置为 false 的字段，只能从 _source 中获取，不可搜索，只有 映射类型 type 和 object 类型的字段可以设置 enabled 属性。</p>
</li>
<li>
<p>eager_global_ordinals<br />
表示是否提前加载全局顺序号。Global ordinals 是一个建立在 doc_values 和 fielddata 基础上的数据结构, 它为每一个精确词按照字母顺序维护递增的编号，只支持 text 和 keyword 型字段，在 keyword 类型字段中默认是开启的，而在 text 型字段中只有 fielddata 和相关属性开启的状态下才是可用的</p>
</li>
<li>
<p>fielddata<br />
默认false，为了解决排序与聚合，es 为了支持 text 字段高效排序与聚合，引入了一种新的数据结构(fielddata)，使用内存进行存储。<br />
默认构建时机为第一次聚合查询、排序操作时构建，主要存储倒排索引中的词根与文档的映射关系，聚合，排序操作在内存中执行。因此 fielddata 需要消耗大量的 JVM 堆内存。<br />
一旦 fielddata 加载到内存后，它将永久存在。谨慎使用</p>
</li>
<li>
<p>format<br />
在 JSON 文档中，日期表示为字符串。Elasticsearch 使用一组预先配置的格式 来识别和解析这些字符串，并将其解析为 long 类型的数值(毫秒)，支持自定义格式，也有内置格式</p>
</li>
<li>
<p>ignore_above<br />
用于指定字段索引和存储的长度最大值，超过这个最大值会被忽略掉</p>
</li>
<li>
<p>index<br />
默认true，指定字段是否索引，不索引也就不可搜索，取值可以为 true 或 false</p>
</li>
<li>
<p>index_options<br />
index_options 控制索引时存储哪些信息到倒排索引中，，用于搜索和突出显 示目的。</p>
<ul>
<li>docs: 只存储文档编号</li>
<li>freqs: 存储文档编号和词项频率</li>
<li>positions: 存储文档编号、词项频率、词项的位置</li>
<li>offsets: 存储文档编号、词项频率、词项的位置、词项开始和结束字符位置都被存储</li>
</ul>
</li>
<li>
<p>fields<br />
fields 可以让同一文本有多种不同的索引方式</p>
</li>
<li>
<p>norms<br />
norms 参数用于标准化文档，以便查询时计算文档的相关性。norms 虽然对评分有用，但是会消耗较多的磁盘空间，如果不需要对某个字段进行评分，最好不要开启 norms</p>
</li>
<li>
<p>null_value<br />
一般来说值为 null 的字段不索引也不可以搜索，null_value 参数可以让值为 null 的字段显式的可索引、可搜索。</p>
</li>
<li>
<p>position_increment_gap<br />
文本数组元素之间位置信息添加的额外值。<br />
举例，一个字段的值为数组类型:&quot;names&quot;: [ &quot;John Abraham&quot;, &quot;Lincoln Smith&quot;]<br />
为了区别第一个字段和第二个字段，Abraham 和 Lincoln 在索引中有一个间 距，默认是 100。</p>
</li>
</ul>
<pre><code class="language-shell">PUT my_index/groups/1 {&quot;names&quot;: [ &quot;John Abraham&quot;, &quot;Lincoln Smith&quot;] }
# 设置 slop 设置的大于100可查询到 Abraham Lincoln
GET my_index/groups/_search 
{
	&quot;query&quot;: { 
		&quot;match_phrase&quot;: {
			&quot;names&quot;: {
				&quot;query&quot;: &quot;Abraham Lincoln&quot;, 
				&quot;slop&quot;: 101
			}
		}
	}
}
</code></pre>
<ul>
<li>properties<br />
文档属性，可指定 Object 或者 nested 类型</li>
<li>search_analyzer<br />
通常，在索引时和搜索时应用相同的分析器，以确保查询中的术语与反向索 引中的术语具有相同的格式，如果想要在搜索时使用与存储时不同的分词器，则 使用 search_analyzer 属性指定，通常用于 ES 实现即时搜索(edge_ngram)</li>
<li>similarity<br />
指定相似度算法，其可选值:
<ul>
<li>BM25 当前版本的默认相似度算法</li>
<li>classic 使用 TF/IDF算法</li>
<li>boolean 一个简单的布尔相似度，当不需要全文排序时使用，并且分数应该只基于查 询条件是否匹配。</li>
</ul>
</li>
<li>store<br />
默认情况下，字段值被索引以使其可搜索，但它们不存储。<br />
在某些情况下，存储字段是有意义的。例如，如果您有一个包含标题、日期 和非常大的内容字段的文档，您可能只想检索标题和日期，而不需要从大型 _source 字段中提取这些字段，可以将标题和日期字段的 store 定义为 ture。</li>
<li>term_vector</li>
</ul>
<h3><a id="%E5%85%83%E5%AD%97%E6%AE%B5meta-fields" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>元字段 meta-fields</h3>
<p>一个文档存有我们定义的业务字段数据之外，它还包含了元数据字段 (meta-fields)。元字段不需要用户定义，在任一文档中都存在，有点类似于数据库的表结构数据。在名称上有个显著的特征，都是以下划线“_”开头。</p>
<p>大体分为五种类型: 身份(标识)元数据、索引元数据、文档元数据、路由<br />
元数据以及其他类型的元数据，当然不是每个文档都有这些元字段的。</p>
<h4><a id="%E8%BA%AB%E4%BB%BD%E6%A0%87%E8%AF%86%E5%85%83%E6%95%B0%E6%8D%AE" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>身份标识元数据</h4>
<ul>
<li>_index:文档所属索引 , 自动被索引，可被查询，聚合，排序使用，或者脚本里访问</li>
<li>_type:文档所属类型，自动被索引，可被查询，聚合，排序使用，或者脚本里访问</li>
<li>_id:文档的唯一标识，建索引时候传入 ，不被索引，可通过_uid 被查询， 脚本里使用，不能参与聚合或排序</li>
</ul>
<h4><a id="%E7%B4%A2%E5%BC%95%E5%85%83%E6%95%B0%E6%8D%AE" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>索引元数据</h4>
<p>_field_names:索引了每个字段的名字，可以包含 null 值，可以通过 exists 查询或 missing 查询方法来校验特定的字段</p>
<h4><a id="%E6%96%87%E6%A1%A3%E5%85%83%E6%95%B0%E6%8D%AE" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>文档元数据</h4>
<ul>
<li>_source : 一个 doc 的原生的 json 数据，不会被索引，用于获取提取字段 值 ，启动此字段，索引体积会变大，如果既想使用此字段又想兼顾索引体积， 可以开启索引压缩。<br />
_source 是可以被禁用的，不过禁用之后部分功能不再支持，这些功能包括: 部分 update api、运行时高亮搜索结果、索引重建、修改 mapping 以及分词、索引升级、debug 查询或者聚合语句、索引自动修复</li>
<li>_size: 整个_source 字段的字节数大小，需要单独安装一个 mapper-size 插 件才能展示。</li>
</ul>
<h4><a id="%E8%B7%AF%E7%94%B1%E5%85%83%E6%95%B0%E6%8D%AE" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>路由元数据</h4>
<p>_routing: 一个 doc 可以被路由到指定的 shard 上。</p>
<h4><a id="%E5%85%B6%E4%BB%96" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>其他</h4>
<p>_meta:一般用来存储应用相关的元信息</p>
<h2><a id="elasticsearch%E9%9B%86%E7%BE%A4%E6%A6%82%E5%BF%B5" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>ElasticSearch 集群概念</h2>
<h3><a id="cluster%EF%BC%88%E9%9B%86%E7%BE%A4%EF%BC%89" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>Cluster（集群）</h3>
<p>cluster（集群）是一个或者多个节点的集合，它们一起保存数据并且提供所有节点联合索引以及搜索功能。集群存在一个唯一的名字身份且默认为 “elasticsearch”。这个名字非常重要，因为如果节点安装时通过它自己的名字加入到集群中的话，那么一个节点只能是一个集群中的一部分。</p>
<p>请确保您在不同环境中不要重复使用相同的集群名字，否则您可能最终会将节点加入到了错误的集群中。例如，您可以使用 order-dev，order-stage，以及 order-prod，用于 development（开发），staging（演示）和 production（生产）集群。</p>
<p>注意，一个集群如果只有一个结点也是有效的，并且完全可行的。此外，您还可以有多个独立的集群并且每个集群都有它自己唯一的 cluster name（集群名）。</p>
<h3><a id="node%EF%BC%88%E8%8A%82%E7%82%B9%EF%BC%89" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>Node（节点）</h3>
<p>node（节点）是一个单独的服务器，它是集群的一部分，存储数据，参与集群中的索引和搜索功能。像一个集群一样，一个节点通过一个在它启动时默认分配的一个随机的 UUID（通用唯一标识符）名称来识别。如果您不想使用默认名称您也可自定义任何节点名称。这个名字是要识别网络中的服务器对应这在您的 Elasticsearch 集群节点管理的目的是很重要的。</p>
<p>节点可以通过配置 cluster name 来加入到指定的集群中。默认情况下，每个节点安装时都会加入到名为 elasticsearch 的集群中，这也就意味着如果您在网络中启动许多节点（假设它们可以发现彼此），它们全部将自动的构成并且加入到一个名为 elasticsearch 的单独的集群中。</p>
<h3><a id="shards-replicas%EF%BC%88%E5%88%86%E7%89%87%E5%89%AF%E6%9C%AC%EF%BC%89" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>Shards &amp; Replicas（分片 &amp; 副本）</h3>
<p>索引可以存储大量数据，可以超过单个节点的硬件限制。例如，十亿个文档占用了 1TB 的磁盘空间的单个索引可能不适合放在单个节点的磁盘上，并且从单个节点服务请求会变得很慢。</p>
<p>为了解决这个问题，Elasticsearch 提供了把 Index（索引）拆分到多个 Shard（分片）中的能力。在创建索引时，您可以简单的定义 Shard（分片）的数量。每个 Shard 本身就是一个 fully-functional（全功能的）和独立的 “Index（索引）”，（Shard）它可以存储在集群中的任何节点上。</p>
<p>Sharding（分片）非常重要两个理由是 :</p>
<p>水平的拆分/扩展。<br />
分布式和并行跨 Shard 操作（可能在多个节点），从而提高了性能/吞吐量。<br />
Shard 的分布式机制以及它的文档是如何聚合支持搜索请求是完全由 Elasticsearch 管理的，并且是对用户透明的。</p>
<p>在 网络/云 环境中可能随时会故障，无论出于何种原因，在 shard/node 不知何故会离线或者消失的情况下强烈建议设置故障转移是非常有效的。为了达到这个目的，Elasticsearch 可以让您设置一个或多个索引的 Shard 副本到所谓的副本分片，或者副本中去。</p>
<p>副本非常重要的两个理由是 :</p>
<p>在 shard/node 故障的情况下提供了高可用性。为了达到这个目的，需要注意的是在原始的/主 Shard 被复制时副本的 Shard 不会被分配到相同的节点上。<br />
它可以让你水平扩展搜索量/吞吐量，因为搜索可以在所有的副本上并行执行。<br />
总而言之，每个索引可以被拆分成多个分片，一个索引可以设置 0 个（没有副本）或多个副本。开启副本后，每个索引将有主分片（被复制的原始分片）和副本分片（主分片的副本）。分片和副本的数量在索引被创建时都能够被指定。在创建索引后，您也可以在任何时候动态的改变副本的数量，但是不能够改变分片数量。</p>
<p>默认情况下，Elasticsearch 中的每个索引分配了 5 个主分片和 1 个副本，这也就意味着如果您的集群至少有两个节点的话，您的索引将会有 5 个主分片和另外 5 个副本分片（1 个完整的副本），每个索引共计 10 个分片。Shards &amp; Replicas（分片 &amp; 副本）<br />
索引可以存储大量数据，可以超过单个节点的硬件限制。例如，十亿个文档占用了 1TB 的磁盘空间的单个索引可能不适合放在单个节点的磁盘上，并且从单个节点服务请求会变得很慢。</p>
<p>为了解决这个问题，Elasticsearch 提供了把 Index（索引）拆分到多个 Shard（分片）中的能力。在创建索引时，您可以简单的定义 Shard（分片）的数量。每个 Shard 本身就是一个 fully-functional（全功能的）和独立的 “Index（索引）”，（Shard）它可以存储在集群中的任何节点上。</p>
<p>Sharding（分片）非常重要两个理由是 :</p>
<p>水平的拆分/扩展。<br />
分布式和并行跨 Shard 操作（可能在多个节点），从而提高了性能/吞吐量。<br />
Shard 的分布式机制以及它的文档是如何聚合支持搜索请求是完全由 Elasticsearch 管理的，并且是对用户透明的。</p>
<p>在 网络/云 环境中可能随时会故障，无论出于何种原因，在 shard/node 不知何故会离线或者消失的情况下强烈建议设置故障转移是非常有效的。为了达到这个目的，Elasticsearch 可以让您设置一个或多个索引的 Shard 副本到所谓的副本分片，或者副本中去。</p>
<p>副本非常重要的两个理由是 :</p>
<p>在 shard/node 故障的情况下提供了高可用性。为了达到这个目的，需要注意的是在原始的/主 Shard 被复制时副本的 Shard 不会被分配到相同的节点上。<br />
它可以让你水平扩展搜索量/吞吐量，因为搜索可以在所有的副本上并行执行。<br />
总而言之，每个索引可以被拆分成多个分片，一个索引可以设置 0 个（没有副本）或多个副本。开启副本后，每个索引将有主分片（被复制的原始分片）和副本分片（主分片的副本）。分片和副本的数量在索引被创建时都能够被指定。在创建索引后，您也可以在任何时候动态的改变副本的数量，但是不能够改变分片数量。</p>
<p>默认情况下，Elasticsearch 中的每个索引分配了 5 个主分片和 1 个副本，这也就意味着如果您的集群至少有两个节点的话，您的索引将会有 5 个主分片和另外 5 个副本分片（1 个完整的副本），每个索引共计 10 个分片。</p>
<p><a href="https://www.elastic.co/cn/blog/elasticsearch-caching-deep-dive-boosting-query-speed-one-cache-at-a-time">es 缓存</a></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Zookeeper]]></title>
    <link href="http://www.throne4j.com/16207420695701.html"/>
    <updated>2021-05-11T22:07:49+08:00</updated>
    <id>http://www.throne4j.com/16207420695701.html</id>
    <content type="html"><![CDATA[

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Kafka 基本操作]]></title>
    <link href="http://www.throne4j.com/16207410864835.html"/>
    <updated>2021-05-11T21:51:26+08:00</updated>
    <id>http://www.throne4j.com/16207410864835.html</id>
    <content type="html"><![CDATA[
<p>我们通过前面文档的了解，知道 Kafka 是由 Zookeeper 管理、协调各 Broker 节点的，我们介绍下对 Zookeeper 和 Kafka 的基本操作。</p>
<h2><a id="zookeeper%E9%9B%86%E7%BE%A4%E6%93%8D%E4%BD%9C" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>Zookeeper 集群操作</h2>
<p>在启动 Kafka 集群之前，需要先启动 Zookeeper 集群。</p>
<h3><a id="zookeeper%E7%9A%84%E4%BD%9C%E7%94%A8%E5%8F%8A%E8%83%8C%E6%99%AF" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>Zookeeper 的作用及背景</h3>
<h3><a id="%E7%AE%A1%E7%90%86%E4%BB%A3%E7%A0%81%E4%B8%AD%E5%8F%98%E9%87%8F%E7%9A%84%E9%85%8D%E7%BD%AE" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>管理代码中变量的配置</h3>
<h3><a id="%E8%AE%BE%E7%BD%AE%E5%91%BD%E5%90%8D%E6%9C%8D%E5%8A%A1" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>设置命名服务</h3>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Kafka 单机安装以及集群安装]]></title>
    <link href="http://www.throne4j.com/16207410719033.html"/>
    <updated>2021-05-11T21:51:11+08:00</updated>
    <id>http://www.throne4j.com/16207410719033.html</id>
    <content type="html"><![CDATA[

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[初识 Kafka]]></title>
    <link href="http://www.throne4j.com/16207202294866.html"/>
    <updated>2021-05-11T16:03:49+08:00</updated>
    <id>http://www.throne4j.com/16207202294866.html</id>
    <content type="html"><![CDATA[
<p>Kafka 是一个分布式实时数据流平台，可独立部署在单台服务器上，也可以在多台服务器上构成集群。它用Zookeeper 来管理、协调 Kafka 集群的各个代理Broker节点，提供了发布与订阅功能。</p>
<h2><a id="%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>基本概念</h2>
<h3><a id="%E4%BB%A3%E7%90%86%E3%80%81%E7%94%9F%E4%BA%A7%E8%80%85%E3%80%81%E6%B6%88%E8%B4%B9%E8%80%85%E3%80%81%E6%B6%88%E8%B4%B9%E8%80%85%E7%BB%84" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>代理、生产者、消费者、消费者组</h3>
<ul>
<li>代理<br />
通常，一个Kafka 进程被称为 一个代理 Broker 节点。</li>
<li>生产者<br />
生产者将消息记录发送到 Kafka 集群指定的 主题 Topic 中进行存储，同时生产者也可以通过自定义算法决定将消息发送到哪个分区(partition)。</li>
<li>消费者<br />
消费者 consumer 从 Kafka 系统指定的主题 Topic 中读取消息记录<br />
在读取主题数据的时候，需要设置消费组名称(GroupId),如果不设置Kafka消费者会默认生成一个消费组名称。</li>
<li>消费者组<br />
消费者程序在读取 Kafka 系统主题 Topic 中的数据时，通常会使用多个线程来执行。一个消费组可以包含一个或多个消费者程序，使用多分区和多线程模式可以极大提高读取数据的效率。</li>
</ul>
<p>一般而言，一个消费者对应一个线程，并遵循 <em><strong><em>线程数小于等于分区数</em></strong></em> 的原则。</p>
<h3><a id="%E4%B8%BB%E9%A2%98%E3%80%81%E5%88%86%E5%8C%BA%E3%80%81%E5%89%AF%E6%9C%AC%E3%80%81%E8%AE%B0%E5%BD%95" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>主题、分区、副本、记录</h3>
<ul>
<li>
<p>主题 Topic<br />
Kafka 系统通过主题来区分不同业务类型的消息记录。</p>
</li>
<li>
<p>分区 Partition<br />
每个主题中可以有一个或者多个分区。每个分区内部的消息记录是有序的，每个消息都有一个连续的偏移量序号(offset)。一个分区只对应一个代理节点，一个代理节点可以管理多个分区</p>
</li>
<li>
<p>副本 replication<br />
在 Kafka 系统中，每个主题 Topic 在创建时会要求指定它的副本数，默认是 1。通过副本机制来保证 kafka 分布式集群数据的高可用性。创建主题时，主题的副本系数值应如下设置：</p>
<ul>
<li>若集群数量大于等于 3，则主题的副本系数可以设置为 3，如果副本系数小于3的话，可能会造成数据丢失的问题</li>
<li>若集群数量小于 3，则主题的副本系数可以设置为小于等于集群数量值</li>
</ul>
</li>
<li>
<p>记录 Record<br />
被实际写入到 Kafka 集群并且可以被消费者应用程序读取的数据，被称为记录 Record，每条记录包含一个 键(Key)、值(value)、和时间戳(timestamp)</p>
</li>
</ul>
<h2><a id="kafka%E8%AE%BE%E8%AE%A1%E7%9B%AE%E6%A0%87" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>Kafka 设计目标</h2>
<ul>
<li>高吞吐量</li>
<li>高可用队列</li>
<li>低延时</li>
<li>分布式机制</li>
</ul>
<h2><a id="kafka%E7%9A%84%E4%BD%BF%E7%94%A8%E5%9C%BA%E6%99%AF" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>Kafka 的使用场景</h2>
<p>Kafka 拥有处理海量数据的能力，可以在实时业务场景发挥天然优势，也可以在离线任务场景下发挥不俗的能力，这都得力于 Kafka 底层的通用性和其强大的应用接口API。</p>
<p>在实时业务场景中，Kafka 能够和 Spark、Flink、Storm 等实时计算引擎完美结合，同时 Kafka 也提供了应用接口 API，可以将 主题 Topic 中国的数据导入到 Hive 仓库做离线计算。</p>
<p>日志收集、消息系统、用户轨迹、记录运营监控数据、实现流处理、事件源</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[InnoDB 记录存储结构和索引页结构]]></title>
    <link href="http://www.throne4j.com/16203036348535.html"/>
    <updated>2021-05-06T20:20:34+08:00</updated>
    <id>http://www.throne4j.com/16203036348535.html</id>
    <content type="html"><![CDATA[
<p><figure><img src="media/16203036348535/16203039376703.jpg" alt="" style="width:980px;" /></figure></p>
<p>从记录存储结构和索引结构图来看，记录是通过 next record 指针进行关系组织的，他们形成单链表的数据结构。</p>
<p>单链表的查询效率对于mysql 查询来说是比较低效的，</p>
<p>数据记录里面有一个delete_mask 这样一个标志位，也就是说 Mysql 在删除数据并commit 之后，并不会立刻就从磁盘上把数据删除了，而是将删除的数据的 delete_mask 标记为已删除，并组织成一个新的链表，然后再定时调用 fsync 刷新到磁盘</p>
<p>表空间 2^32 个页面</p>
<p>表空间--段--组--256个区--64页(64 1M)</p>
<h2><a id="%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A6%81%E6%8F%90%E5%8F%96%E5%87%BA%E4%B8%80%E4%B8%AA%E5%8C%BA%E7%9A%84%E6%A6%82%E5%BF%B5%EF%BC%9F" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>为什么要提取出一个区的概念？</h2>
<p>mysql 尽可能保证一个区上面的 64 个页面在存储上是连续的，避免了磁盘随机 IO。</p>
<h2><a id="%E8%A1%8C%E6%BA%A2%E5%87%BA" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>行溢出</h2>
<p>Mysql 中一个页面中必须至少保存两条记录</p>
<h2><a id="innodb%E4%B8%89%E5%A4%A7%E7%89%B9%E6%80%A7%EF%BC%9A%E5%8F%8C%E5%86%99%E7%BC%93%E5%86%B2%E5%8C%BA%E3%80%81-ahi%E3%80%81buffer-pool" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>InnoDB 三大特性：双写缓冲区、AHI、buffer pool</h2>
<p>脏页刷新的时候才会使用双写缓冲区<br />
数据首先双写缓冲区然后通过 fsync 函数强制刷新到磁盘上</p>
<p>双写缓冲区中保存的是脏页的全量数据。</p>
<p>redo log 记录的是对每个页面的物理操作</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Import 注解]]></title>
    <link href="http://www.throne4j.com/16202942111296.html"/>
    <updated>2021-05-06T17:43:31+08:00</updated>
    <id>http://www.throne4j.com/16202942111296.html</id>
    <content type="html"><![CDATA[
<p>@Import 注解可以引入 @Configuration、ImportSelector、ImportBeanDefinitionRegistrar</p>
<p>ConfigurationClassPostProcessor</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[openfeign 原理解析]]></title>
    <link href="http://www.throne4j.com/16202911787586.html"/>
    <updated>2021-05-06T16:52:58+08:00</updated>
    <id>http://www.throne4j.com/16202911787586.html</id>
    <content type="html"><![CDATA[
<p>我们在使用 springCloud 进行微服务开发的时候，当需要进行远程调用的时候，我们会经常使用到 feign 这样一个组件。下面就来看下它是如何工作的吧。</p>
<p>首先我们需要通过一个注解 @EnableFeignClients 开启feign功能</p>
<pre><code class="language-java">@Retention(RetentionPolicy.RUNTIME)
@Target(ElementType.TYPE)
@Documented
@Import(FeignClientsRegistrar.class)
public @interface EnableFeignClients {
    // 同 basePackages
    String[] value() default {};
    // 需要扫描的包路径
    String[] basePackages() default {};
    // 
    Class&lt;?&gt;[] basePackageClasses() default {};
    /**
     *使用的配置类 可查看 FeignClientsConfiguration，
     *可自定义 {@link feign.codec.Decoder}, {@link feign.codec.Encoder}, {@link feign.Contract}.
    */
    Class&lt;?&gt;[] defaultConfiguration() default {};
    // 非 basePackages 路径下的 feignClient
    Class&lt;?&gt;[] clients() default {};
}
</code></pre>
<p>从源码中我们可以看到 EnableFeignClients 通过 Import 注解引入了 FeignClientsRegistrar 类，此类实现了 ImportBeanDefinitionRegistrar 接口，下面主要看实现类的方法 registerBeanDefinitions</p>
<pre><code class="language-java">@Override
public void registerBeanDefinitions(AnnotationMetadata metadata,
		BeanDefinitionRegistry registry) {
		// 注册默认的配置信息
    registerDefaultConfiguration(metadata, registry);
    // 这里注册 feign 客户端
    registerFeignClients(metadata, registry);
}
</code></pre>
<p>方法 registerDefaultConfiguration 获取注解 @EnableFeignClients 中的配置信息。</p>
<p>registerFeignClients 通过对EnableFeignClients 中定义的包路径basePackages 和 clients 属性进行解析，扫描出来所有被 @FeignClient 注解修饰的类，并调用 registerFeignClient 进行feign 客户端的注册。</p>
<pre><code class="language-java">private void registerFeignClient(BeanDefinitionRegistry registry,
		AnnotationMetadata annotationMetadata, Map&lt;String, Object&gt; attributes) {
	String className = annotationMetadata.getClassName();
	BeanDefinitionBuilder definition = BeanDefinitionBuilder
			.genericBeanDefinition(FeignClientFactoryBean.class);
	validate(attributes);
	definition.addPropertyValue(&quot;url&quot;, getUrl(attributes));
	definition.addPropertyValue(&quot;path&quot;, getPath(attributes));
	String name = getName(attributes);
	definition.addPropertyValue(&quot;name&quot;, name);
	String contextId = getContextId(attributes);
	definition.addPropertyValue(&quot;contextId&quot;, contextId);
	definition.addPropertyValue(&quot;type&quot;, className);
	definition.addPropertyValue(&quot;decode404&quot;, attributes.get(&quot;decode404&quot;));
	definition.addPropertyValue(&quot;fallback&quot;, attributes.get(&quot;fallback&quot;));
	definition.addPropertyValue(&quot;fallbackFactory&quot;, attributes.get(&quot;fallbackFactory&quot;));
	definition.setAutowireMode(AbstractBeanDefinition.AUTOWIRE_BY_TYPE);

	String alias = contextId + &quot;FeignClient&quot;;
	AbstractBeanDefinition beanDefinition = definition.getBeanDefinition();

	boolean primary = (Boolean) attributes.get(&quot;primary&quot;); 
	beanDefinition.setPrimary(primary);

	String qualifier = getQualifier(attributes);
	if (StringUtils.hasText(qualifier)) {
		alias = qualifier;
	}

	BeanDefinitionHolder holder = new BeanDefinitionHolder(beanDefinition, className,
			new String[] { alias });
	BeanDefinitionReaderUtils.registerBeanDefinition(holder, registry);
}
</code></pre>
<p>在上面的代码中，对feignClient注解进行解析，获取相关配置参数并进行 BeanDefinition 的注册。</p>
<p>这里有一个非常重要的类： FeignClientFactoryBean，这个是 spring 框架中非常重要的接口 FactoryBean 的实现类，用于自定义实例化对象，</p>
<p>对于FactoryBean，我们以 getObject 方法为切入口，看下feignClient 是如何实例化的。</p>
<pre><code class="language-java">public Object getObject() throws Exception {
	return getTarget();
}

&lt;T&gt; T getTarget() {
	FeignContext context = this.applicationContext.getBean(FeignContext.class);
	Feign.Builder builder = feign(context);

	if (!StringUtils.hasText(this.url)) {
		if (!this.name.startsWith(&quot;http&quot;)) {
			this.url = &quot;http://&quot; + this.name;
		}
		else {
			this.url = this.name;
		}
		this.url += cleanPath();
		return (T) loadBalance(builder, context,
				new HardCodedTarget&lt;&gt;(this.type, this.name, this.url));
	}
	if (StringUtils.hasText(this.url) &amp;&amp; !this.url.startsWith(&quot;http&quot;)) {
		this.url = &quot;http://&quot; + this.url;
	}
	String url = this.url + cleanPath();
	Client client = getOptional(context, Client.class);
	if (client != null) {
		if (client instanceof LoadBalancerFeignClient) {
			// not load balancing because we have a url,
			// but ribbon is on the classpath, so unwrap
			client = ((LoadBalancerFeignClient) client).getDelegate();
		}
		builder.client(client);
	}
	Targeter targeter = get(context, Targeter.class);
	return (T) targeter.target(this, builder, context,
			new HardCodedTarget&lt;&gt;(this.type, this.name, url));
}
</code></pre>
<p>代码中 feign(context) 对feign 进行配置，可以配置的属性有 FeignLoggerFactory，feign.Logger.Level、feign.codec.Encoder、feign.codec.Decoder、feign.Contract、feign.Request.Options、feign.Retryer、feign.RequestInterceptor.</p>
<p>以上代码中的 targeter 是在</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[HTTP1.X 与 HTTP2.0 的区别]]></title>
    <link href="http://www.throne4j.com/16202881379578.html"/>
    <updated>2021-05-06T16:02:17+08:00</updated>
    <id>http://www.throne4j.com/16202881379578.html</id>
    <content type="html"><![CDATA[
<p>主要从以下几点来分析</p>
<h2><a id="http1-x" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>Http1.x</h2>
<p>缺点是：线程阻塞，在同一时间，同一域名的请求有一定数量限制，超过限制数目的请求会被阻塞</p>
<h3><a id="http1-0" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>Http1.0</h3>
<p>缺陷：客户端与服务器只<strong>保持短连接</strong><br />
解决办法：添加头信息，Connection:keep-alive</p>
<h3><a id="http1-1" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>Http1.1</h3>
<p>头信心是ASCII编码的文本信息，数据可以是文本也可以是二进制</p>
<p>与 http1.0 比较，有所改进</p>
<ul>
<li><strong>引入长连接</strong>，即TCP 连接默认不关闭，可以别多个请求复用</li>
<li>管道机制，在同一个 TCP 连接里面，客户端可以同时发送多个请求</li>
<li>分块传输编码，服务端每产生一块数据，就发送一块数据，采用”流模式“代替”缓存模式“</li>
<li>新增请求方式
<ul>
<li>PUT</li>
<li>DELETE</li>
<li>OPTIONS</li>
<li>TRACE 主要用于测试或者诊断</li>
<li>CONNECT</li>
</ul>
</li>
</ul>
<p>缺点：</p>
<ul>
<li>虽然允许复用 TCP 连接，但是同一个 TCP 连接里面，所有数据通信时按次序进行的。服务只能够处理完一个请求之后，再处理下一个请求，如果前面的请求处理非常忙，则后面的请求都需要排队，这就导致请求队列队头阻塞</li>
</ul>
<p>对于 HTTP1.1 的缺点，要怎么进行解决呢？</p>
<ul>
<li>减少连接数量，但是吞吐量大大降低</li>
<li>增加长连接的数量，但是耗费服务器资源</li>
</ul>
<h2><a id="http2-0" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>Http2.0</h2>
<h3><a id="%E7%89%B9%E7%82%B9" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>特点</h3>
<ul>
<li>
<p>采用二进制格式而非文本格式进行传输<br />
HTTP2.0 是一个测地的二进制协议，头信息和数据体都是二进制，统称为帧。二进制处理起来更加高效。</p>
</li>
<li>
<p>采用多路复用，而非有序并阻塞的，只需一个连接即可实现并行</p>
</li>
<li>
<p>使用报头压缩，降低开销<br />
HTTP2.0 引入头信息压缩机制，一方面头信息使用 GZIP 或者 compress 压缩后在发送，另一方面，服务器和客户端共同维护一张头信息表，所有字段都会存入这个表，产生一个索引号，之后就不发送同样的字段了，只需要发送索引号</p>
</li>
<li>
<p>服务器推送<br />
HTTP2.0 允许服务端主动向客户端发送信息</p>
</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[EnableWebMvc 开启springMVC 功能]]></title>
    <link href="http://www.throne4j.com/16188047284332.html"/>
    <updated>2021-04-19T11:58:48+08:00</updated>
    <id>http://www.throne4j.com/16188047284332.html</id>
    <content type="html"><![CDATA[
<p>DelegatingWebMvcConfiguration</p>
<p>我们常用的开启springMVC功能的注解 @EnableWebMvc</p>
<pre><code class="language-java">@Retention(RetentionPolicy.RUNTIME)
@Target(ElementType.TYPE)
@Documented
@Import(DelegatingWebMvcConfiguration.class)
public @interface EnableWebMvc {
}
</code></pre>
<p>这里主要通过 @Import 注解引入DelegatingWebMvcConfiguration 类</p>
<pre><code class="language-java">@Configuration
public class DelegatingWebMvcConfiguration extends WebMvcConfigurationSupport {
    
}
</code></pre>

]]></content>
  </entry>
  
</feed>
